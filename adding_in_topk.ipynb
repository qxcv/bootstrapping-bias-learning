{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "from pref_bootstrap import priors\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "INFO:root:Occupancy measure error@iter   0: 6.313213 (||params||=4.090950, ||grad||=8.927335, ||E[dr/dw]||=7.410994)\n",
      "INFO:root:Occupancy measure error@iter  100: 0.068500 (||params||=4.339939, ||grad||=0.083829, ||E[dr/dw]||=6.542890)\n",
      "INFO:root:Occupancy measure error@iter  200: 0.011741 (||params||=4.173945, ||grad||=0.013915, ||E[dr/dw]||=6.487565)\n",
      "INFO:root:Occupancy measure error@iter  300: 0.003088 (||params||=4.136445, ||grad||=0.003614, ||E[dr/dw]||=6.484353)\n",
      "INFO:root:Occupancy measure error@iter  400: 0.001399 (||params||=4.132376, ||grad||=0.001755, ||E[dr/dw]||=6.483721)\n",
      "INFO:root:Occupancy measure error@iter  500: 0.001254 (||params||=4.135142, ||grad||=0.001459, ||E[dr/dw]||=6.483640)\n",
      "INFO:root:Occupancy measure error@iter  600: 0.001118 (||params||=4.138918, ||grad||=0.001293, ||E[dr/dw]||=6.483665)\n"
     ]
    }
   ],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state visitation frequencies for each grid cell:\n",
      "[[0.     0.     0.     0.    ]\n",
      " [0.     1.088  0.007  0.    ]\n",
      " [0.     6.3697 0.5354 0.    ]\n",
      " [0.     0.     0.     0.    ]]\n",
      "\n",
      "Recovered state visitation frequencies for each grid cell:\n",
      "[[0.     0.     0.     0.    ]\n",
      " [0.     1.0876 0.008  0.    ]\n",
      " [0.     6.3694 0.535  0.    ]\n",
      " [0.     0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (IRL): -12.056126594543457\n",
      "Gradient w.r.t. reward params (IRL):\n",
      " [ 0.      0.      0.      0.      0.     -0.8208 -1.3916  0.      0.      0.9748 -0.0863  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (IRL):\n",
      " [ 0.      0.      0.      0.      0.      0.2844 -1.5819  0.      0.      5.867   0.0113  0.\n",
      "  0.      0.      0.      0.    ]\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (IRL):', float(irl_feedback_model.log_likelihood(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. reward params (IRL):\\n', np.asarray(irl_feedback_model.log_likelihood_grad_rew(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. bias params (IRL):\\n', irl_feedback_model.log_likelihood_grad_bias(irl_dataset, rmodel, irl_bias_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example with the paired comparison learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (PC): -0.7906373739242554\n",
      "Gradient w.r.t. reward params (PC):\n",
      " [ 0.      0.      0.      0.      0.     -0.5831 -0.2774  0.      0.     -0.0568  0.9174  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (PC):\n",
      " -0.34942213\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (PC):', float(pc_feedback_model.log_likelihood(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. reward params (PC):\\n', np.asarray(pc_feedback_model.log_likelihood_grad_rew(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. bias params (PC):\\n', pc_feedback_model.log_likelihood_grad_bias(comparison_dataset, rmodel, pc_bias_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example with the topk\n",
    "\n",
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng, top_K_bias_params = top_K_feedback_model.init_bias_params(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff 18.0\n",
      "Log likelihood (topk): 0.42988160252571106\n",
      "Gradient w.r.t. reward params (PC):\n",
      " [0.     0.     0.     0.     0.     0.1291 0.1114 0.     0.     0.0388 0.0131 0.     0.     0.\n",
      " 0.     0.    ]\n",
      "Gradient w.r.t. bias params (PC):\n",
      " [-0.6913 -0.0325]\n"
     ]
    }
   ],
   "source": [
    "def generate_topk_dataset(n_traj):\n",
    "    # def genereate_top_K_dataset(n_traj): \n",
    "    import random\n",
    "    traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "    labels= top_K_expert.interact(traj)\n",
    "\n",
    "    labels_final, traj_final = [], []\n",
    "    for l, t in zip(labels, traj['states']): \n",
    "        if not(l):\n",
    "            if True:#TODO if there are issues update this so that we balance our dataset.\n",
    "                labels_final.append(l), traj_final.append(t)\n",
    "        else: \n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "\n",
    "    labels_final = np.array(labels_final)\n",
    "    \n",
    "    return {\n",
    "        'trajectories':np.array(traj_final), \n",
    "        'labels':labels_final\n",
    "    }\n",
    "\n",
    "top_K_dataset = generate_topk_dataset(100)\n",
    "\n",
    "print('Log likelihood (topk):', float(top_K_feedback_model.log_likelihood(top_K_dataset,rmodel, top_K_bias_params)))\n",
    "print('Gradient w.r.t. reward params (PC):\\n', np.asarray(top_K_feedback_model.log_likelihood_grad_rew(top_K_dataset, rmodel, top_K_bias_params)))\n",
    "print('Gradient w.r.t. bias params (PC):\\n', top_K_feedback_model.log_likelihood_grad_bias(top_K_dataset, rmodel,top_K_bias_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we can create a combined demo in which we use both MCE IRL and paired comparisons to learn a reward model and set of modality-specific bias models.\n",
    "\n",
    "Ideas for ergonomic improvements:\n",
    "\n",
    "- Incorporate reward priors on the `RewardModel` class, just as we have for bias priors and `EnvFeedbackModel`.\n",
    "- Make parameter storage consistent between reward models and feedback models. Either feedback models should store parameters (instead of keeping them external), or reward models should keep parameters external (instead of storing them as class attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(sam): try finding a step size with backtracking line search, rather\n",
    "# than using a fixed step. Also consider using a Nesterov step.\n",
    "def joint_training(env, feedback_models, all_names, all_datasets,\n",
    "                   all_bias_params, rmodel, rprior, init_step_size=1e-3,\n",
    "                   n_steps=100):\n",
    "    \"\"\"Jointly trains a single reward model and a set of feedback bias models\n",
    "    on a given environment. Trains reward model with gradient descent, and\n",
    "    individual bias models with projected gradient descent.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Environment): environment to train on.\n",
    "        feedback_models ([EnvFeedbackModel]): list of environment feedback\n",
    "            models.\n",
    "        all_names ([str]): human-readable names for the feedback models, to\n",
    "            be used for logging (`all_names[i]` is the name for\n",
    "            `feedback_models[i]`).\n",
    "        all_datasets (list): list of datasets for the feedback models\n",
    "            (`all_datasets[i]` is the dataset for `feedback_models[i]`).\n",
    "        all_bias_params (list): list of bias parameters for feedback models.\n",
    "            (`all_bias_params[i]` contains bias parameters for\n",
    "            `feedback_models[i]`).\n",
    "        rmodel (RewardModel): reward model, assumed to be shared across all\n",
    "            feedback modalities.\n",
    "        rprior (Prior): prior on reward model parameters.\n",
    "        init_step_size (float): length of the first step. Step size scales\n",
    "            linearly down to (just above) zero over the course of training.\n",
    "        n_steps (int): total number of steps to take.\n",
    "        \n",
    "    Returns: tuple of `(rmodel, all_bias_params)`, where `rmodel` is the same\n",
    "        model passed into the functino (this is updated in-place), and\n",
    "        `all_bias_params` is a set of updated bias parameters (these are not\n",
    "        updated in-place).\"\"\"\n",
    "    assert len(feedback_models) == len(all_datasets)\n",
    "    assert len(feedback_models) == len(all_bias_params)\n",
    "    all_bias_params = list(all_bias_params)\n",
    "    \n",
    "    def compute_ll():\n",
    "        \"\"\"Compute log likelihood of data under current reward model, bias\n",
    "        params, etc.\"\"\"\n",
    "        ll_terms = [\n",
    "            ('rmodel_prior', rprior.log_prior(rmodel.get_params())),\n",
    "        ]\n",
    "        name_model_ds_bias_iter = zip(\n",
    "            all_names, feedback_models, all_datasets, all_bias_params)\n",
    "        for name, model, dataset, bias_params in name_model_ds_bias_iter:\n",
    "            ll = model.log_likelihood(dataset, rmodel, bias_params)\n",
    "            ll_terms.append((f'{name}', ll))\n",
    "            ll_terms.append((f'{name}_prior', model.bias_prior.log_prior(bias_params)))\n",
    "        total_ll = sum(ll for name, ll in ll_terms)\n",
    "        return_dict = dict(ll_terms)\n",
    "        return_dict['all_terms'] = total_ll\n",
    "        return return_dict\n",
    "\n",
    "    ll_history = {k: [v] for k, v in compute_ll().items()}\n",
    "\n",
    "    for step_num in range(n_steps):\n",
    "        # progress_10 is 1 at the start of training, and linear drops to\n",
    "        # (just above) 0 at the end (after n_steps)\n",
    "        progress_10 = 1 - step_num / n_steps\n",
    "        step_size = init_step_size * progress_10 \n",
    "\n",
    "        # first do a joint reward model update w/ plain gradient descent\n",
    "        rmodel_grad = np.zeros_like(rmodel.get_params())\n",
    "        model_ds_bias_iter = zip(feedback_models, all_datasets, all_bias_params)\n",
    "        for model, dataset, bias_params in model_ds_bias_iter:\n",
    "            rmodel_grad += model.log_likelihood_grad_rew(\n",
    "                dataset, rmodel, bias_params)\n",
    "        rmodel_grad += rprior.log_prior_grad(rmodel.get_params())\n",
    "        rmodel.set_params(rmodel.get_params() + step_size * rmodel_grad)\n",
    "\n",
    "        # now do individual bias model updates\n",
    "        model_ds_bias_enum = enumerate(zip(\n",
    "            feedback_models, all_datasets, all_bias_params))\n",
    "        for model_idx, (model, dataset, bias_params) in model_ds_bias_enum:\n",
    "            # Projected gradient descent: we take a gradient descent step, then\n",
    "            # project back to the support of the prior.\n",
    "            bias_param_grad = model.log_likelihood_grad_bias(\n",
    "                dataset, rmodel, bias_params)\n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            bias_grad = bias_param_grad + bias_prior_grad\n",
    "            unproj_step = all_bias_params[model_idx] + step_size * bias_grad\n",
    "            proj_step = model.bias_prior.project_to_support(bias_grad)\n",
    "            all_bias_params[model_idx] = proj_step\n",
    "\n",
    "            for k, v in compute_ll().items():\n",
    "                ll_history[k].append(v)\n",
    "\n",
    "    return rmodel, all_bias_params, ll_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userdata/smetzger/gim/lib/python3.6/site-packages/jax/_src/lax/lax.py:6271: UserWarning: Explicitly requested dtype float64 requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    }
   ],
   "source": [
    "jt_names = ['blind_irl', 'paired_comparisons']\n",
    "jt_feedback_models = [irl_feedback_model, pc_feedback_model]\n",
    "jt_datasets = [irl_dataset, comparison_dataset]\n",
    "rng, jt_irl_feedback_params = irl_feedback_model.init_bias_params(rng)\n",
    "rng, jt_pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "jt_bias_params = [jt_irl_feedback_params, jt_pc_bias_params]\n",
    "jt_rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "jt_rprior = priors.FixedGaussianPrior(\n",
    "    mean=0.0, std=1.0, shape=rmodel.get_params().shape)\n",
    "jt_rmodel, jt_bias_params, ll_history = joint_training(\n",
    "    env=env, feedback_models=jt_feedback_models, all_names=jt_names,\n",
    "    all_datasets=jt_datasets, all_bias_params=jt_bias_params,\n",
    "    rmodel=jt_rmodel, rprior=jt_rprior, init_step_size=1e-3, n_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "jt_names = ['top_k']\n",
    "jt_feedback_models = [top_K_feedback_model]\n",
    "jt_datasets = [top_K_dataset]\n",
    "rng, jt_topK_feedback_params = top_K_feedback_model.init_bias_params(rng)\n",
    "jt_bias_params = jt_topK_feedback_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting just the total log likelihood\n",
    "loss_values = ll_history['all_terms']\n",
    "plt.plot(np.arange(len(loss_values)), loss_values)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Log likelihood')\n",
    "plt.title('Log likelihood during training (higher is better)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting all terms in the log likelihood\n",
    "plt.figure(figsize=(5, 20))\n",
    "all_hist_items = list(ll_history.items())\n",
    "for idx, (plot_name, plot_values) in enumerate(all_hist_items):\n",
    "    plt.subplot(len(all_hist_items), 1, idx + 1)\n",
    "    plt.plot(np.arange(len(plot_values)), plot_values)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel(plot_name)\n",
    "plt.suptitle('Values of all components of the log likelihood')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
