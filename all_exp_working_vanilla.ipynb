{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALRIGHT, HERE IS WHERE I WILL DEBUG THE TOPK FEEDBACK LEARNER\n",
    "# ABLATIONS: COULD WE DO BETTER IF ALL OUR TRAJECTORIES ARE GOOD? \n",
    "# OR IS IT NECESSARY TO HAVE BAD TRAJECTORIES? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def genereate_top_K_dataset(n_traj): \n",
    "import random\n",
    "\n",
    "n_traj = 1000\n",
    "traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "labels= top_K_expert.interact(traj)\n",
    "\n",
    "labels_final, traj_final = [], []\n",
    "for l, t in zip(labels, traj['states']): \n",
    "    if not(l):\n",
    "        if True:#random.random() < (np.sum(labels)/len(labels)):\n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "    else: \n",
    "        labels_final.append(l), traj_final.append(t)\n",
    "        \n",
    "labels_final = np.array(labels_final)\n",
    "        \n",
    "print(np.sum(labels)/len(labels))\n",
    "print(np.sum(labels_final)/len(labels_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_dataset(n_traj):\n",
    "    # def genereate_top_K_dataset(n_traj): \n",
    "    import random\n",
    "    traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "    labels= top_K_expert.interact(traj)\n",
    "\n",
    "    labels_final, traj_final = [], []\n",
    "    for l, t in zip(labels, traj['states']): \n",
    "        if not(l):\n",
    "            if random.random() < top_K_expert.K:#TODO if there are issues update this so that we balance our dataset.\n",
    "                labels_final.append(l), traj_final.append(t)\n",
    "        else: \n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "\n",
    "    labels_final = np.array([int(l) for l in labels_final])\n",
    "    \n",
    "    return {\n",
    "        'trajectories':np.array(traj_final), \n",
    "        'labels':labels_final\n",
    "    }\n",
    "\n",
    "top_K_dataset = generate_topk_dataset(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "steps = 10000\n",
    "loss_prev = float('Inf')\n",
    "delta = 100\n",
    "eps = 1e-5\n",
    "\n",
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, top_K_bias_params = top_K_feedback_model.init_bias_params(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def trad_optimize(model, data, rmodel, bias_params, eps=1e-9):\n",
    "    \n",
    "    lr = 1e-2\n",
    "    steps = 1000\n",
    "    loss_prev = float('Inf')\n",
    "    delta = 100\n",
    " \n",
    "    step = 0\n",
    "    \n",
    "    while(np.abs(delta) > eps and step<steps):\n",
    "        gbias = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "        bias_params = bias_params + lr*gbias\n",
    "        grew = model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "        new_r = rmodel.get_params() + lr*grew\n",
    "        rmodel.set_params(new_r)\n",
    "        loss = model.log_likelihood(data, rmodel, bias_params)\n",
    "        if step % 100 == 0:\n",
    "            print('step %d loss %.3f' %(step, loss))\n",
    "        if step > 0: \n",
    "            delta = loss-loss_prev\n",
    "        loss_prev = loss\n",
    "        step +=1\n",
    "\n",
    "\n",
    "       \n",
    "     \n",
    "\n",
    "        \n",
    "\n",
    "    if np.abs(delta) <= .0001: \n",
    "        print('terminated due to delta')\n",
    "    else: \n",
    "        print('terminated due to steps exceeding %d' %steps)\n",
    "        \n",
    "    return model, rmodel, bias_params\n",
    "\n",
    "_, rmodel, _ = trad_optimize(top_K_feedback_model, top_K_dataset, rmodel, top_K_bias_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rmodel):\n",
    "    plt.plot(rmodel.get_params(), label='estimated')\n",
    "    plt.plot(env.reward_matrix, label='real reward')\n",
    "    plt.legend()\n",
    "    plt.title('Estimated Reward Function')\n",
    "\n",
    "    _, topk_om = mce_irl.mce_occupancy_measures(env, R=rmodel.get_params())\n",
    "\n",
    "    print('Optimal state visitation frequencies for each grid cell:')\n",
    "    print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "\n",
    "    print('Inferred ')\n",
    "    print(topk_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "    \n",
    "evaluate(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now can we get another feedback modality to work with the traditional grad descent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "_, rmodel, _ = trad_optimize(pc_feedback_model, comparison_dataset, rmodel, pc_bias_params)\n",
    "evaluate(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)\n",
    "_, rmodel, _ = trad_optimize(irl_feedback_model, irl_dataset, rmodel, irl_bias_params)\n",
    "evaluate(rmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
