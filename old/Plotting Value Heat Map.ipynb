{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import visual_data_explanation as vde\n",
    "from gridworld import GridworldMdp\n",
    "from utils import plot_trajectory, plot_reward, plot_reward_and_trajectories\n",
    "from fast_agents import FastOptimalAgent\n",
    "import numpy as np\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% ls -lah model_save/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"model_save_sess_0/.meta\")\n",
    "graph = tf.get_default_graph()\n",
    "sess = tf.Session(graph = graph)\n",
    "# necessary to reload the ** tf.VARIABLES ** that allow training to occur\n",
    "saver.restore(sess, \"model_save_sess_0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be a list of <tf.Variable '...', >\n",
    "graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "reward_var = None\n",
    "for v in tfvars:\n",
    "    if 'reward:0' in v.name:\n",
    "        print(v)\n",
    "        reward_var = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in graph.get_operations() if 'q_' in x.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need 3 things: reward_optimize_op, err, step2_cost\n",
    "labels = graph.get_tensor_by_name('y:0')\n",
    "w_o = graph.get_tensor_by_name(\"VIN/w_o:0\")\n",
    "q = graph.get_tensor_by_name(\"q_10:0\")\n",
    "q_out = tf.reshape(q, [-1, 5])\n",
    "logits = tf.matmul(q_out, w_o)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=labels, name='cross_entropy-z')\n",
    "cross_entropy_mean = tf.reduce_mean(\n",
    "    cross_entropy, name='cross_entropy_mean-z')\n",
    "logits_cost = tf.add_n([cross_entropy_mean], name='logits_loss')\n",
    "\n",
    "reward_regularizer_C = 0.1\n",
    "l1_regularizer = tf.contrib.layers.l1_regularizer(reward_regularizer_C)\n",
    "reward_regularizer_cost = tf.contrib.layers.apply_regularization(l1_regularizer, [reward_var])\n",
    "step2_cost = logits_cost \n",
    "\n",
    "reward_lr = 1.5 # default in config file\n",
    "reward_optimizer = tf.train.AdamOptimizer(reward_lr, name='adam-opt')\n",
    "reward_optimize_op = reward_optimizer.minimize(step2_cost, var_list=[reward_var], name='reward_optim')\n",
    "# reward_optimize_op = graph.get_operation_by_name('reward/Adam')\n",
    "\n",
    "num_actions = 5\n",
    "labels = tf.reshape(graph.get_tensor_by_name('y:0'), [-1, num_actions])\n",
    "cp = tf.cast(tf.argmax(graph.get_tensor_by_name(\"output:0\"), 1), tf.int32)\n",
    "most_likely_labels = tf.cast(tf.argmax(labels, axis=1), tf.int32)\n",
    "err = tf.reduce_mean(tf.cast(tf.not_equal(cp, most_likely_labels), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_collection(\"losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtensors = set()\n",
    "for op in graph.get_operations():\n",
    "    if 'v_' in op.name and 'grad' not in op.name and '/' not in op.name:\n",
    "        vtensors.add(op)\n",
    "\n",
    "# Find all the value maps, later we'll sort them & plot them\n",
    "sortable_vtensors = []\n",
    "greatest = None\n",
    "greatestnum = 1\n",
    "for op in vtensors:\n",
    "    opname = op.name.split('/')[0]\n",
    "    opnum = int(opname[2:])\n",
    "    sortable_vtensors.append((opnum, op))\n",
    "    if opnum > greatestnum:\n",
    "        greatest = op\n",
    "        greatestnum = opnum\n",
    "    \n",
    "print(greatest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = vde.grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridworldMdp(grids[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walls, reward, start = mdp.convert_to_numpy_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_vector(walls, reward, num=20):\n",
    "    \"\"\"\n",
    "    Use to fill out the batch dimension of the VIN model's input.\n",
    "    Returns the batched (repeated) walls and rewards as tuple\"\"\"\n",
    "    w = np.stack([walls] * 20)\n",
    "    r = np.stack([reward] * 20)\n",
    "    return w, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vals):\n",
    "    vals = vals - vals.mean()\n",
    "    vals = vals / (vals.max() - vals.min())\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "\n",
    "w, r = repeat_vector(walls, reward)\n",
    "for ax, gr in zip(axes, grids[2:4]):\n",
    "    walls, reward, start = GridworldMdp(gr).convert_to_numpy_input()\n",
    "    \n",
    "    # plotting softmax output of VIN (last layer)\n",
    "    oo = graph.get_operation_by_name('output')\n",
    "    qs = sess.run(feed_dict={'image:0': w, 'reward:0': r}, fetches=oo.outputs)[0]\n",
    "    values = np.max(qs, axis=-1)\n",
    "    val = np.reshape(values, (20, 8, 8))[0]\n",
    "    plot_reward(normalize(val), walls, \"softmax\", fig, ax[2])\n",
    "    # plotting actual reward\n",
    "    agent = FastOptimalAgent(beta=1.0)\n",
    "    plot_trajectory(walls, reward, start, agent, fig, ax[0])\n",
    "    plot_reward(reward, walls, \"true reward\", fig, ax[0])\n",
    "    # plotting last value map of VIN (last VIN block)\n",
    "    rvs = greatest\n",
    "    rvs = sess.run(feed_dict={'image:0':w, 'reward:0': r}, fetches=rvs.outputs)\n",
    "    rvs = rvs[0][0].squeeze()\n",
    "    plot_reward(normalize(rvs), walls, 'last value map', fig, ax[1])\n",
    "    \n",
    "fig.savefig(\"testing_value_maps.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vtensors = list(sorted(sortable_vtensors))\n",
    "sorted_vtensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, r = repeat_vector(walls, reward, num=20)\n",
    "# Comparing value maps. Rows of 5 value maps. Starting from the first\n",
    "num_cols = 5\n",
    "num_rows = round(len(sortable_vtensors) // num_cols + 1)\n",
    "size_scale = 2\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * size_scale, num_rows * size_scale))\n",
    "\n",
    "\n",
    "gr = grids[2] # or choose grid 3\n",
    "walls, reward, start = GridworldMdp(gr).convert_to_numpy_input()\n",
    "for i, ax_row in enumerate(axes):\n",
    "    for j, ax in enumerate(ax_row):\n",
    "        idx = 5 * i + j\n",
    "        if idx >= len(sortable_vtensors):\n",
    "            # If no more value plots, just plot the trajectory on true reward\n",
    "            agent = FastOptimalAgent(beta=1.0)\n",
    "            plot_trajectory(walls, reward, start, agent, fig, ax)\n",
    "            plot_reward(reward, walls, \"Optimal Agent | True R\", fig, ax)\n",
    "        else:\n",
    "            # Plot value map\n",
    "            opnum, op = sorted_vtensors[5 * i + j]\n",
    "            vmap = sess.run(feed_dict={'image:0':w, 'reward:0': r}, fetches=op.outputs)\n",
    "            vmap = vmap[0][0].squeeze()\n",
    "            plot_reward(normalize(vmap), walls, 'Value Iter: {}'.format(opnum), fig, ax)\n",
    "fig.savefig(\"ValueMapComparison-OptimalAgent-VIN-20.jpg\")\n",
    "            \n",
    "# Plot rollout of the agent on the actual policy..., compared again with the optimal agent\n",
    "# Plot rollout on inferred reward as well... \n",
    "# fig, axes = plt.subplots(1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward(sess, image_data, reward_data, y_data, num_epochs, verbosity, bsize=20):\n",
    "        \"\"\"Infers the reward using backprop, holding the planner fixed.\n",
    "\n",
    "        Due to Tensorflow constraints, image_data must contain exactly\n",
    "        batch_size number of MDPs on which the reward should be inferred.\n",
    "\n",
    "        The rewards are initialized to the values in reward_data. If reward_data\n",
    "        is None, the rewards are initialized to all zeroes.\n",
    "        \"\"\"\n",
    "        from utils import fmt_row\n",
    "        import time\n",
    "        if verbosity >= 3:\n",
    "            print(fmt_row(10, [\"Iteration\", \"Train Cost\", \"Train Err\", \"Iter Time\"]))\n",
    "        if reward_data is None:\n",
    "            reward_data = np.random.randn(*image_data.shape)\n",
    "\n",
    "        batch_size = bsize\n",
    "        num_batches = int(image_data.shape[0] / batch_size)\n",
    "        costs, errs = [], []\n",
    "        for batch_num in range(num_batches):\n",
    "            if verbosity >= 2 and batch_num % 10 == 0:\n",
    "                print('Batch {} of {}'.format(batch_num, num_batches))\n",
    "            start, end = batch_num * batch_size, (batch_num + 1) * batch_size\n",
    "            # We can't feed in reward_data directly to self.reward, because then\n",
    "            # it will treat it as a constant and will not be able to update it\n",
    "            # with backprop. Instead, we first run an op that assigns the\n",
    "            # reward, and only then do the backprop.\n",
    "            fd = {\n",
    "                \"reward_input:0\": reward_data[start:end],\n",
    "            }\n",
    "            assign_reward = reward_var.assign(graph.get_tensor_by_name(\"reward_input:0\"))\n",
    "            sess.run([assign_reward], feed_dict=fd)\n",
    "            print(np.sum(np.ravel(sess.run([]))))\n",
    "\n",
    "            if batch_num % 10 == 0:\n",
    "                costs.append([])\n",
    "                errs.append([])\n",
    "            for epoch in range(num_epochs):\n",
    "                tstart = time.time()\n",
    "                fd = {\n",
    "                    \"image:0\": image_data[start:end],\n",
    "                    \"y:0\": y_data[start:end]\n",
    "                }\n",
    "                _, e_, c_ = sess.run(\n",
    "                    [reward_optimize_op, err, step2_cost],\n",
    "                    feed_dict=fd)\n",
    "                elapsed = time.time() - tstart\n",
    "                if verbosity >= 3 and batch_num % 10 == 0:\n",
    "                    print(fmt_row(10, [epoch, c_, e_, elapsed]))\n",
    "                    costs[-1].append(c_)\n",
    "                    errs[-1].append(e_)\n",
    "\n",
    "            reward_data[start:end] = reward_var.eval(sess)\n",
    "\n",
    "#         logs['train_reward_costs'].append(costs)\n",
    "#         logs['train_reward_errs'].append(errs)\n",
    "        return reward_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_data_for_reward( agent, num_w/oreward)\n",
    "# y_irl = ydata = 4th return of generate data AKA labels\n",
    "imsize = 8\n",
    "num_actions = 5\n",
    "def generate_data_for_reward(mdp, agent):\n",
    "    from utils import Distribution\n",
    "    from gridworld import Direction\n",
    "    def dist_to_numpy(dist):\n",
    "        return dist.as_numpy_array(Direction.get_number_from_direction, num_actions)\n",
    "\n",
    "    def action(state):\n",
    "        # Walls are invalid states and the MDP will refuse to give an action for\n",
    "        # them. However, the VIN's architecture requires it to provide an action\n",
    "        # distribution for walls too, so hardcode it to always be STAY.\n",
    "        x, y = state\n",
    "        if mdp.walls[y][x]:\n",
    "            return dist_to_numpy(Distribution({Direction.STAY : 1}))\n",
    "        return dist_to_numpy(agent.get_action_distribution(state))\n",
    "    \n",
    "    agent.set_mdp(mdp)\n",
    "    action_dists = [[action((x, y)) for x in range(imsize)] for y in range(imsize)]\n",
    "    action_dists = np.array(action_dists)\n",
    "\n",
    "    walls, rewards, start_state = mdp.convert_to_numpy_input()\n",
    "    return action_dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_inferred_on_grid(grid, train_agent, optimal_agent, ax, norm=False, ax_title=None,\n",
    "                              num_epochs=40, verbosity=3):\n",
    "    \"\"\"\n",
    "    Infers the reward for a given grid mdp, the agent the model was trained on, and then plots the trajectory of\n",
    "    the optimal agent on the inferred reward. If normalize is true, we normalize the reward before plotting \n",
    "    trajectory & visualizing.\n",
    "    \n",
    "    Uses `train_agent` to infer the reward, and `optimal_agent` to visualize best path planning with the inferred reward.\n",
    "    \"\"\"\n",
    "    mdp = GridworldMdp(grid)\n",
    "    walls, reward, start = mdp.convert_to_numpy_input()\n",
    "    \n",
    "    agent = FastOptimalAgent(beta=1.0)\n",
    "    y_data = generate_data_for_reward(mdp, train_agent)\n",
    "\n",
    "    repeat_walls, repeat_ys = repeat_vector(walls, y_data)\n",
    "    reward_data = train_reward(sess, repeat_walls, None, repeat_ys, num_epochs, verbosity, bsize=20)\n",
    "    if norm:\n",
    "        reward_data[0] = normalize(reward_data[0])\n",
    "        \n",
    "    if ax_title is None:\n",
    "        ax_title = \"Optimal Agent | Inferred R\"\n",
    "    agent = FastOptimalAgent(beta=None)\n",
    "    plot_trajectory(walls, reward_data[0], start, optimal_agent, fig, ax)\n",
    "    plot_reward(reward_data[0], walls, ax_title, fig, ax)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "opt_agent = FastOptimalAgent(beta=None)\n",
    "train_agent = FastOptimalAgent(beta=1.0)\n",
    "\n",
    "visualize_inferred_on_grid(grids[2], train_agent, opt_agent, axes[0], num_epochs=40)\n",
    "visualize_inferred_on_grid(grids[3], train_agent, opt_agent, axes[1], num_epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying the Visualizations Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vis_grid(grid, train_agent, optimal_agent=None, fname=\"ValueMapComparison.jpg\",\n",
    "                 num_cols=5, size_scale=2):\n",
    "    \"\"\"\n",
    "    Visualizes model's performance on a given grid. Utilizes a saved model constructed in train.py \n",
    "    passed with --savedmodel True.\n",
    "    \n",
    "    Visualizes value maps, then optimal performance on the true reward, optiaml performance on inferred rewards,\n",
    "    demonstrator's performance on the inferred rewards, and optimal performance on values from the model's output\n",
    "    \"\"\"\n",
    "    if optimal_agent is None:\n",
    "        optimal_agent = FastOptimalAgent(beta=1.0)\n",
    "    NUM_LAST = 3\n",
    "    if num_cols < NUM_LAST:\n",
    "        raise ValueError(\"Need at least 3 columns to visualize inferred reward's relationship to true reward\")\n",
    "    # Calculate the number of rows to see every value iteration in the VIN\n",
    "    num_rows = round(len(sortable_vtensors) // num_cols + 1)\n",
    "    # Add an extra row to see the inferred reward & stuff\n",
    "    fig, axes = plt.subplots(num_rows + 1, num_cols, figsize=(num_cols * size_scale, num_rows * size_scale))\n",
    "\n",
    "    mdp = GridworldMdp(grid)\n",
    "    walls, reward, start = mdp.convert_to_numpy_input()\n",
    "    w, r = repeat_vector(walls, reward,num=20)\n",
    "    for i, ax_row in enumerate(axes[:-1]):\n",
    "        for j, ax in enumerate(ax_row):\n",
    "            idx = 5 * i + j\n",
    "            if idx < len(sorted_vtensors):\n",
    "                # Plot value map\n",
    "                opnum, op = sorted_vtensors[5 * i + j]\n",
    "                vmap = sess.run(feed_dict={'image:0':w, 'reward:0': r}, fetches=op.outputs)\n",
    "                vmap = vmap[0][0].squeeze()\n",
    "                plot_reward(normalize(vmap), walls, 'Value Iter: {}'.format(opnum), fig, ax)\n",
    "            else:\n",
    "                ax.set_visible(False)\n",
    "    \n",
    "    last_row = axes[-1, :]\n",
    "    # Demonstrator Agent | True Reward\n",
    "    plot_trajectory(walls, reward, start, train_agent, fig, last_row[0])\n",
    "    plot_reward(reward, walls, \"Demonstrator | True\", fig, last_row[0])\n",
    "    # Optimal Agent | Inferred Reward\n",
    "    visualize_inferred_on_grid(grid, train_agent, optimal_agent, last_row[1], ax_title=\"Optimal | Inferred\")\n",
    "    # Train Agent | Inferred Reward\n",
    "#     visualize_inferred_on_grid(grid, train_agent, train_agent, last_row[2], ax_title='Demonstrator | Inferred')\n",
    "    # Demonstrator | Model's Values\n",
    "    vals = plot_models_best_trajectory(mdp, train_agent, fig, last_row[2])\n",
    "    plot_reward(vals, walls, \"Demonstrator | Model\", fig, last_row[2])\n",
    "    for ax in last_row[NUM_LAST:]:\n",
    "        ax.set_visible(False)\n",
    "    fig.savefig(fname, dpi=150)\n",
    "\n",
    "def plot_models_best_trajectory(mdp, demonstrator, fig, ax, bsize=20, optimal_base_agent=None):\n",
    "    \"\"\"\n",
    "    Uses optimal agent to plan using the model's values.\n",
    "    \"\"\"\n",
    "    from mdp_interface import Mdp\n",
    "    from agent_runner import run_agent\n",
    "    from utils import plot_pos, plot_lines\n",
    "    EPISODE_LENGTH = 30\n",
    "    arrow_width = 0.5\n",
    "    animate=False\n",
    "    fname=None\n",
    "    if optimal_base_agent is None:\n",
    "        optimal_base_agent = FastOptimalAgent(beta=None)\n",
    "    \n",
    "    walls, reward, start = mdp.convert_to_numpy_input()\n",
    "    imsize = len(walls)\n",
    "    w, r = repeat_vector(walls, reward, num=20)\n",
    "    vmap = sess.run(feed_dict={'image:0': w, 'reward:0':r}, fetches=graph.get_tensor_by_name('output:0'))\n",
    "    # Transposing bc internal values for the agent are transposed\n",
    "    vmap = np.argmax(vmap, -1).reshape((bsize, imsize, imsize))[0].T\n",
    "    \n",
    "    # Performing rollout\n",
    "    demonstrator.set_mdp(mdp)\n",
    "    # Override the value computation\n",
    "    demonstrator.values = vmap\n",
    "    env = Mdp(mdp)\n",
    "    trajectory = run_agent(demonstrator, env, episode_length=EPISODE_LENGTH)\n",
    "\n",
    "    if len(trajectory) <= 1:\n",
    "        raise ValueError(\"Trajectory rolled out unsuccessfully\")\n",
    "\n",
    "    # Tuples of (state, next) - to be used for plotting\n",
    "    state_trans = [(info[0], info[2]) for info in trajectory]\n",
    "    count = 0\n",
    "    for trans in state_trans:\n",
    "        if trans[0] == trans[1]:\n",
    "            count += 1\n",
    "    if count == len(state_trans):\n",
    "        print(\"Yes, the agent given stayed in the same spot for {} iterations...\".format(len(state_trans)))\n",
    "\n",
    "    if fig is None or ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    if ax is not None and type(ax) is list:\n",
    "        raise ValueError(\"Given {} axes, but can only use 1 axis\".format(len(ax)))\n",
    "\n",
    "    # Plot starting point\n",
    "    plot_pos(start, ax=ax, color='k', marker='o', grid_size=imsize)\n",
    "    # Plot ending trajectory point\n",
    "    finish = state_trans[-1][0]\n",
    "    plot_pos(finish, ax=ax, color='k', marker='*', grid_size=imsize)\n",
    "    plot_lines(ax, fig, trans_list=state_trans, color='black', arrow_width=arrow_width, grid_size=imsize,\n",
    "               animate=animate, fname=fname)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_tensor_by_name('output:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent = FastOptimalAgent(beta=1.0)\n",
    "plot_vis_grid(grids[2], train_agent, fname='ValueMapComparison-gr1-b.jpg')\n",
    "plot_vis_grid(grids[3], train_agent, fname='ValueMapComparison-gr2-b.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying that the model is properly calculating the error / policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridworldMdp(grids[2])\n",
    "walls, reward, start = mdp.convert_to_numpy_input()\n",
    "\n",
    "c\n",
    "\n",
    "w, r = repeat_vector(np.zeros_like(walls), np.ones_like(reward))\n",
    "ys = repeat_vector(dists, dists)[0]\n",
    "fd = {\n",
    "    \"image:0\": w,\n",
    "    \"reward_input:0\":r,\n",
    "    \"y:0\": ys\n",
    "}\n",
    "sess.run([err], feed_dict=fd)\n",
    "# Why is the error so high on this?? Shouldn't it have learned better??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_opt = tf.train.RMSPropOptimizer(1e-6).minimize(\n",
    "    logits_cost, name=\"train-adam\", var_list=graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boink = graph.get_tensor_by_name(\"output:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "police = sess.run([boink], feed_dict=fd)[0]\n",
    "police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "police[:, 0].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run([tf.global_variables_initializer()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run([train_opt, err, step2_cost], feed_dict=fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying that plotting the model values works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_agent = FastOptimalAgent(beta=1.0)\n",
    "mdp = GridworldMdp(grids[2])\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "walls, reward, start = mdp.convert_to_numpy_input()\n",
    "plot_trajectory(walls, reward, start, optimal_agent, fig, axes[0])\n",
    "plot_reward(reward, walls, \"Normal\", fig, axes[0])\n",
    "\n",
    "optimal_agent = FastOptimalAgent(beta=1.0)\n",
    "vmap = plot_models_best_trajectory(mdp, optimal_agent, fig, axes[1])\n",
    "plot_reward(vmap, walls, \"Hijacked\", fig, axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL",
   "language": "python",
   "name": "irl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
