{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% mv /Users/codetesting/Downloads/benchmark.zip benchmark.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(\"benchmark_data/index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedder = pd.read_csv(\"benchmark_data/header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.columns = hedder.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_frame = summary.loc[summary[' performance'] != ' None']\n",
    "completed_frame = completed_frame.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of successful calls\n",
    "\n",
    "Algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_frame[' algorithm'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_frame[' model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not used\n",
    "useful = [' algorithm', ' model', ' final_accuracy', ' performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column called 'perf' with the float values of the ' perfomance' column\n",
    "compare_success = completed_frame.loc[:, (' performance')].apply(lambda x: float(x.split()[0]))\n",
    "completed_frame['perf'] = compare_success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_by_col(pdframe, colname, val):\n",
    "    \"\"\"Gets the rows where the colname meets some value\"\"\"\n",
    "    return pdframe.loc[pdframe[colname]==val]\n",
    "\n",
    "def get_nontrivial_columns(pdframe):\n",
    "    \"\"\"Gets the nontrivial columns from the pdframe\"\"\"\n",
    "    nontrivial = []\n",
    "    for column in pdframe.columns:\n",
    "        if pdframe[column].unique().size > 1:\n",
    "            nontrivial.append(column)\n",
    "    return nontrivial\n",
    "\n",
    "def get_trans_col(frame, grouped, func_name, colname):\n",
    "    \"\"\"Gets the row with the value given by grouped.transform(func_name)\"\"\"\n",
    "    idx = grouped.transform(func_name) == frame[colname]\n",
    "    return frame.loc[idx]\n",
    "\n",
    "def print_basic_stats(pdframe, group_col, colname):\n",
    "    \"\"\"Prints basic statistics about grouped columns\n",
    "    \n",
    "    i.e. conditions underwhich max & min are achieved\n",
    "    \n",
    "    group_col –– the column we want to groupby (e.g. ' model')\n",
    "    colname   –– the values we want to compare (e.g. 'perf')\n",
    "    \"\"\"\n",
    "    pdgrouped = pdframe.groupby(group_col)[colname]\n",
    "    max_row = get_trans_col(pdframe, pdgrouped, 'max', colname)\n",
    "    min_row = get_trans_col(pdframe, pdgrouped, 'min', colname)\n",
    "    \n",
    "    print(\"\\nMean performance of models is: \")\n",
    "    print(pdgrouped.mean())\n",
    "    print(\"\\nStd performance of models is: \")\n",
    "    print(pdgrouped.std())\n",
    "    print(\"\\nMax performance of models is: \")\n",
    "    print(max_row)\n",
    "    print(\"\\nMin performance of models is: \")\n",
    "    print(min_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing: given_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_rewards = get_rows_by_col(completed_frame, ' algorithm', ' given_rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nontrivial columns of given_rewards, and compare by performance\n",
    "cols = get_nontrivial_columns(given_rewards)\n",
    "print_basic_stats(given_rewards[cols], ' model', 'perf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing: no_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rewards = get_rows_by_col(completed_frame, ' algorithm', ' no_rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = get_nontrivial_columns(no_rewards)\n",
    "print_basic_stats(no_rewards[cols], ' model', 'perf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing: boltzmann_planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_planner = get_rows_by_col(completed_frame, ' algorithm', ' boltzmann_planner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = get_nontrivial_columns(b_planner)\n",
    "print_basic_stats(b_planner[cols], ' model', 'perf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing: vi_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_alg = get_rows_by_col(completed_frame, ' algorithm', ' vi_inference')\n",
    "cols = get_nontrivial_columns(vi_alg)\n",
    "print_basic_stats(vi_alg[cols], ' model', 'perf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SUSA]",
   "language": "python",
   "name": "conda-env-SUSA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
