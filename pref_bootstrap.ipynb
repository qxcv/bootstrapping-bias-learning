{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "INFO:root:Occupancy measure error@iter   0: 2.346383 (||params||=3.100423, ||grad||=2.998609, ||E[dr/dw]||=5.389757)\n",
      "INFO:root:Occupancy measure error@iter  100: 0.013800 (||params||=3.935119, ||grad||=0.016888, ||E[dr/dw]||=6.946326)\n",
      "INFO:root:Occupancy measure error@iter  200: 0.004269 (||params||=3.913445, ||grad||=0.005684, ||E[dr/dw]||=6.939804)\n",
      "INFO:root:Occupancy measure error@iter  300: 0.002089 (||params||=3.918314, ||grad||=0.002720, ||E[dr/dw]||=6.940283)\n",
      "INFO:root:Occupancy measure error@iter  400: 0.001010 (||params||=3.921670, ||grad||=0.001334, ||E[dr/dw]||=6.940668)\n"
     ]
    }
   ],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 4.6748e-02 1.2728e-03 0.0000e+00]\n",
      " [0.0000e+00 6.8535e+00 1.0985e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "\n",
      "Recovered state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 4.7744e-02 1.3822e-03 0.0000e+00]\n",
      " [0.0000e+00 6.8532e+00 1.0977e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "irl_bias_params, rng = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (IRL): -15.31452751159668\n",
      "Gradient w.r.t. reward params (IRL):\n",
      " [ 0.      0.      0.      0.      0.     -0.0431 -0.057   0.      0.      0.0872 -0.0634  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (IRL):\n",
      " [ 0.      0.      0.      0.      0.     -0.0423  0.0072  0.      0.     -0.4394 -0.1398  0.\n",
      "  0.      0.      0.      0.    ]\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (IRL):', float(irl_feedback_model.log_likelihood(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. reward params (IRL):\\n', np.asarray(irl_feedback_model.log_likelihood_grad_rew(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. bias params (IRL):\\n', irl_feedback_model.log_likelihood_grad_bias(irl_dataset, rmodel, irl_bias_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example with the paired comparison learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "pc_bias_params, rng = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (PC): -1.2642439603805542\n",
      "Gradient w.r.t. reward params (PC):\n",
      " [ 0.      0.      0.      0.      0.      0.4411 -0.5471  0.      0.      0.563  -0.457   0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (PC):\n",
      " -1.7073208\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (PC):', float(pc_feedback_model.log_likelihood(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. reward params (PC):\\n', np.asarray(pc_feedback_model.log_likelihood_grad_rew(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. bias params (PC):\\n', pc_feedback_model.log_likelihood_grad_bias(comparison_dataset, rmodel, pc_bias_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
