{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "from pref_bootstrap import priors\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "INFO:root:Occupancy measure error@iter   0: 5.996717 (||params||=4.294651, ||grad||=7.298931, ||E[dr/dw]||=4.522275)\n",
      "INFO:root:Occupancy measure error@iter  100: 0.072167 (||params||=5.001379, ||grad||=0.089312, ||E[dr/dw]||=6.967020)\n",
      "INFO:root:Occupancy measure error@iter  200: 0.020320 (||params||=5.023421, ||grad||=0.025931, ||E[dr/dw]||=6.912886)\n",
      "INFO:root:Occupancy measure error@iter  300: 0.013011 (||params||=5.128381, ||grad||=0.016747, ||E[dr/dw]||=6.914132)\n",
      "INFO:root:Occupancy measure error@iter  400: 0.009003 (||params||=5.204486, ||grad||=0.011775, ||E[dr/dw]||=6.915234)\n",
      "INFO:root:Occupancy measure error@iter  500: 0.006521 (||params||=5.261590, ||grad||=0.008705, ||E[dr/dw]||=6.915982)\n",
      "INFO:root:Occupancy measure error@iter  600: 0.004858 (||params||=5.305677, ||grad||=0.006656, ||E[dr/dw]||=6.916512)\n",
      "INFO:root:Occupancy measure error@iter  700: 0.003685 (||params||=5.340435, ||grad||=0.005218, ||E[dr/dw]||=6.916900)\n",
      "INFO:root:Occupancy measure error@iter  800: 0.002825 (||params||=5.368277, ||grad||=0.004174, ||E[dr/dw]||=6.917192)\n",
      "INFO:root:Occupancy measure error@iter  900: 0.002179 (||params||=5.390864, ||grad||=0.003398, ||E[dr/dw]||=6.917416)\n"
     ]
    }
   ],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 1.4471e-01 2.4406e-04 0.0000e+00]\n",
      " [0.0000e+00 6.8427e+00 1.0124e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "\n",
      "Recovered state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 1.4324e-01 1.3432e-03 0.0000e+00]\n",
      " [0.0000e+00 6.8414e+00 1.0141e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (IRL): -16.119115829467773\n",
      "Gradient w.r.t. reward params (IRL):\n",
      " [ 0.      0.      0.      0.      0.     -0.1888 -0.51    0.      0.      1.4123 -1.5606  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (IRL):\n",
      " [  0.       0.       0.       0.       0.       0.0797   0.7014   0.       0.     -14.0017  -3.7943\n",
      "   0.       0.       0.       0.       0.    ]\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (IRL):', float(irl_feedback_model.log_likelihood(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. reward params (IRL):\\n', np.asarray(irl_feedback_model.log_likelihood_grad_rew(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. bias params (IRL):\\n', irl_feedback_model.log_likelihood_grad_bias(irl_dataset, rmodel, irl_bias_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example with the paired comparison learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (PC): -1.7770366668701172\n",
      "Gradient w.r.t. reward params (PC):\n",
      " [ 0.      0.      0.      0.      0.     -0.9277 -1.0753  0.      0.      1.3023  0.7007  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (PC):\n",
      " -2.5096657\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (PC):', float(pc_feedback_model.log_likelihood(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. reward params (PC):\\n', np.asarray(pc_feedback_model.log_likelihood_grad_rew(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. bias params (PC):\\n', pc_feedback_model.log_likelihood_grad_bias(comparison_dataset, rmodel, pc_bias_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we can create a combined demo in which we use both MCE IRL and paired comparisons to learn a reward model and set of modality-specific bias models.\n",
    "\n",
    "Ideas for ergonomic improvements:\n",
    "\n",
    "- Incorporate reward priors on the `RewardModel` class, just as we have for bias priors and `EnvFeedbackModel`.\n",
    "- Make parameter storage consistent between reward models and feedback models. Either feedback models should store parameters (instead of keeping them external), or reward models should keep parameters external (instead of storing them as class attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(sam): try finding a step size with backtracking line search, rather\n",
    "# than using a fixed step. Also consider using a Nesterov step.\n",
    "def joint_training(env, feedback_models, all_datasets, all_bias_params,\n",
    "                   rmodel, rprior, init_step_size=1e-3, n_steps=100):\n",
    "    \"\"\"Jointly trains a single reward model and a set of feedback bias models\n",
    "    on a given environment. Trains reward model with gradient descent, and\n",
    "    individual bias models with projected gradient descent.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Environment): environment to train on.\n",
    "        feedback_models ([EnvFeedbackModel]): list of environment feedback\n",
    "            models.\n",
    "        all_datasets (list): list of datasets for the feedback models\n",
    "            (`all_datasets[i]` is the dataset for `feedback_models[i]`).\n",
    "        all_bias_params (list): list of bias parameters for feedback models.\n",
    "            (`all_bias_params[i]` contains bias parameters for\n",
    "            `feedback_models[i]`).\n",
    "        rmodel (RewardModel): reward model, assumed to be shared across all\n",
    "            feedback modalities.\n",
    "        rprior (Prior): prior on reward model parameters.\n",
    "        init_step_size (float): length of the first step. Step size scales\n",
    "            linearly down to (just above) zero over the course of training.\n",
    "        n_steps (int): total number of steps to take.\n",
    "        \n",
    "    Returns: tuple of `(rmodel, all_bias_params)`, where `rmodel` is the same\n",
    "        model passed into the functino (this is updated in-place), and\n",
    "        `all_bias_params` is a set of updated bias parameters (these are not\n",
    "        updated in-place).\"\"\"\n",
    "    assert len(feedback_models) == len(all_datasets)\n",
    "    assert len(feedback_models) == len(all_bias_params)\n",
    "    all_bias_params = list(all_bias_params)\n",
    "\n",
    "    for step_num in range(n_steps):\n",
    "        # progress_10 is 1 at the start of training, and linear drops to\n",
    "        # (just above) 0 at the end (after n_steps)\n",
    "        progress_10 = 1 - step_num / n_steps\n",
    "        step_size = init_step_size * progress_10 \n",
    "\n",
    "        # first do a joint reward model update w/ plain gradient descent\n",
    "        rmodel_grad = np.zeros_like(rmodel.get_params())\n",
    "        model_ds_bias_iter = zip(feedback_models, all_datasets, all_bias_params)\n",
    "        for model, dataset, bias_params in model_ds_bias_iter:\n",
    "            rmodel_grad += model.log_likelihood_grad_rew(\n",
    "                dataset, rmodel, bias_params)\n",
    "        rmodel_grad += rprior.log_prior_grad(rmodel.get_params())\n",
    "        rmodel.set_params(rmodel.get_params() + step_size * rmodel_grad)\n",
    "\n",
    "        # now do individual bias model updates\n",
    "        model_ds_bias_enum = enumerate(zip(\n",
    "            feedback_models, all_datasets, all_bias_params))\n",
    "        for model_idx, (model, dataset, bias_params) in model_ds_bias_enum:\n",
    "            # Projected gradient descent: we take a gradient descent step, then\n",
    "            # project back to the support of the prior.\n",
    "            bias_param_grad = model.log_likelihood_grad_bias(\n",
    "                dataset, rmodel, bias_params)\n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            bias_grad = bias_param_grad + bias_prior_grad\n",
    "            unproj_step = all_bias_params[model_idx] + step_size * bias_grad\n",
    "            proj_step = model.bias_prior.project_to_support(bias_grad)\n",
    "            all_bias_params[model_idx] = proj_step\n",
    "\n",
    "    return rmodel, all_bias_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jt_feedback_models = [irl_feedback_model, pc_feedback_model]\n",
    "jt_datasets = [irl_dataset, comparison_dataset]\n",
    "rng, jt_irl_feedback_params = irl_feedback_model.init_bias_params(rng)\n",
    "rng, jt_pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "jt_bias_params = [jt_irl_feedback_params, jt_pc_bias_params]\n",
    "jt_rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "jt_rprior = priors.FixedGaussianPrior(\n",
    "    mean=0.0, std=1.0, shape=rmodel.get_params().shape)\n",
    "jt_rmodel, jt_bias_params = joint_training(\n",
    "    env=env, feedback_models=jt_feedback_models, all_datasets=jt_datasets,\n",
    "    all_bias_params=jt_bias_params, rmodel=jt_rmodel, rprior=jt_rprior, \n",
    "    init_step_size=1e-3, n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
