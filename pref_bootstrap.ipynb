{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax.experimental.optimizers as jopt\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_reward_learning():\n",
    "    # QUESTIONS:\n",
    "    #\n",
    "    # - What if I want to share rationality models over environments with different reward functions?\n",
    "    # - What if I want to share rationality models between different feedback modalities?\n",
    "    #   (like I'm sharing reward models)\n",
    "    # - What if I want to create hierarchical reward or rationality models? (e.g. have a pseudo-prior\n",
    "    #   that is shared between different rationality models, to keep them tied together)\n",
    "    #\n",
    "    # Seems like it at least makes sense to have the same treatment for rationality model params &\n",
    "    # reward model params, so we can share them between models.\n",
    "    while True:\n",
    "        # do reward update\n",
    "        grad_accum = np.zeros_like(reward_parameters)\n",
    "        for algo in algos:\n",
    "            grad_accum += algo.grad_wrt_reward(reward_parameters)\n",
    "        parameters += grad_accum\n",
    "    for algo in algos:\n",
    "        algo.grad_wrt_reward(reward_parameters)\n",
    "\n",
    "# REMARK: these all seem heavily tied to modality. e.g. from an implementation\n",
    "# perspective, it probably makes sense to group a reward model, bias model,\n",
    "# and associated priors together for each modality.\n",
    "reward_model_priors = [reward_prior1(), reward_prior2(), ...]\n",
    "reward_models = [reward_model_model1(reward_model_priors[i]), reward_model_model2(reward_model_priors[j]), ...]\n",
    "bias_model_priors = [bias_prior1(), bias_prior2(), ...]\n",
    "bias_mdoels = [bias_model1(bias_model_priors[k]), bias_model2(bias_model_priors[l]), ...]\n",
    "behaviour_datasets = [collect_dataset_1(), collect_dataset_2(), ...]\n",
    "\n",
    "# Now do GD on log likelihood of behaviour datasets!\n",
    "# Just need some way of associating a bias model with each of them.\n",
    "# Maybe just pass in a dict? IDK.\n",
    "# Other things I might need:\n",
    "# - Computing the actual value of each term in the log posterior. This will be\n",
    "#   useful for line search, and is probably not too much effort. Not sure\n",
    "#   whether line search really matters for this task, though, since values are\n",
    "#   almost as expensive to compute as gradients in IRL (both need planning).\n",
    "# - Evaluating the joint Hessian for all parameters. Unfortunately doing so is\n",
    "#   going to be rather complicated, so it might make sense to skip it for this\n",
    "#   project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(5, 5, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = mce_irl.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
