{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALRIGHT, HERE IS WHERE I WILL DEBUG THE TOPK FEEDBACK LEARNER\n",
    "# ABLATIONS: COULD WE DO BETTER IF ALL OUR TRAJECTORIES ARE GOOD? \n",
    "# OR IS IT NECESSARY TO HAVE BAD TRAJECTORIES? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def genereate_top_K_dataset(n_traj): \n",
    "import random\n",
    "\n",
    "n_traj = 1000\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rmodel.set_params(env.reward_matrix)\n",
    "traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "labels= top_K_expert.interact(traj, rmodel)\n",
    "\n",
    "labels_final, traj_final = [], []\n",
    "for l, t in zip(labels, traj['states']): \n",
    "    if not(l):\n",
    "        if True:#random.random() < (np.sum(labels)/len(labels)):\n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "    else: \n",
    "        labels_final.append(l), traj_final.append(t)\n",
    "        \n",
    "labels_final = np.array(labels_final)\n",
    "        \n",
    "print(np.sum(labels)/len(labels))\n",
    "print(np.sum(labels_final)/len(labels_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_dataset(n_traj):\n",
    "    # def genereate_top_K_dataset(n_traj): \n",
    "    import random\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    rmodel.set_params(env.reward_matrix)\n",
    "    traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "    labels= top_K_expert.interact(traj, rmodel)\n",
    "\n",
    "    labels_final, traj_final = [], []\n",
    "    for l, t in zip(labels, traj['states']): \n",
    "        if not(l):\n",
    "            if random.random() < top_K_expert.K:#TODO if there are issues update this so that we balance our dataset.\n",
    "                labels_final.append(l), traj_final.append(t)\n",
    "        else: \n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "\n",
    "    labels_final = np.array([int(l) for l in labels_final])\n",
    "    \n",
    "    return {\n",
    "        'trajectories':np.array(traj_final), \n",
    "        'labels':labels_final\n",
    "    }\n",
    "\n",
    "top_K_dataset = generate_topk_dataset(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "steps = 1000\n",
    "loss_prev = float('Inf')\n",
    "delta = 100\n",
    "eps = 1e-5\n",
    "\n",
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "rng = jrandom.PRNGKey(23)\n",
    "rng, top_K_bias_params = top_K_feedback_model.init_bias_params(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(point, direction, eval_fn, *, init_step=1.0, armijo_c=0.05, armijo_tau=0.1, min_step=1e-5):\n",
    "    \"\"\"Performs backtracking line search until Armijo condition is satisfied.\n",
    "    Assumes function is continuously differentiable (which should be true\n",
    "    as long as we stay away from L1 penalties, hard maxes, etc.).\"\"\"\n",
    "    value_at_point = eval_fn(point)\n",
    "    armijo_satisfied = False\n",
    "    t = -armijo_c * jnp.dot(point, direction)\n",
    "    num_steps = 0\n",
    "    best_step_size = init_step\n",
    "    value_for_best_step_size = float('inf')\n",
    "    while not armijo_satisfied:\n",
    "        # compute step size and update num_steps\n",
    "        step_size = init_step * armijo_tau ** num_steps\n",
    "        num_steps += 1\n",
    "        \n",
    "        # break early if we're running for suspiciously long\n",
    "        if step_size < min_step:\n",
    "            print(\n",
    "                f\"Backtracked to step size {step_size} after {num_steps} iterations, \"\n",
    "                \"but Armijo is still not satisfied; exiting early\")\n",
    "            break\n",
    "\n",
    "        # compute new value & Armijo condition\n",
    "        value_at_step = eval_fn(point - step_size * direction)\n",
    "        armijo_satisfied = value_at_point - value_at_step >= step_size * t\n",
    "        \n",
    "        # we keep the 'best' step size in case we need to exit early (this is a bit of hack, but oh well)\n",
    "        if value_at_step < value_for_best_step_size:\n",
    "            value_for_best_step_size = value_at_step\n",
    "            best_step_size = step_size\n",
    "\n",
    "    return best_step_size\n",
    "\n",
    "def trad_optimize(model, data, rmodel, bias_params, use_bias_prior=False, eps=1e-9, optimize_bias=True,\n",
    "                  one_bias=False, auto_backtrack=False, iters=1000, lr=1e-2, armijo_kwargs=None,\n",
    "                  polyak_momentum_mu=0.0):\n",
    "    \"\"\"\n",
    "    Note: Going to add back the projected G.D\n",
    "    I'm not really sure why he was trying to optimize the probability of the bias terms under the bias prior, \n",
    "    i don't really think that makes much sense. You'll basically just push it towards the mean terms even\n",
    "    thought its not really there...\n",
    "    \"\"\"\n",
    "    steps = iters\n",
    "    loss_prev = float('Inf')\n",
    "    delta = 100\n",
    " \n",
    "    step = 0\n",
    "    old_r = rmodel.get_params()  # we track this for Polyak momentum\n",
    "    \n",
    "    while(step<steps):\n",
    "        grew = model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "        if auto_backtrack:\n",
    "            # backtrack the step size until Armijo's condition is satisfied\n",
    "            def rew_eval_fn(rew_params):\n",
    "                old = rmodel.get_params()\n",
    "                rmodel.set_params(rew_params)\n",
    "                value = model.log_likelihood(data, rmodel, bias_params)\n",
    "                rmodel.set_params(old)\n",
    "                return float(value)\n",
    "            if armijo_kwargs is None:\n",
    "                armijo_kwargs = dict()\n",
    "            rew_step_size = backtracking_line_search(rmodel.get_params(),\n",
    "                                                     grew, rew_eval_fn,\n",
    "                                                     init_step=lr,\n",
    "                                                     **armijo_kwargs)\n",
    "        else:\n",
    "            rew_step_size = lr\n",
    "        current_r = rmodel.get_params()\n",
    "        new_r = current_r + rew_step_size*grew\n",
    "        new_r_momentum = grew + polyak_momentum_mu * (current_r - old_r)\n",
    "        old_r = current_r\n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        gbias = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "       \n",
    "        if use_bias_prior and optimize_bias: \n",
    "            print('using bias prior')\n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            print(gbias, bias_prior_grad, bias_params)\n",
    "            gbias = gbias+bias_prior_grad\n",
    "        if optimize_bias: \n",
    "            bias_params = bias_params+lr*gbias\n",
    "        elif (not optimize_bias) and one_bias: \n",
    "            bias_params = jnp.ones_like(bias_params) # TODO: \n",
    "        \n",
    "        if use_bias_prior: \n",
    "            bias_params = model.bias_prior.project_to_support(bias_params)\n",
    "        \n",
    "        loss = model.log_likelihood(data, rmodel, bias_params)\n",
    "        if step % 100 == 0:\n",
    "            print('step %d loss %.3f' %(step, loss))\n",
    "        if step > 0: \n",
    "            delta = loss-loss_prev\n",
    "        loss_prev = loss\n",
    "        step +=1\n",
    "\n",
    "    if np.abs(delta) <= .0001: \n",
    "        print('terminated due to delta')\n",
    "    else: \n",
    "        print('terminated due to steps exceeding %d' %steps)\n",
    "        \n",
    "    return model, rmodel, bias_params\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "_, rmodel, _ = trad_optimize(\n",
    "    top_K_feedback_model, top_K_dataset, rmodel, top_K_bias_params, optimize_bias=False,\n",
    "    auto_backtrack=False, lr=1e-2, polyak_momentum_mu=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rmodel):\n",
    "    _, topk_om = mce_irl.mce_occupancy_measures(env, R=rmodel.get_params())\n",
    "\n",
    "    visited_states, = np.nonzero((optimal_om > 1e-5) | (topk_om > 1e-5))\n",
    "    plt.plot(visited_states, rmodel.get_params()[visited_states], label='estimated')\n",
    "    plt.plot(visited_states, env.reward_matrix[visited_states], label='real reward')\n",
    "    plt.legend()\n",
    "    plt.title('Estimated Reward Function')\n",
    "\n",
    "    print('Optimal state visitation frequencies for each grid cell:')\n",
    "    print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "\n",
    "    print('Inferred ')\n",
    "    print(topk_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "    \n",
    "evaluate(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "_, rmodel, _ = trad_optimize(pc_feedback_model, comparison_dataset, rmodel, pc_bias_params, use_bias_prior=False, \n",
    "                            optimize_bias=False, one_bias=True, iters=1000,\n",
    "                            auto_backtrack=False, lr=1.0, polyak_momentum_mu=0.0)\n",
    "evaluate(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "_, rmodel, _ = trad_optimize(\n",
    "    irl_feedback_model, irl_dataset, rmodel, irl_bias_params, use_bias_prior=True,\n",
    "    optimize_bias=False, one_bias=True, iters=1000,\n",
    "    auto_backtrack=True, lr=1, polyak_momentum_mu=0.0)\n",
    "evaluate(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_optimize(model_list, data_list, rmodel, bias_list, use_bias_list, optimize_reward=True): \n",
    "    \n",
    "    lr = 1\n",
    "    steps = 1000\n",
    "    loss_prev = float('Inf')\n",
    "    delta = 100\n",
    "    step = 0\n",
    "    \n",
    "    while(step<steps):\n",
    "    \n",
    "        grew = jnp.zeros_like(env.reward_matrix)\n",
    "        \n",
    "        if optimize_reward: \n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                grew += model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "\n",
    "        new_r = rmodel.get_params() + lr*grew\n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        for k, (model, data, bias_params, use_bias_prior) in enumerate(zip(model_list, data_list, bias_list, use_bias_list)):\n",
    "            gbias = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "            if use_bias_prior: \n",
    "                bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "                gbias = gbias + bias_prior_grad\n",
    "        \n",
    "            bias_list[k] = bias_params + lr*(gbias)\n",
    "            \n",
    "                    \n",
    "            if use_bias_prior: \n",
    "                bias_list[k] = model.bias_prior.project_to_support(bias_list[k])\n",
    "\n",
    "\n",
    "        for k, (model, data, bias_params) in enumerate(zip(model_list, data_list, bias_list)):\n",
    "            loss = model.log_likelihood(data, rmodel, bias_params)\n",
    "          \n",
    "            if step % 100 == 0:\n",
    "                print('step %d loss %.3f model %d' %(step, loss, k))\n",
    "                print('---', bias_params)\n",
    "        if step > 0: \n",
    "            delta = loss-loss_prev\n",
    "        loss_prev = loss\n",
    "        step +=1\n",
    "        \n",
    "    return model_list, rmodel, bias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitializing all the models\n",
    "\n",
    "def init_models(opt_reward=False):\n",
    "    irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    rng = jrandom.PRNGKey(42)\n",
    "    rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "    irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "    # we'll do IRL based on 10 trajectories\n",
    "    irl_dataset = irl_expert.interact(20)\n",
    "\n",
    "    pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "    rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "    pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "    # generate some random trajectories & compare a random subset of them\n",
    "    def generate_comparison_dataset(pc_ntraj):\n",
    "        pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "        to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "        comparisons = []\n",
    "        for first_idx in range(pc_ntraj):\n",
    "            second_idx = np.random.randint(pc_ntraj - 1)\n",
    "            if second_idx >= first_idx:\n",
    "                second_idx += 1\n",
    "            traj1_is_better = pc_expert.interact(\n",
    "                dict(states=pc_trajectories['states'][first_idx]),\n",
    "                dict(states=pc_trajectories['states'][second_idx]))\n",
    "            if traj1_is_better:\n",
    "                # the better trajectory comes before the worse one\n",
    "                comparisons.append((first_idx, second_idx))\n",
    "            else:\n",
    "                comparisons.append((second_idx, first_idx))\n",
    "        return {\n",
    "            'trajectories': pc_trajectories,\n",
    "            'comparisons': np.asarray(comparisons),\n",
    "        }\n",
    "\n",
    "    comparison_dataset = generate_comparison_dataset(20)\n",
    "\n",
    "    top_K_expert = experts.TopKExpert(env, temp=.3, K=.01, seed=42)\n",
    "    def generate_topk_dataset(n_traj):\n",
    "        # def genereate_top_K_dataset(n_traj): \n",
    "        import random\n",
    "        rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "        traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "        labels= top_K_expert.interact(traj, rmodel)\n",
    "\n",
    "        labels_final, traj_final = [], []\n",
    "        for l, t in zip(labels, traj['states']): \n",
    "            if not(l):\n",
    "                if random.random() < top_K_expert.K:#TODO if there are issues update this so that we balance our dataset.\n",
    "                    labels_final.append(l), traj_final.append(t)\n",
    "            else: \n",
    "                labels_final.append(l), traj_final.append(t)\n",
    "\n",
    "        labels_final = np.array([int(l) for l in labels_final])\n",
    "\n",
    "        return {\n",
    "            'trajectories':np.array(traj_final), \n",
    "            'labels':labels_final\n",
    "        }\n",
    "\n",
    "    top_K_dataset = generate_topk_dataset(1000)\n",
    "\n",
    "    import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "    top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    rng = jrandom.PRNGKey(23)\n",
    "    rng, top_K_bias_params = top_K_feedback_model.init_bias_params(rng)\n",
    "\n",
    "    model_list = [top_K_feedback_model, pc_feedback_model, irl_feedback_model]\n",
    "    data_list = [top_K_dataset, comparison_dataset, irl_dataset]\n",
    "    bias_list = [top_K_bias_params, pc_bias_params, irl_bias_params]\n",
    "    use_bias_list = [False, True, True]\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    if opt_reward:\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "        \n",
    "    names = ['top_K', 'paired_comparisons', 'blind_irl']\n",
    "    \n",
    "    biases_actual = []\n",
    "    \n",
    "    # TOP K biases \n",
    "    biases_actual.append((top_K_expert.temp, top_K_expert.cutoff))\n",
    "    \n",
    "    # pc actual bias\n",
    "    biases_actual.append((pc_expert.boltz_temp))\n",
    "    \n",
    "    #blind irl actual bias\n",
    "    biases_actual.append((irl_bias_params))\n",
    "        \n",
    "    return model_list, data_list, rmodel, bias_list, use_bias_list, names, biases_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, data_list, rmodel, bias_list, use_bias_list, names, _ = init_models(opt_reward=True)\n",
    "models, rmodel, biases = multi_optimize(model_list, data_list, rmodel, bias_list, use_bias_list, optimize_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(biases[-1], label=\"recovered\")\n",
    "plt.plot(irl_bias_params, label='Actual')\n",
    "plt.legend()\n",
    "plt.title('Blind IRL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of rewards and labels: \n",
    "def topK_dist(top_K_dataset): \n",
    "    states = top_K_dataset['trajectories']\n",
    "    flat_states = states.flatten()\n",
    "    all_fn_values = rmodel.get_params() #(self.env.observation_matrix)\n",
    "    rew_est = (all_fn_values[flat_states]) # hopefully jax can do this, if not...need 1-hot.\n",
    "    per_obs_rew  = jnp.reshape(rew_est, states.shape[:2] + rew_est.shape[1:])\n",
    "    per_traj_rew_est = jnp.sum(per_obs_rew, axis=1)\n",
    "    return per_traj_rew_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1. Method comparison. \n",
    "model_list = [top_K_feedback_model, pc_feedback_model, irl_feedback_model]\n",
    "data_list = [top_K_dataset, comparison_dataset, irl_dataset]\n",
    "bias_list = [top_K_bias_params, pc_bias_params, irl_bias_params]\n",
    "use_bias_list = [False, True, True]\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rmodel.set_params(env.reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_rew(trajs, rmodel): \n",
    "    states = trajs['states']\n",
    "    flat_states = states.flatten()\n",
    "    all_fn_values = rmodel\n",
    "    rew_est = (all_fn_values[flat_states]) # hopefully jax can do this, if not...need 1-hot.\n",
    "    per_obs_rew  = jnp.reshape(rew_est, states.shape[:2] + rew_est.shape[1:])\n",
    "    per_traj_rew_est = jnp.sum(per_obs_rew, axis=1)\n",
    "    return np.mean(per_traj_rew_est)\n",
    "\n",
    "def evaluate_full(rmodel): \n",
    "    _, om = mce_irl.mce_occupancy_measures(env, R=rmodel.get_params())\n",
    "    trajs = mce_irl.mce_irl_sample(env, 100, R=rmodel.get_params())\n",
    "    rews = get_rew(trajs, rmodel.get_params())\n",
    "\n",
    "    \n",
    "    return rmodel.get_params(), rews, om"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases_recovered, biases_actual, recovered_reward, mean_reward_obtained, obs_obtained, method, fold = [],[],[],[],[],[], []\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(3): \n",
    "\n",
    "    # eval each method\n",
    "        # reset reward_model\n",
    "\n",
    "    model_list, data_list, rmodel, bias_list, use_bias_list, names, biases_actual = init_models()\n",
    "    use_bias_list = [False, False, True]\n",
    "    obs = [False, True, True]\n",
    "    for model, dataset, bias_params, ub, name, b_actual, ob in zip(model_list, data_list, bias_list, use_bias_list, names, biases_actual, obs):     \n",
    "        rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "        _, rmodel, bias_p_recovered = trad_optimize(model, dataset, rmodel, bias_params, use_bias_prior=ub, \n",
    "                                                   optimize_bias=False, one_bias=ob, iters=2000)\n",
    "#         assert (bias_p_recovered == bias_params).all()\n",
    "\n",
    "        recovered_reward_vec, mean_reward, obs_f = evaluate_full(rmodel)\n",
    "\n",
    "        biases_recovered.append(bias_p_recovered)\n",
    "        biases_actual.append(b_actual)\n",
    "        recovered_reward.append(recovered_reward_vec)\n",
    "        mean_reward_obtained.append(mean_reward)\n",
    "        obs_obtained.append(obs_f)\n",
    "        method.append(name)\n",
    "        fold.append(_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel.set_params(env.reward_matrix)\n",
    "r, mean_rew_opt, obs_f = evaluate_full(rmodel)\n",
    "obs_optimal = [obs_f]*len(method)\n",
    "optimal_rew = [mean_rew_opt]*len(method)\n",
    "real_rew = [r]*len(method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(biases_recovered), len(real_rew), len(recovered_reward), len(mean_reward_obtained), \n",
    "     len(obs_obtained), len(optimal_rew), len(biases_actual))\n",
    "res = pd.DataFrame({\n",
    "    'biases_recoverd':biases_recovered, \n",
    "    'recovered_reward':recovered_reward,\n",
    "    'mean_reward_obtained':mean_reward_obtained,\n",
    "    'obs_obtained':obs_obtained,\n",
    "    'obs_optimal':obs_optimal, \n",
    "    'optimal_rew':optimal_rew, \n",
    "    'method':method\n",
    "})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='method', y='mean_reward_obtained', data=res)\n",
    "plt.axhline(22, label='optimal rew')\n",
    "plt.legend()\n",
    "plt.savefig('./images/init_results_linear_modded.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, m in zip(res['recovered_reward'].values, res['method'].values): \n",
    "    print(m)\n",
    "    def evaluate(rmodel):\n",
    "        plt.plot(rmodel, label='estimated')\n",
    "        plt.plot(env.reward_matrix, label='real reward')\n",
    "        plt.legend()\n",
    "        plt.title('Estimated Reward Function')\n",
    "\n",
    "        _, topk_om = mce_irl.mce_occupancy_measures(env, R=rmodel)\n",
    "\n",
    "        print('Optimal state visitation frequencies for each grid cell:')\n",
    "        print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "\n",
    "        print('Inferred ')\n",
    "        print(topk_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "    \n",
    "    evaluate(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "\n",
    "res.to_pickle('./results/no_bias_updates_%s.pkl' %ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in set(list(res['method'].values)):\n",
    "    obs = res.loc[res['method']==m]['obs_obtained']\n",
    "    obs = obs.values\n",
    "    res_L = []\n",
    "    for o in obs: \n",
    "        res_L.append(o)\n",
    "        plt.imshow(o.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "        plt.title(m)\n",
    "        plt.show()\n",
    "    res_l = np.array(res_L)\n",
    "    res_l = np.median(res_l, axis=0)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
