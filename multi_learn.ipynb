{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def genereate_top_K_dataset(n_traj): \n",
    "import random\n",
    "\n",
    "n_traj = 1000\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rmodel.set_params(env.reward_matrix)\n",
    "traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "labels= top_K_expert.interact(traj, rmodel)\n",
    "\n",
    "labels_final, traj_final = [], []\n",
    "for l, t in zip(labels, traj['states']): \n",
    "\n",
    "    if not(l):\n",
    "        if True:#random.random() < (np.sum(labels)/len(labels)):\n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "    else: \n",
    "        labels_final.append(l), traj_final.append(t)\n",
    "        \n",
    "labels_final = np.array(labels_final)\n",
    "        \n",
    "print(np.sum(labels)/len(labels))\n",
    "print(np.sum(labels_final)/len(labels_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_dataset(n_traj):\n",
    "    # def genereate_top_K_dataset(n_traj): \n",
    "    import random\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    rmodel.set_params(env.reward_matrix)\n",
    "    traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "    labels= top_K_expert.interact(traj, rmodel)\n",
    "\n",
    "    labels_final, traj_final = [], []\n",
    "    for l, t in zip(labels, traj['states']): \n",
    "        if not(l):\n",
    "            if random.random() < top_K_expert.K:#TODO if there are issues update this so that we balance our dataset.\n",
    "                labels_final.append(l), traj_final.append(t)\n",
    "        else: \n",
    "            labels_final.append(l), traj_final.append(t)\n",
    "\n",
    "    labels_final = np.array([int(l) for l in labels_final])\n",
    "    \n",
    "    return {\n",
    "        'trajectories':np.array(traj_final), \n",
    "        'labels':labels_final\n",
    "    }\n",
    "\n",
    "top_K_dataset = generate_topk_dataset(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "steps = 1000\n",
    "loss_prev = float('Inf')\n",
    "delta = 100\n",
    "eps = 1e-5\n",
    "\n",
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.3, K=.05, seed=42)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "rng = jrandom.PRNGKey(23)\n",
    "rng, top_K_bias_params = top_K_feedback_model.init_bias_params(rng)\n",
    "\n",
    "top_K_bias_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trad_optimize(model, data, rmodel, bias_params, use_bias_prior=False, \n",
    "                  eps=1e-9, optimize_bias=True,one_bias=False, iters=1000, lr=1e-2, printz=100):\n",
    "    \"\"\"\n",
    "    Note: Going to add back the projected G.D\n",
    "    I'm not really sure why he was trying to optimize the probability of the bias terms under the bias prior, \n",
    "    i don't really think that makes much sense. You'll basically just push it towards the mean terms even\n",
    "    thought its not really there...\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    steps = iters\n",
    "    loss_prev = float('Inf')\n",
    "    delta = 100\n",
    " \n",
    "    step = 0\n",
    "    while(step<steps):\n",
    "\n",
    "        grew = model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "        new_r = rmodel.get_params() + lr*grew\n",
    "        \n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        gbias = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "       \n",
    "        if use_bias_prior and optimize_bias: \n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            gbias = gbias+bias_prior_grad\n",
    "        if optimize_bias: \n",
    "            bias_params = bias_params+lr*gbias\n",
    "        elif (not optimize_bias) and one_bias: \n",
    "            bias_params = jnp.ones_like(bias_params) # TODO: \n",
    "        \n",
    "        if use_bias_prior: \n",
    "            bias_params = model.bias_prior.project_to_support(bias_params)\n",
    "        \n",
    "        loss = model.log_likelihood(data, rmodel, bias_params)\n",
    "        if step % printz == 0:\n",
    "            print('step %d loss %.3f' %(step, loss))\n",
    "        if step > 0: \n",
    "            delta = loss-loss_prev\n",
    "        loss_prev = loss\n",
    "        step +=1\n",
    "\n",
    "        \n",
    "\n",
    "    if np.abs(delta) <= .0001: \n",
    "        print('terminated due to delta')\n",
    "    else: \n",
    "        print('terminated due to steps exceeding %d' %steps)\n",
    "        \n",
    "    return model, rmodel, bias_params\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "\n",
    "_, rmodel, _ = trad_optimize(top_K_feedback_model, top_K_dataset, rmodel, top_K_bias_params, optimize_bias=True, \n",
    "                            use_bias_prior=True, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rmodel, name):\n",
    "\n",
    "\n",
    "    _, topk_om = mce_irl.mce_occupancy_measures(env, R=rmodel.get_params())\n",
    "\n",
    "    print('Optimal state visitation frequencies for each grid cell:')\n",
    "    print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "\n",
    "    print('Inferred ')\n",
    "    print(topk_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "    \n",
    "    visited_states = np.nonzero((optimal_om > 1e-5) | (topk_om > 1e-5))[0]\n",
    "    plt.plot(visited_states, rmodel.get_params()[visited_states], label='est')\n",
    "    plt.plot(visited_states, env.reward_matrix[visited_states], label='real_reward')\n",
    "    plt.legend()\n",
    "    plt.title('Estimated Reward Function %s' %name)\n",
    "    plt.savefig('./images/reward_%s.png' %name)\n",
    "    \n",
    "evaluate(rmodel, name='Top-K feedback model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "_, rmodel, _ = trad_optimize(pc_feedback_model, comparison_dataset, rmodel, pc_bias_params, use_bias_prior=True, \n",
    "                            optimize_bias=True, one_bias=False)\n",
    "evaluate(rmodel, name='Paired Comparisons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)\n",
    "_, rmodel, _ = trad_optimize(irl_feedback_model, irl_dataset, rmodel, irl_bias_params, use_bias_prior=True, optimize_bias=True, \n",
    "                            one_bias=False)\n",
    "evaluate(rmodel, name='Blind IRL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pref_bootstrap import feedback_learner_scalar as fbl_scalar\n",
    "s_feedback_model = fbl_scalar.ScalarFeedbackModel(env)\n",
    "\n",
    "rng, s_feedback_params = s_feedback_model.init_bias_params(rng)\n",
    "s_expert = experts.ScalarFeedbackExpert(env, seed=3)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_corrupted_ds(ntraj=20):\n",
    "    trajectories = mce_irl.mce_irl_sample(env, ntraj, R=np.ones((env.n_states, )))\n",
    "    return s_expert.interact(trajectories)\n",
    "s_dataset = generate_corrupted_ds(20)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "_, rmodel, _ = trad_optimize(s_feedback_model, s_dataset, rmodel, s_feedback_params, use_bias_prior=False, \n",
    "                            optimize_bias=True, one_bias=False, iters=200)\n",
    "evaluate(rmodel, name='Scalar feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_optimize(model_list, data_list, rmodel, bias_list, use_bias_list, optimize_reward=True, \n",
    "                   optimize_bias=True, one_bias=False): \n",
    "    \n",
    "    lr = 1e-2\n",
    "    steps = 1000\n",
    "    loss_prev = float('Inf')\n",
    "    delta = 100\n",
    "    step = 0\n",
    "    \n",
    "    while(step<steps):\n",
    "    \n",
    "        grew = jnp.zeros_like(env.reward_matrix)\n",
    "        \n",
    "        if optimize_reward: \n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                grew += model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "        new_r = rmodel.get_params() + lr*grew\n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        for k, (model, data, bias_params, use_bias_prior) in enumerate(zip(model_list, data_list, bias_list, use_bias_list)):\n",
    "            gbias = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "            if use_bias_prior and optimize_bias: \n",
    "                bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "                gbias = gbias+bias_prior_grad\n",
    "            if optimize_bias: \n",
    "                bias_params = bias_params + lr*gbias\n",
    "            elif (not optimize_bias) and one_bias: \n",
    "                bias_params = jnp.ones_like(bias_params) # TODO: \n",
    "                \n",
    "            if use_bias_prior: \n",
    "                bias_params = model.bias_prior.project_to_support(bias_params)\n",
    "            bias_list[k] = bias_params\n",
    "            \n",
    "       \n",
    "            \n",
    "        for k, (model, data, bias_params) in enumerate(zip(model_list, data_list, bias_list)):\n",
    "            loss = model.log_likelihood(data, rmodel, bias_params)\n",
    "          \n",
    "            if step % 100 == 0:\n",
    "                print('step %d loss %.3f model %d' %(step, loss, k))\n",
    "                print('---', bias_params)\n",
    "        if step > 0: \n",
    "            delta = loss-loss_prev\n",
    "        loss_prev = loss\n",
    "        step +=1\n",
    "        \n",
    "    return model_list, rmodel, bias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitializing all the models\n",
    "\n",
    "def init_models(opt_reward=False):\n",
    "    irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    rng = jrandom.PRNGKey(42)\n",
    "    rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "    irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "    # we'll do IRL based on 10 trajectories\n",
    "    irl_dataset = irl_expert.interact(20)\n",
    "\n",
    "    pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "    rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "    pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "    # generate some random trajectories & compare a random subset of them\n",
    "    def generate_comparison_dataset(pc_ntraj):\n",
    "        pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "        to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "        comparisons = []\n",
    "        for first_idx in range(pc_ntraj):\n",
    "            second_idx = np.random.randint(pc_ntraj - 1)\n",
    "            if second_idx >= first_idx:\n",
    "                second_idx += 1\n",
    "            traj1_is_better = pc_expert.interact(\n",
    "                dict(states=pc_trajectories['states'][first_idx]),\n",
    "                dict(states=pc_trajectories['states'][second_idx]))\n",
    "            if traj1_is_better:\n",
    "                # the better trajectory comes before the worse one\n",
    "                comparisons.append((first_idx, second_idx))\n",
    "            else:\n",
    "                comparisons.append((second_idx, first_idx))\n",
    "        return {\n",
    "            'trajectories': pc_trajectories,\n",
    "            'comparisons': np.asarray(comparisons),\n",
    "        }\n",
    "\n",
    "    comparison_dataset = generate_comparison_dataset(20)\n",
    "\n",
    "    top_K_expert = experts.TopKExpert(env, temp=.3, K=.01, seed=42)\n",
    "    def generate_topk_dataset(n_traj):\n",
    "        # def genereate_top_K_dataset(n_traj): \n",
    "        import random\n",
    "        rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "        traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "        labels= top_K_expert.interact(traj, rmodel)\n",
    "\n",
    "        labels_final, traj_final = [], []\n",
    "        for l, t in zip(labels, traj['states']): \n",
    "            if not(l):\n",
    "                if random.random() < top_K_expert.K:#TODO if there are issues update this so that we balance our dataset.\n",
    "                    labels_final.append(l), traj_final.append(t)\n",
    "            else: \n",
    "                labels_final.append(l), traj_final.append(t)\n",
    "\n",
    "        labels_final = np.array([int(l) for l in labels_final])\n",
    "\n",
    "        return {\n",
    "            'trajectories':np.array(traj_final), \n",
    "            'labels':labels_final\n",
    "        }\n",
    "\n",
    "    top_K_dataset = generate_topk_dataset(1000)\n",
    "\n",
    "    import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "    top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    rng = jrandom.PRNGKey(23)\n",
    "    rng, top_K_bias_params = top_K_feedback_model.init_bias_params(rng)\n",
    "    \n",
    "    \n",
    "    \n",
    "    from pref_bootstrap import feedback_learner_scalar as fbl_scalar\n",
    "    s_feedback_model = fbl_scalar.ScalarFeedbackModel(env)\n",
    "\n",
    "    rng, s_feedback_params = s_feedback_model.init_bias_params(rng)\n",
    "    s_expert = experts.ScalarFeedbackExpert(env, seed=3)\n",
    "\n",
    "    # generate some random trajectories & compare a random subset of them\n",
    "    def generate_corrupted_ds(ntraj=20):\n",
    "        trajectories = mce_irl.mce_irl_sample(env, ntraj, R=np.ones((env.n_states, )))\n",
    "        return s_expert.interact(trajectories)\n",
    "    s_dataset = generate_corrupted_ds(20)\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "#     _, rmodel, _ = trad_optimize(s_feedback_model, s_dataset, rmodel, s_feedback_params, use_bias_prior=False, \n",
    "#                                 optimize_bias=True, one_bias=False, iters=200)\n",
    "#     evaluate(rmodel, name='Scalar feedback')\n",
    "\n",
    "    model_list = [top_K_feedback_model, pc_feedback_model, irl_feedback_model, s_feedback_model]\n",
    "    data_list = [top_K_dataset, comparison_dataset, irl_dataset, s_dataset]\n",
    "    bias_list = [top_K_bias_params, pc_bias_params, irl_bias_params, s_feedback_params]\n",
    "    use_bias_list = [True, True, True, True]\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    if opt_reward:\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "        \n",
    "    names = ['top_K', 'paired_comparisons', 'blind_irl', 'scalar_feedback']\n",
    "    \n",
    "    biases_actual = []\n",
    "    \n",
    "    # TOP K biases \n",
    "    biases_actual.append((top_K_expert.temp, top_K_expert.cutoff))\n",
    "    \n",
    "    # pc actual bias\n",
    "    biases_actual.append((pc_expert.boltz_temp))\n",
    "    \n",
    "    #blind irl actual bias\n",
    "    biases_actual.append((irl_bias_params))\n",
    "    \n",
    "    biases_actual.append((0, 1))\n",
    "        \n",
    "    return model_list, data_list, rmodel, bias_list, use_bias_list, names, biases_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, data_list, rmodel, bias_list, use_bias_list, names, _ = init_models(opt_reward=True)\n",
    "# models, rmodel, biases = multi_optimize(model_list, data_list, rmodel, bias_list, use_bias_list, optimize_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(biases[-1], label=\"recovered\")\n",
    "# plt.plot(irl_bias_params, label='Actual')\n",
    "plt.legend()\n",
    "plt.title('Blind IRL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of rewards and labels: \n",
    "def topK_dist(top_K_dataset): \n",
    "    states = top_K_dataset['trajectories']\n",
    "    flat_states = states.flatten()\n",
    "    all_fn_values = rmodel.get_params() #(self.env.observation_matrix)\n",
    "    rew_est = (all_fn_values[flat_states]) # hopefully jax can do this, if not...need 1-hot.\n",
    "    per_obs_rew  = jnp.reshape(rew_est, states.shape[:2] + rew_est.shape[1:])\n",
    "    per_traj_rew_est = jnp.sum(per_obs_rew, axis=1)\n",
    "    return per_traj_rew_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1. Method comparison. \n",
    "model_list = [top_K_feedback_model, pc_feedback_model, irl_feedback_model]\n",
    "data_list = [top_K_dataset, comparison_dataset, irl_dataset]\n",
    "bias_list = [top_K_bias_params, pc_bias_params, irl_bias_params]\n",
    "use_bias_list = [False, True, True]\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rmodel.set_params(env.reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_rew(trajs, rmodel): \n",
    "    states = trajs['states']\n",
    "    flat_states = states.flatten()\n",
    "    all_fn_values = rmodel\n",
    "    rew_est = (all_fn_values[flat_states]) # hopefully jax can do this, if not...need 1-hot.\n",
    "    per_obs_rew  = jnp.reshape(rew_est, states.shape[:2] + rew_est.shape[1:])\n",
    "    per_traj_rew_est = jnp.sum(per_obs_rew, axis=1)\n",
    "    return np.mean(per_traj_rew_est)\n",
    "\n",
    "def evaluate_full(rmodel): \n",
    "    _, om = mce_irl.mce_occupancy_measures(env, R=rmodel.get_params())\n",
    "    trajs = mce_irl.mce_irl_sample(env, 100, R=rmodel.get_params())\n",
    "    rews = get_rew(trajs, rmodel.get_params())\n",
    "\n",
    "    \n",
    "    return rmodel.get_params(), rews, om\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases_recovered, biases_actual, recovered_reward, mean_reward_obtained, obs_obtained, method, fold = [],[],[],[],[],[], []\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(3): \n",
    "\n",
    "    # eval each method\n",
    "        # reset reward_model\n",
    "\n",
    "    model_list, data_list, rmodel, bias_list, use_bias_list, names, biases_actual = init_models()\n",
    "    use_bias_list = [True, True, True, True]\n",
    "    obs = [True, True, True, True]\n",
    "    iterz = [1000, 1000, 1000, 200]\n",
    "    for model, dataset, bias_params, ub, name, b_actual, ob, it in zip(model_list, data_list, bias_list, use_bias_list, names, biases_actual, obs, iterz):     \n",
    "        rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "        _, rmodel, bias_p_recovered = trad_optimize(model, dataset, rmodel, bias_params, use_bias_prior=ub, \n",
    "                                                   optimize_bias=ob, one_bias=False, printz=10, lr=1e-3,\n",
    "                                                   iters=it)\n",
    "\n",
    "        recovered_reward_vec, mean_reward, obs_f = evaluate_full(rmodel)\n",
    "\n",
    "        biases_recovered.append(bias_p_recovered)\n",
    "        biases_actual.append(b_actual)\n",
    "        recovered_reward.append(recovered_reward_vec)\n",
    "        mean_reward_obtained.append(mean_reward)\n",
    "        obs_obtained.append(obs_f)\n",
    "        method.append(name)\n",
    "        fold.append(_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmodel.set_params(env.reward_matrix)\n",
    "r, mean_rew_opt, obs_f = evaluate_full(rmodel)\n",
    "obs_optimal = [obs_f]*len(method)\n",
    "optimal_rew = [mean_rew_opt]*len(method)\n",
    "real_rew = [r]*len(method)\n",
    "\n",
    "res = pd.DataFrame({\n",
    "    'biases_recoverd':biases_recovered,\n",
    "    'recovered_reward':recovered_reward,\n",
    "    'mean_reward_obtained':mean_reward_obtained,\n",
    "    'obs_obtained':obs_obtained,\n",
    "    'obs_optimal':obs_optimal, \n",
    "    'optimal_rew':optimal_rew, \n",
    "    'method':method\n",
    "})\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "ts = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "res.to_pickle('./results/single_model_res_%s.pkl' %ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "res = pd.read_pickle('./results/single_model_res_%s.pkl' %ts)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='method', y='mean_reward_obtained', data=res)\n",
    "plt.axhline(optimal_rew[0], label='optimal rew')\n",
    "plt.legend()\n",
    "plt.ylim([0, optimal_rew[0]+1])\n",
    "plt.savefig('./images/final-results-optimize-bias_%s.png' %ts)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for r, m in zip(res['recovered_reward'].values, res['method'].values): \n",
    "    print(m)\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    rmodel.set_params(r)\n",
    "    evaluate(rmodel, name=m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "for m in set(list(res['method'].values)):\n",
    "    obs = res.loc[res['method']==m]['obs_obtained']\n",
    "    obs = obs.values\n",
    "    res_L = []\n",
    "    for o in obs: \n",
    "        res_L.append(o)\n",
    "        plt.imshow(o.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "        plt.title(m)\n",
    "        plt.show()\n",
    "    res_l = np.array(res_L)\n",
    "    res_l = np.median(res_l, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases_recovered, biases_actual, recovered_reward, mean_reward_obtained, obs_obtained, method, fold = [],[],[],[],[],[], []\n",
    "\n",
    "for _ in range(3): \n",
    "\n",
    "    # eval each method\n",
    "        # reset reward_model\n",
    "\n",
    "    model_list, data_list, rmodel, bias_list, use_bias_list, names, biases_actual = init_models()\n",
    "    use_bias_list = [True, True, True]\n",
    "    obs = [True, True, True]\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    _, rmodel, bias_p_recovered = multi_optimize(model_list, data_list, rmodel, bias_list, use_bias_list,\n",
    "                                               optimize_bias=True, one_bias=False)\n",
    "\n",
    "    recovered_reward_vec, mean_reward, obs_f = evaluate_full(rmodel)\n",
    "\n",
    "    biases_recovered.append(bias_p_recovered)\n",
    "    biases_actual.append(b_actual)\n",
    "    recovered_reward.append(recovered_reward_vec)\n",
    "    mean_reward_obtained.append(mean_reward)\n",
    "    obs_obtained.append(obs_f)\n",
    "    method.append(name)\n",
    "    fold.append(_)\n",
    "    \n",
    "print(len(biases_recovered), len(real_rew), len(recovered_reward), len(mean_reward_obtained), \n",
    "     len(obs_obtained), len(optimal_rew), len(biases_actual))\n",
    "\n",
    "rmodel.set_params(env.reward_matrix)\n",
    "r, mean_rew_opt, obs_f = evaluate_full(rmodel)\n",
    "obs_optimal = [obs_f]*len(method)\n",
    "optimal_rew = [mean_rew_opt]*len(method)\n",
    "real_rew = [r]*len(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({\n",
    "    'biases_recoverd':biases_recovered,\n",
    "    'recovered_reward':recovered_reward,\n",
    "    'mean_reward_obtained':mean_reward_obtained,\n",
    "    'obs_obtained':obs_obtained,\n",
    "    'obs_optimal':obs_optimal, \n",
    "    'optimal_rew':optimal_rew, \n",
    "    'method':['combined' for m in method]\n",
    "})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "ts = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "sns.barplot(x='method', y='mean_reward_obtained', data=res)\n",
    "plt.axhline(optimal_rew[0], label='optimal rew')\n",
    "plt.legend()\n",
    "plt.ylim([0, optimal_rew[0]+4])\n",
    "plt.savefig('./images/final-results-multi_train_%s.png' %ts)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for r, m in zip(res['recovered_reward'].values, res['method'].values): \n",
    "    print(m)\n",
    "    \n",
    "#     evaluate(r, name=m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "res.to_pickle('./results/full_multi_train_%s.pkl' %ts)\n",
    "\n",
    "for m in set(list(res['method'].values)):\n",
    "    obs = res.loc[res['method']==m]['obs_obtained']\n",
    "    obs = obs.values\n",
    "    res_L = []\n",
    "    for o in obs: \n",
    "        res_L.append(o)\n",
    "        plt.imshow(o.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "        plt.title(m)\n",
    "        plt.show()\n",
    "    res_l = np.array(res_L)\n",
    "    res_l = np.median(res_l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare shared bias  params vs the other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single = pd.read_pickle('/userdata/smetzger/repos/cvx_project/bootstrapping-bias-learning/results/single_model_res_2020_12_16_09_49_03.pkl')\n",
    "\n",
    "df_single = pd.read_pickle('/userdata/smetzger/repos/cvx_project/bootstrapping-bias-learning/results/single_model_res_2020_12_16_10_54_52.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared = pd.read_pickle('/userdata/smetzger/repos/cvx_project/bootstrapping-bias-learning/results/full_multi_train_2020_12_16_11_14_54.pkl')\n",
    "# df_shared = pd.read_pickle('/userdata/smetzger/repos/cvx_project/bootstrapping-bias-learning/results/full_multi_train_2020_12_16_10_10_16.pkl')\n",
    "#pd.read_pickle('/userdata/smetzger/repos/cvx_project/bootstrapping-bias-learning/results/full_multi_train_2020_12_07_13_57_28.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_pickle('/userdata/smetzger/repos/cvx_project/bootstrapping-bias-learning/results/scalar_feedback_res_2020_12_07_23_13_09.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endv_single = df_single['biases_recoverd'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endv_shared = []\n",
    "g = df_shared['biases_recoverd'].values\n",
    "for gg in g:\n",
    "    endv_shared.extend(gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endv_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endv_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endv_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = np.array(endv_shared[::4])\n",
    "\n",
    "b = top_K_expert.bias\n",
    "print(b.shape)\n",
    "b = list(np.array(b))\n",
    "actual = np.array(b + [top_K_expert.cutoff])\n",
    "print(shares.shape, actual.shape)\n",
    "b_err_shared = np.linalg.norm(shares-np.array(actual), axis=-1)\n",
    "print(b_err_shared.shape)\n",
    "endv_single[::4]\n",
    "singles = endv_single[::4]\n",
    "singles = np.array([s for s in singles])\n",
    "\n",
    "\n",
    "b_err_single = np.linalg.norm(singles-np.array(actual), axis=-1)\n",
    "print(np.linalg.norm(shares-np.array(actual), axis=-1))\n",
    "sns.barplot(x='Optimization Scheme', y='Bias Error', data = pd.DataFrame({'Optimization Scheme':['Single']*3 + ['Shared']*3,\n",
    "                                                                         'Bias Error': np.concatenate((b_err_single, b_err_shared), axis=0)\n",
    "                                                                        }))\n",
    "plt.title('Top-K bias parameter error')\n",
    "plt.savefig('./images/shared_bias_params_topK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = np.array(endv_shared[1::4])\n",
    "\n",
    "actual = (1.0)\n",
    "b_err_shared = np.linalg.norm(np.array(shares)-np.array(actual), axis=-1)\n",
    "endv_single[1::4]\n",
    "singles = endv_single[1::4]\n",
    "singles = np.array([s for s in singles])\n",
    "\n",
    "b_err_single = np.linalg.norm(singles-np.array(actual), axis=-1)\n",
    "print(b_err_single, b_err_shared)\n",
    "sns.barplot(x='Optimization Scheme', y='Bias Error', data = pd.DataFrame({'Optimization Scheme':['Single']*3 + ['Shared']*3,\n",
    "                                                                         'Bias Error': np.concatenate((b_err_single,\n",
    "                                                                                                       b_err_shared), axis=0)\n",
    "                                                                        }))\n",
    "plt.title('Paired Comparisons bias parameter error')\n",
    "plt.savefig('./images/shared_bias_params_topK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = np.array(endv_shared[3::4])\n",
    "\n",
    "actual = (1.0)\n",
    "b_err_shared = np.linalg.norm(np.array(shares)-np.array(actual), axis=-1)\n",
    "endv_single[3::4]\n",
    "singles = endv_single[3::4]\n",
    "singles = np.array([s for s in singles])\n",
    "\n",
    "b_err_single = np.linalg.norm(singles-np.array(actual), axis=-1)\n",
    "print(b_err_single, b_err_shared)\n",
    "sns.barplot(x='Optimization Scheme', y='Bias Error', data = pd.DataFrame({'Optimization Scheme':['Single']*3 + ['Shared']*3,\n",
    "                                                                         'Bias Error': np.concatenate((b_err_single,\n",
    "                                                                                                       b_err_shared), axis=0)\n",
    "                                                                        }))\n",
    "plt.title('Paired Comparisons bias parameter error')\n",
    "plt.savefig('./images/shared_bias_params_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(env.reward_matrix.reshape(4, 4))\n",
    "plt.title('Simple Gridworld')\n",
    "plt.savefig('./images/simple_gridworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single\n",
    "df_single = df_single.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single['inf norm, visitation'] = df_single['obs_obtained'] - df_single['obs_optimal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single['inf norm, visitation'] = df_single['inf norm, visitation'].apply(lambda x: np.linalg.norm(np.array(x), ord=np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='method', y='inf norm, visitation', data = df_single)\n",
    "plt.title('Visitation Frequency Comparison')\n",
    "plt.savefig('./images/visitation_freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared['inf norm, visitation'] = df_shared['obs_obtained'] - df_shared['obs_optimal']\n",
    "df_shared['inf norm, visitation'] = df_shared['inf norm, visitation'].apply(lambda x: np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_combo = df_single.append(df_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='method', y='inf norm, visitation', data = df_combo)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/vistation_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='method', y='mean_reward_obtained', data=df_combo)\n",
    "plt.axhline(df_combo['optimal_rew'].values[0], label='MCE IRL mean r')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Obtained Reward With Recovered Policy')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./images/reward_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "for k in range(3):\n",
    "    plt.subplot(150+k+1)\n",
    "    plt.imshow(df_single['obs_obtained'].values[k+6].reshape(4, 4))\n",
    "    plt.title(['Top K', 'Paired Comparisons', 'Blind IRL'][k])\n",
    "    if k == 0: \n",
    "        plt.xlabel('State X')\n",
    "        plt.ylabel('State Y')\n",
    "plt.subplot(150+4)\n",
    "plt.imshow(df_single['obs_obtained'].values[-2].reshape(4, 4))\n",
    "plt.title('Scalar Feedback')\n",
    "if k == 0: \n",
    "    plt.xlabel('State X')\n",
    "    plt.ylabel('State Y')\n",
    "plt.subplot(155)\n",
    "plt.imshow(df_single['obs_optimal'].values[3].reshape(4, 4))\n",
    "plt.title('Optimal (MCE IRL)')\n",
    "plt.savefig('./images/vistation_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(df_single['obs_optimal'].values[3].reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
