{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface, avse_world\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(point, direction, eval_fn, *, init_step=1.0, armijo_c=0.05, armijo_tau=0.1, min_step=1e-5):\n",
    "    \"\"\"Performs backtracking line search until Armijo condition is satisfied.\n",
    "    Assumes function is continuously differentiable (which should be true\n",
    "    as long as we stay away from L1 penalties, hard maxes, etc.).\"\"\"\n",
    "    value_at_point = eval_fn(point)\n",
    "    armijo_satisfied = False\n",
    "    t = -armijo_c * jnp.dot(point, direction)\n",
    "    num_steps = 0\n",
    "    best_step_size = init_step\n",
    "    value_for_best_step_size = float('inf')\n",
    "    while not armijo_satisfied:\n",
    "        # compute step size and update num_steps\n",
    "        step_size = init_step * armijo_tau ** num_steps\n",
    "        num_steps += 1\n",
    "        \n",
    "        # break early if we're running for suspiciously long\n",
    "        if step_size < min_step:\n",
    "            print(\n",
    "                f\"Backtracked to step size {step_size} after {num_steps} iterations, \"\n",
    "                \"but Armijo is still not satisfied; exiting early\")\n",
    "            break\n",
    "\n",
    "        # compute new value & Armijo condition\n",
    "        value_at_step = eval_fn(point - step_size * direction)\n",
    "        armijo_satisfied = value_at_point - value_at_step >= step_size * t\n",
    "        \n",
    "        # we keep the 'best' step size in case we need to exit early (this is a bit of hack, but oh well)\n",
    "        if value_at_step < value_for_best_step_size:\n",
    "            value_for_best_step_size = value_at_step\n",
    "            best_step_size = step_size\n",
    "\n",
    "    return best_step_size\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj, env, pc_expert):\n",
    "    \"\"\"Generates a paired comparisons dataset by generating n_traj trajectories\n",
    "    and then doing a single comparison (with a random other trajectory) for\n",
    "    each generated trajectory.\"\"\"\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "def multi_optimize(model_list, data_list, rmodel, bias_list, *, optimize_reward=True): \n",
    "    \"\"\"Joint optimization of several models at once.\"\"\"\n",
    "    lr = 1\n",
    "    steps = 1000\n",
    "    loss_prev = float('Inf')\n",
    "    delta = 100\n",
    "    step = 0\n",
    "    \n",
    "    for step in range(steps):\n",
    "        grew = jnp.zeros_like(env.reward_matrix)\n",
    "        \n",
    "        if optimize_reward: \n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                grew += model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "\n",
    "        new_r = rmodel.get_params() + lr*grew\n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        for k, (model, data, bias_params) in enumerate(zip(model_list, data_list, bias_list)):\n",
    "            gbias = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "            # bias prior\n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            gbias = gbias + bias_prior_grad\n",
    "        \n",
    "            bias_list[k] = bias_params + lr*(gbias)\n",
    "            bias_list[k] = model.bias_prior.project_to_support(bias_list[k])\n",
    "\n",
    "        for k, (model, data, bias_params) in enumerate(zip(model_list, data_list, bias_list)):\n",
    "            loss = model.log_likelihood(data, rmodel, bias_params)\n",
    "          \n",
    "            if step % 100 == 0:\n",
    "                print('step %d loss %.3f model %d' % (step, loss, k))\n",
    "                print('---', bias_params)\n",
    "        if step > 0: \n",
    "            delta = loss - loss_prev\n",
    "        loss_prev = loss\n",
    "\n",
    "    return model_list, rmodel, bias_list\n",
    "\n",
    "def init_models(env, irl_ntraj, pc_ntraj, *, dummy_use_opt_reward=False):\n",
    "    \"\"\"Initialize paired comparisons & IRL models.\"\"\"\n",
    "    irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    rng = jrandom.PRNGKey(42)\n",
    "    rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "    irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "    # we'll do IRL based on 10 trajectories\n",
    "    irl_dataset = irl_expert.interact(irl+ntraj)\n",
    "\n",
    "    pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "    rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "    pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "    comparison_dataset = generate_comparison_dataset(pc_ntraj)\n",
    "\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "\n",
    "    rng = jrandom.PRNGKey(23)\n",
    "\n",
    "    model_list = [pc_feedback_model, irl_feedback_model]\n",
    "    data_list = [comparison_dataset, irl_dataset]\n",
    "    bias_list = [pc_bias_params, irl_bias_params]\n",
    "    use_bias_list = [True, True]\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "    if dummy_use_opt_reward:\n",
    "        print(\"WARNING: Initializing with optimal reward parameters!\", file=sys.stderr)\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "\n",
    "    names = ['paired_comparisons', 'blind_irl']\n",
    "\n",
    "    biases_actual = [pc_expert.boltz_temp, irl_bias_params]\n",
    "\n",
    "    # pc actual bias\n",
    "    biases_actual.append(pc_expert.boltz_temp)\n",
    "    \n",
    "    # blind irl actual bias\n",
    "    biases_actual.append((irl_bias_params))\n",
    "        \n",
    "    return model_list, data_list, rmodel, bias_list, use_bias_list, names, biases_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the \"vase world\" environment\n",
    "vw_env = vase_world.VaseWorld()\n",
    "sns.heatmap(vw_env.reward_matrix.reshape((vw_env.n_rows, vw_env.n_cols)))\n",
    "plt.title('Reward values')\n",
    "plt.show()\n",
    "opt_om_ts, opt_om = mce_irl.mce_occupancy_measures(vw_env)\n",
    "sns.heatmap(opt_om.reshape((vw_env.n_rows, vw_env.n_cols)))\n",
    "plt.title('Occupancy measure')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
