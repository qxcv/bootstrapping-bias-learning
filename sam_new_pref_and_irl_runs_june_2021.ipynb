{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface, vase_world\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(point, direction, eval_fn, *, init_step=1.0, armijo_c=0.05, armijo_tau=0.1, min_step=1e-5):\n",
    "    \"\"\"Performs backtracking line search until Armijo condition is satisfied.\n",
    "    Assumes function is continuously differentiable (which should be true\n",
    "    as long as we stay away from L1 penalties, hard maxes, etc.).\"\"\"\n",
    "    value_at_point = eval_fn(point)\n",
    "    armijo_satisfied = False\n",
    "    t = -armijo_c * jnp.dot(point, direction)\n",
    "    num_steps = 0\n",
    "    best_step_size = init_step\n",
    "    value_for_best_step_size = float('inf')\n",
    "    while not armijo_satisfied:\n",
    "        # compute step size and update num_steps\n",
    "        step_size = init_step * armijo_tau ** num_steps\n",
    "        num_steps += 1\n",
    "        \n",
    "        # break early if we're running for suspiciously long\n",
    "        if step_size < min_step:\n",
    "            print(\n",
    "                f\"Backtracked to step size {step_size} after {num_steps} iterations, \"\n",
    "                \"but Armijo is still not satisfied; exiting early\")\n",
    "            break\n",
    "\n",
    "        # compute new value & Armijo condition\n",
    "        value_at_step = eval_fn(point - step_size * direction)\n",
    "        armijo_satisfied = value_at_point - value_at_step >= step_size * t\n",
    "        \n",
    "        # we keep the 'best' step size in case we need to exit early (this is a bit of hack, but oh well)\n",
    "        if value_at_step < value_for_best_step_size:\n",
    "            value_for_best_step_size = value_at_step\n",
    "            best_step_size = step_size\n",
    "\n",
    "    return best_step_size\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj, env, pc_expert):\n",
    "    \"\"\"Generates a paired comparisons dataset by generating n_traj trajectories\n",
    "    and then doing a single comparison (with a random other trajectory) for\n",
    "    each generated trajectory.\"\"\"\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "def multi_optimize(*, model_list, data_list, rmodel, bias_list, env, names, bias_lrs, rew_lr=0.01,\n",
    "                   steps=1000, optimize_reward=True, debug=False, use_prior_grad=True): \n",
    "    \"\"\"Joint optimization of several models at once.\"\"\"\n",
    "    loss_prev = float('Inf')\n",
    "\n",
    "    for step in range(steps):\n",
    "        rew_grad = jnp.zeros_like(rmodel.get_params())\n",
    "        if debug: print('rew_grad:', rew_grad)\n",
    "        \n",
    "        # FIXME(sam): there is no prior on this reward model for some reason\n",
    "        if optimize_reward: \n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                rew_grad += model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "                if debug: print('rew_grad:', rew_grad)\n",
    "\n",
    "        new_r = rmodel.get_params() + rew_lr * rew_grad\n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        bias_grad_norms = []\n",
    "        for k, (model, data, bias_params, bias_lr) in enumerate(zip(model_list, data_list, bias_list, bias_lrs)):\n",
    "            bias_grad = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "            # bias prior\n",
    "            if use_prior_grad:\n",
    "                bias_grad += model.bias_prior.log_prior_grad(bias_params)\n",
    "            bias_grad_norms.append(np.linalg.norm(np.asarray(bias_grad)))\n",
    "            bias_list[k] = model.bias_prior.project_to_support(bias_params + bias_lr * bias_grad)\n",
    "            if debug: print('bias_grad:', bias_grad)\n",
    "\n",
    "        for k, (model, data, bias_params, name, bias_grad_norm) in enumerate(zip(model_list, data_list, bias_list, names, bias_grad_norms)):\n",
    "            log_likelihood = model.log_likelihood(data, rmodel, bias_params)\n",
    "            if step % 100 == 0:\n",
    "                print('[%s] step %d: ll %.3f, |bias_grad| %.3f' % (name, step, log_likelihood, bias_grad_norm))\n",
    "                print('[%s] biases' % (name, ), bias_params)\n",
    "        if step % 100 == 0:\n",
    "            print(f'[reward] params {np.asarray(rmodel.get_params())}, |grad| {np.linalg.norm(rew_grad):.3f}')\n",
    "            print()\n",
    "\n",
    "    return model_list, rmodel, bias_list\n",
    "\n",
    "def init_models(env, irl_ntraj, pc_ntraj, seed=42, *,\n",
    "                dummy_use_opt_reward=False):\n",
    "    \"\"\"Initialize paired comparisons & IRL models.\"\"\"\n",
    "    irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "    rng = jrandom.PRNGKey(seed)\n",
    "    rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "    rng, true_irl_bias_params_jax = irl_feedback_model.init_bias_params(rng)\n",
    "    # round the actual parameters to 0/1\n",
    "    true_irl_bias_params = np.round(np.asarray(true_irl_bias_params_jax))\n",
    "    # these weird hex values are here so we have different seeds in\n",
    "    # each place where we need to seed a new RNG\n",
    "    irl_expert = experts.BlindMEDemonstratorExpert(\n",
    "        env=env, feature_weights=true_irl_bias_params, seed=seed ^ 0x7364181f)\n",
    "    # we'll do IRL based on 10 trajectories\n",
    "    irl_dataset = irl_expert.interact(irl_ntraj)\n",
    "\n",
    "    pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "    rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "    rng, true_pc_bias_params_jax = pc_feedback_model.init_bias_params(rng)\n",
    "    true_pc_bias_params = float(true_pc_bias_params_jax)\n",
    "    pc_expert = experts.PairedComparisonExpert(\n",
    "        env, boltz_temp=true_pc_bias_params_jax, seed=seed ^ 0x35e0251f)\n",
    "    assert pc_expert.boltz_temp == true_pc_bias_params\n",
    "    comparison_dataset = generate_comparison_dataset(pc_ntraj=pc_ntraj,\n",
    "                                                     env=env,\n",
    "                                                     pc_expert=pc_expert)\n",
    "\n",
    "    model_list = [pc_feedback_model, irl_feedback_model]\n",
    "    data_list = [comparison_dataset, irl_dataset]\n",
    "    bias_list = [pc_bias_params, irl_bias_params]\n",
    "    names = ['paired_comparisons', 'blind_irl']\n",
    "    biases_actual = [pc_expert.boltz_temp, true_irl_bias_params]\n",
    "\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim, seed=seed ^ 0x6b2a8d53)\n",
    "    if dummy_use_opt_reward:\n",
    "        print(\"WARNING: Initializing with optimal reward parameters!\", file=sys.stderr)\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "        \n",
    "    return model_list, data_list, rmodel, bias_list, names, biases_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the \"vase world\" environment\n",
    "vw_env = vase_world.VaseWorld()\n",
    "# vw_env.reward_weights = vw_env.reward_weights\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(vw_env.reward_matrix.reshape((vw_env.n_rows, vw_env.n_cols)))\n",
    "plt.title('Reward values')\n",
    "plt.subplot(1, 2, 2)\n",
    "opt_om_ts, opt_om = mce_irl.mce_occupancy_measures(vw_env)\n",
    "sns.heatmap(opt_om.reshape((vw_env.n_rows, vw_env.n_cols)))\n",
    "plt.title('Occupancy measure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_joint_exp(vw_env, seed, irl_ntraj=5, pc_ntraj=30, n_steps=2000,\n",
    "                 rew_lr=1e-2, pc_bias_lr=1e-4, irl_bias_lr=1e-3,\n",
    "                 use_prior_grad=True):\n",
    "    model_list, data_list, rmodel, bias_list, names, biases_actual = init_models(\n",
    "        vw_env, irl_ntraj=irl_ntraj, pc_ntraj=pc_ntraj, seed=seed)\n",
    "    print('Reward model params:', rmodel.get_params())\n",
    "    print('True reward weights:', vw_env.reward_weights)\n",
    "    print('Bias params:', bias_list)\n",
    "    print('Real biases:', biases_actual)\n",
    "\n",
    "    final_rmodels = {}\n",
    "    final_bias_lists = {}\n",
    "\n",
    "    # we use copy.deecopy to avoid mutating things in place\n",
    "    common_kwargs = dict(\n",
    "        env=vw_env,\n",
    "        rew_lr=rew_lr,\n",
    "        steps=n_steps,\n",
    "        use_prior_grad=use_prior_grad,\n",
    "    )\n",
    "    bias_lrs = [pc_bias_lr, irl_bias_lr]\n",
    "    joint_final_model_list, final_rmodels['joint'], final_bias_lists['joint'] = multi_optimize(**copy.deepcopy(dict(\n",
    "        names=names,\n",
    "        model_list=model_list,\n",
    "        data_list=data_list,\n",
    "        bias_list=bias_list,\n",
    "        rmodel=rmodel,\n",
    "        bias_lrs=bias_lrs,\n",
    "        **common_kwargs)))\n",
    "\n",
    "    for method_idx, name in enumerate(names):\n",
    "        meth_slice = slice(method_idx, method_idx+1)\n",
    "        final_model_list, final_rmodels[name], final_bias_lists[name] = multi_optimize(**copy.deepcopy(dict(\n",
    "            names=names[meth_slice],\n",
    "            model_list=model_list[meth_slice],\n",
    "            data_list=data_list[meth_slice],\n",
    "            bias_list=bias_list[meth_slice],\n",
    "            rmodel=rmodel,\n",
    "            bias_lrs=bias_lrs[meth_slice],\n",
    "            **common_kwargs)))\n",
    "        \n",
    "    return final_rmodels, final_bias_lists, biases_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_run_results = []\n",
    "for seed in range(3):\n",
    "    result = do_joint_exp(vw_env, seed=seed, irl_ntraj=5, pc_ntraj=50, n_steps=2000, use_prior_grad=False)  # XXX set use_prior_grad=True to actually use prior\n",
    "    all_run_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of reward recovery accuracy\n",
    "def make_records(name, rew_vec):\n",
    "    assert len(rew_vec) == len(vw_env.plane_names)\n",
    "    for plane_name, value in zip(vw_env.plane_names, rew_vec):\n",
    "        yield {\n",
    "            'run': name,\n",
    "            'plane': plane_name,\n",
    "            'value': value,\n",
    "        }\n",
    "\n",
    "records = [\n",
    "    *make_records('ground truth', vw_env.reward_weights)\n",
    "]\n",
    "for final_rmodels, _, _ in all_run_results:\n",
    "    for rm_name, rm in final_rmodels.items():\n",
    "        records.extend(make_records(rm_name, rm.get_params()))\n",
    "rew_frame = pd.DataFrame.from_records(records)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rew_frame, x='plane', y='value', hue='run', ci='sd')\n",
    "plt.title('Reward recovery accuracy')\n",
    "plt.ylabel('Feature weight')\n",
    "plt.xlabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of bias recovery accuracy for IRL\n",
    "irl_bias_accs = []\n",
    "def l1_dist(l1, l2):\n",
    "    return np.linalg.norm(np.asarray(l1) - np.asarray(l2), ord=1)\n",
    "\n",
    "for _, bias_rec, bias_actual in all_run_results:\n",
    "    joint_err = l1_dist(bias_rec['joint'][1], bias_actual[1])\n",
    "    blind_err = l1_dist(bias_rec['blind_irl'][0], bias_actual[1])\n",
    "    irl_bias_accs.extend([\n",
    "        {\n",
    "            'run': 'joint',\n",
    "            'l1_err': joint_err,\n",
    "        },\n",
    "        {\n",
    "            'run': 'blind_irl',\n",
    "            'l1_err': blind_err,\n",
    "        },\n",
    "    ])\n",
    "irl_bias_frame = pd.DataFrame.from_records(irl_bias_accs)\n",
    "sns.boxplot(data=irl_bias_frame, x='run', y='l1_err')\n",
    "plt.title('Accuracy of IRL bias recovery')\n",
    "plt.ylabel(r'Error, $\\ell_1$ distance')\n",
    "plt.xlabel('Run')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of bias recovery accuracy for paired comparisons\n",
    "pc_bias_accs = []\n",
    "for _, bias_rec, bias_actual in all_run_results:\n",
    "    joint_err = np.abs(np.log(bias_rec['joint'][0] + 1e-3) - np.log(bias_actual[0]))\n",
    "    pc_err = np.abs(np.log(bias_rec['paired_comparisons'][0] + 1e-3) - np.log(bias_actual[0]))\n",
    "    pc_bias_accs.extend([\n",
    "        {\n",
    "            'run': 'joint',\n",
    "            'abs_err': joint_err,\n",
    "        },\n",
    "        {\n",
    "            'run': 'paired_comparisons',\n",
    "            'abs_err': pc_err,\n",
    "        },\n",
    "    ])\n",
    "pc_bias_frame = pd.DataFrame.from_records(pc_bias_accs)\n",
    "sns.boxplot(data=pc_bias_frame, x='run', y='abs_err')\n",
    "plt.title('Accuracy of paired comparison bias recovery')\n",
    "plt.ylabel(r'Error, $|\\log \\beta - \\log \\hat \\beta|$')\n",
    "plt.xlabel('Run')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
