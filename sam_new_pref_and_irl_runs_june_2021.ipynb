{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface, vase_world\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(point, direction, eval_fn, *, init_step=1.0, armijo_c=0.05, armijo_tau=0.1, min_step=1e-5):\n",
    "    \"\"\"Performs backtracking line search until Armijo condition is satisfied.\n",
    "    Assumes function is continuously differentiable (which should be true\n",
    "    as long as we stay away from L1 penalties, hard maxes, etc.).\"\"\"\n",
    "    value_at_point = eval_fn(point)\n",
    "    armijo_satisfied = False\n",
    "    t = -armijo_c * jnp.dot(point, direction)\n",
    "    num_steps = 0\n",
    "    best_step_size = init_step\n",
    "    value_for_best_step_size = float('inf')\n",
    "    while not armijo_satisfied:\n",
    "        # compute step size and update num_steps\n",
    "        step_size = init_step * armijo_tau ** num_steps\n",
    "        num_steps += 1\n",
    "        \n",
    "        # break early if we're running for suspiciously long\n",
    "        if step_size < min_step:\n",
    "            print(\n",
    "                f\"Backtracked to step size {step_size} after {num_steps} iterations, \"\n",
    "                \"but Armijo is still not satisfied; exiting early\")\n",
    "            break\n",
    "\n",
    "        # compute new value & Armijo condition\n",
    "        value_at_step = eval_fn(point - step_size * direction)\n",
    "        armijo_satisfied = value_at_point - value_at_step >= step_size * t\n",
    "        \n",
    "        # we keep the 'best' step size in case we need to exit early (this is a bit of hack, but oh well)\n",
    "        if value_at_step < value_for_best_step_size:\n",
    "            value_for_best_step_size = value_at_step\n",
    "            best_step_size = step_size\n",
    "\n",
    "    return best_step_size\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj, env, pc_expert):\n",
    "    \"\"\"Generates a paired comparisons dataset by generating n_traj trajectories\n",
    "    and then doing a single comparison (with a random other trajectory) for\n",
    "    each generated trajectory.\"\"\"\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "def multi_optimize(model_list, data_list, rmodel, bias_list, env, names, lr=0.1, steps=1000, *, optimize_reward=True, debug=False): \n",
    "    \"\"\"Joint optimization of several models at once.\"\"\"\n",
    "    loss_prev = float('Inf')\n",
    "\n",
    "    for step in range(steps):\n",
    "        rew_grad = jnp.zeros_like(rmodel.get_params())\n",
    "        if debug: print('rew_grad:', rew_grad)  # XXX\n",
    "        \n",
    "        if optimize_reward: \n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                rew_grad += model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "                if debug: print('rew_grad:', rew_grad)  # XXX\n",
    "\n",
    "        new_r = rmodel.get_params() + lr * rew_grad\n",
    "        rmodel.set_params(new_r)\n",
    "        \n",
    "        bias_grad_norms = []\n",
    "        for k, (model, data, bias_params) in enumerate(zip(model_list, data_list, bias_list)):\n",
    "            bias_grad = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "            # bias prior\n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            bias_grad = bias_grad + bias_prior_grad\n",
    "            bias_grad_norms.append(np.linalg.norm(np.asarray(bias_grad)))\n",
    "            bias_list[k] = model.bias_prior.project_to_support(bias_params + lr * bias_grad)\n",
    "            if debug: print('bias_grad:', bias_grad)  # XXX\n",
    "\n",
    "        for k, (model, data, bias_params, name, bias_grad_norm) in enumerate(zip(model_list, data_list, bias_list, names, bias_grad_norms)):\n",
    "            log_likelihood = model.log_likelihood(data, rmodel, bias_params)\n",
    "            if step % 100 == 0:\n",
    "                print('[%s] step %d: ll %.3f, |bias_grad| %.3f' % (name, step, log_likelihood, bias_grad_norm))\n",
    "                print('[%s] biases' % (name, ), bias_params)\n",
    "        if step % 100 == 0:\n",
    "            print(f'[reward] params {np.asarray(rmodel.get_params())}, |grad| {np.linalg.norm(rew_grad):.3f}')\n",
    "            print()\n",
    "\n",
    "    return model_list, rmodel, bias_list\n",
    "\n",
    "def multi_optimize_line(model_list, data_list, rmodel, bias_list, env, names, lr=0.01, steps=1000, *, optimize_reward=True, debug=False): \n",
    "    \"\"\"Like multi_optimize but does line search.\"\"\"\n",
    "    loss_prev = float('Inf')\n",
    "    for step in range(steps):\n",
    "        rew_grad = jnp.zeros_like(rmodel.get_params())\n",
    "        if debug: print('rew_grad:', rew_grad)  # XXX\n",
    "        \n",
    "        if optimize_reward: \n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                rew_grad += model.log_likelihood_grad_rew(data, rmodel, bias_params)\n",
    "                if debug: print('rew_grad:', rew_grad)  # XXX\n",
    "\n",
    "        def rew_log_likelihood(rparams):\n",
    "            # FIXME(sam): this is probably inefficient\n",
    "            rmodel2 = copy.deepcopy(rmodel)\n",
    "            rmodel2.set_params(rparams)\n",
    "            total = 0.0\n",
    "            for model, data, bias_params in zip(model_list, data_list, bias_list):\n",
    "                total += model.log_likelihood(data, rmodel2, bias_params)\n",
    "            return total\n",
    "\n",
    "        rew_step_size = backtracking_line_search(point=rmodel.get_params(), direction=rew_grad, eval_fn=rew_log_likelihood)\n",
    "        new_r = rmodel.get_params() + rew_step_size * rew_grad\n",
    "        rmodel.set_params(new_r)\n",
    "        raise NotImplementedError(\"Need to finish the rest of htis\")\n",
    "        \n",
    "        bias_grad_norms = []\n",
    "        for k, (model, data, bias_params) in enumerate(zip(model_list, data_list, bias_list)):\n",
    "            bias_grad = model.log_likelihood_grad_bias(data, rmodel, bias_params)\n",
    "            # bias prior\n",
    "            bias_prior_grad = model.bias_prior.log_prior_grad(bias_params)\n",
    "            bias_grad = bias_grad + bias_prior_grad\n",
    "            bias_grad_norms.append(np.linalg.norm(np.asarray(bias_grad)))\n",
    "            bias_list[k] = model.bias_prior.project_to_support(bias_params + lr * bias_grad)\n",
    "            if debug: print('bias_grad:', bias_grad)  # XXX\n",
    "\n",
    "        for k, (model, data, bias_params, name, bias_grad_norm) in enumerate(zip(model_list, data_list, bias_list, names, bias_grad_norms)):\n",
    "            log_likelihood = model.log_likelihood(data, rmodel, bias_params)\n",
    "            if step % 100 == 0:\n",
    "                print('[%s] step %d: ll %.3f, |bias_grad| %.3f' % (name, step, log_likelihood, bias_grad_norm))\n",
    "                print('[%s] biases' % (name, ), bias_params)\n",
    "        if step % 100 == 0:\n",
    "            print(f'[reward] params {np.asarray(rmodel.get_params())}, |grad| {np.linalg.norm(rew_grad):.3f}')\n",
    "            print()\n",
    "\n",
    "    return model_list, rmodel, bias_list\n",
    "\n",
    "def init_models(env, irl_ntraj, pc_ntraj, seed=42, *,\n",
    "                dummy_use_opt_reward=False):\n",
    "    \"\"\"Initialize paired comparisons & IRL models.\"\"\"\n",
    "    irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "    rng = jrandom.PRNGKey(seed)\n",
    "    rng, irl_bias_params = irl_feedback_model.init_bias_params(rng)\n",
    "    rng, true_irl_bias_params_jax = irl_feedback_model.init_bias_params(rng)\n",
    "    # round the actual parameters to 0/1\n",
    "    true_irl_bias_params = np.round(np.asarray(true_irl_bias_params_jax))\n",
    "    # these weird hex values are here so we have different seeds in\n",
    "    # each place where we need to seed a new RNG\n",
    "    irl_expert = experts.BlindMEDemonstratorExpert(\n",
    "        env=env, feature_weights=true_irl_bias_params, seed=seed ^ 0x7364181f)\n",
    "    # we'll do IRL based on 10 trajectories\n",
    "    irl_dataset = irl_expert.interact(irl_ntraj)\n",
    "\n",
    "    pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "    rng, pc_bias_params = pc_feedback_model.init_bias_params(rng)\n",
    "    rng, true_pc_bias_params_jax = pc_feedback_model.init_bias_params(rng)\n",
    "    true_pc_bias_params = float(true_pc_bias_params_jax)\n",
    "    pc_expert = experts.PairedComparisonExpert(\n",
    "        env, boltz_temp=true_pc_bias_params_jax, seed=seed ^ 0x35e0251f)\n",
    "    assert pc_expert.boltz_temp == true_pc_bias_params\n",
    "    comparison_dataset = generate_comparison_dataset(pc_ntraj=pc_ntraj,\n",
    "                                                     env=env,\n",
    "                                                     pc_expert=pc_expert)\n",
    "\n",
    "    model_list = [pc_feedback_model, irl_feedback_model]\n",
    "    data_list = [comparison_dataset, irl_dataset]\n",
    "    bias_list = [pc_bias_params, irl_bias_params]\n",
    "    names = ['paired_comparisons', 'blind_irl']\n",
    "    biases_actual = [pc_expert.boltz_temp, true_irl_bias_params]\n",
    "\n",
    "    rmodel = r_models.LinearRewardModel(env.obs_dim, seed=seed ^ 0x6b2a8d53)\n",
    "    if dummy_use_opt_reward:\n",
    "        print(\"WARNING: Initializing with optimal reward parameters!\", file=sys.stderr)\n",
    "        rmodel.set_params(env.reward_matrix)\n",
    "        \n",
    "    return model_list, data_list, rmodel, bias_list, names, biases_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the \"vase world\" environment\n",
    "vw_env = vase_world.VaseWorld()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(vw_env.reward_matrix.reshape((vw_env.n_rows, vw_env.n_cols)))\n",
    "plt.title('Reward values')\n",
    "plt.subplot(1, 2, 2)\n",
    "opt_om_ts, opt_om = mce_irl.mce_occupancy_measures(vw_env)\n",
    "sns.heatmap(opt_om.reshape((vw_env.n_rows, vw_env.n_cols)))\n",
    "plt.title('Occupancy measure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, data_list, rmodel, bias_list, names, biases_actual = init_models(vw_env, irl_ntraj=5, pc_ntraj=30, seed=45)\n",
    "print('Reward model params:', rmodel.get_params())\n",
    "print('True reward weights:', vw_env.reward_weights)\n",
    "print('Bias params:', bias_list)\n",
    "print('Real biases:', biases_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_list, final_rmodel, final_bias_list = multi_optimize(model_list=model_list, data_list=data_list, rmodel=rmodel, bias_list=bias_list, env=vw_env, names=names, lr=0.01, steps=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
