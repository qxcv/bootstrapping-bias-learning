{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias model bootstrapping notebook\n",
    "\n",
    "Eventually this notebook will (hopefully?) contain all the code necessary to run our final experiments.\n",
    "For now it just contains a demo of MCE IRL on some gridworlds from the \"learning biases\" paper.\n",
    "\n",
    "First we have some not-very-interesting setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# These are useful for debugging, but make code slower:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import jax.experimental.optimizers as jopt\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "\n",
    "from pref_bootstrap.envs import gridworld, mdp_interface\n",
    "from pref_bootstrap.algos import mce_irl\n",
    "import pref_bootstrap.feedback_learner_blind_irl as fbl_blind_irl\n",
    "import pref_bootstrap.feedback_learner_paired_comparisons as fbl_paired_comp\n",
    "import pref_bootstrap.reward_models as r_models\n",
    "import pref_bootstrap.expert_base as experts\n",
    "\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "np.set_printoptions(precision=4, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very simple example of using environments and MCE IRL code\n",
    "\n",
    "This code doesn't use the new agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_gridworld = gridworld.GridworldMdp.generate_random(4, 4, 0.2, 0.1)\n",
    "env = mdp_interface.GridworldEnvWrapper(random_gridworld, random_gridworld.height + random_gridworld.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some optimal demos (represented by a single optimal occupancy measure vector) and run MCE IRL\n",
    "_, optimal_om = mce_irl.mce_occupancy_measures(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "opt_tup = jopt.momentum(1e-2, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Occupancy measure error@iter   0: 5.504314 (||params||=4.317492, ||grad||=7.676059, ||E[dr/dw]||=6.588705)\n",
      "INFO:root:Occupancy measure error@iter  100: 0.045184 (||params||=5.044746, ||grad||=0.059398, ||E[dr/dw]||=6.971343)\n",
      "INFO:root:Occupancy measure error@iter  200: 0.012208 (||params||=4.912653, ||grad||=0.016860, ||E[dr/dw]||=6.940515)\n",
      "INFO:root:Occupancy measure error@iter  300: 0.005907 (||params||=4.908501, ||grad||=0.007435, ||E[dr/dw]||=6.939543)\n",
      "INFO:root:Occupancy measure error@iter  400: 0.002958 (||params||=4.911251, ||grad||=0.003578, ||E[dr/dw]||=6.940352)\n",
      "INFO:root:Occupancy measure error@iter  500: 0.001553 (||params||=4.912214, ||grad||=0.001851, ||E[dr/dw]||=6.940836)\n"
     ]
    }
   ],
   "source": [
    "rew_params, visitations = mce_irl.mce_irl(env, opt_tup, rmodel, optimal_om, print_interval=100, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 6.8535e+00 4.6748e-02 0.0000e+00]\n",
      " [0.0000e+00 1.0985e+00 1.2728e-03 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "\n",
      "Recovered state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 6.8535e+00 4.7743e-02 0.0000e+00]\n",
      " [0.0000e+00 1.0982e+00 6.4709e-04 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))\n",
    "print('\\nRecovered state visitation frequencies for each grid cell:')\n",
    "print(visitations.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex example showing how to use an EnvFeedbackModel to recover both a reward function + sub-rationality model\n",
    "\n",
    "This code actually does use the new API to show how to use the 'blind IRL' feedback model (& its associated expert, which doesn't support observation blinding yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "irl_feedback_model = fbl_blind_irl.BlindIRLFeedbackModel(env)\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "rng = jrandom.PRNGKey(42)\n",
    "irl_bias_params, rng = irl_feedback_model.init_bias_params(rng)\n",
    "irl_expert = experts.MEDemonstratorExpert(env, np.random.randint((1 << 31) - 1))\n",
    "# we'll do IRL based on 10 trajectories\n",
    "irl_dataset = irl_expert.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (IRL): -13.663637161254883\n",
      "Gradient w.r.t. reward params (IRL):\n",
      " [ 0.      0.      0.      0.      0.      0.196  -0.0096  0.      0.     -0.0523 -0.0397  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (IRL):\n",
      " [ 0.      0.      0.      0.      0.     -0.1026  0.0207  0.      0.     -0.0879 -0.0184  0.\n",
      "  0.      0.      0.      0.    ]\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (IRL):', float(irl_feedback_model.log_likelihood(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. reward params (IRL):\\n', np.asarray(irl_feedback_model.log_likelihood_grad_rew(irl_dataset, rmodel, irl_bias_params)))\n",
    "print('Gradient w.r.t. bias params (IRL):\\n', irl_feedback_model.log_likelihood_grad_bias(irl_dataset, rmodel, irl_bias_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example with the paired comparison learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_feedback_model = fbl_paired_comp.PairedCompFeedbackModel(env)\n",
    "pc_bias_params, rng = pc_feedback_model.init_bias_params(rng)\n",
    "pc_expert = experts.PairedComparisonExpert(env, boltz_temp=1.0, seed=42)\n",
    "\n",
    "# generate some random trajectories & compare a random subset of them\n",
    "def generate_comparison_dataset(pc_ntraj):\n",
    "    pc_trajectories = mce_irl.mce_irl_sample(env, pc_ntraj, R=np.ones((env.n_states, )))\n",
    "    to_compare_first = np.arange(len(pc_trajectories['states']))\n",
    "    comparisons = []\n",
    "    for first_idx in range(pc_ntraj):\n",
    "        second_idx = np.random.randint(pc_ntraj - 1)\n",
    "        if second_idx >= first_idx:\n",
    "            second_idx += 1\n",
    "        traj1_is_better = pc_expert.interact(\n",
    "            dict(states=pc_trajectories['states'][first_idx]),\n",
    "            dict(states=pc_trajectories['states'][second_idx]))\n",
    "        if traj1_is_better:\n",
    "            # the better trajectory comes before the worse one\n",
    "            comparisons.append((first_idx, second_idx))\n",
    "        else:\n",
    "            comparisons.append((second_idx, first_idx))\n",
    "    return {\n",
    "        'trajectories': pc_trajectories,\n",
    "        'comparisons': np.asarray(comparisons),\n",
    "    }\n",
    "\n",
    "comparison_dataset = generate_comparison_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood (PC): -1.51496160030365\n",
      "Gradient w.r.t. reward params (PC):\n",
      " [ 0.      0.      0.      0.      0.      0.6405  0.4841  0.      0.     -0.409  -0.7157  0.\n",
      "  0.      0.      0.      0.    ]\n",
      "Gradient w.r.t. bias params (PC):\n",
      " -2.5283113\n"
     ]
    }
   ],
   "source": [
    "print('Log likelihood (PC):', float(pc_feedback_model.log_likelihood(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. reward params (PC):\\n', np.asarray(pc_feedback_model.log_likelihood_grad_rew(comparison_dataset, rmodel, pc_bias_params)))\n",
    "print('Gradient w.r.t. bias params (PC):\\n', pc_feedback_model.log_likelihood_grad_bias(comparison_dataset, rmodel, pc_bias_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALRIGHT, HERE IS WHERE I WILL DEBUG THE TOPK FEEDBACK LEARNER\n",
    "# ABLATIONS: COULD WE DO BETTER IF ALL OUR TRAJECTORIES ARE GOOD? \n",
    "# OR IS IT NECESSARY TO HAVE BAD TRAJECTORIES? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pref_bootstrap.feedback_learner_topk as fbl_topk\n",
    "\n",
    "rmodel = r_models.LinearRewardModel(env.obs_dim)\n",
    "top_K_feedback_model = fbl_topk.TopKFeedbackModel(env)\n",
    "top_K_expert = experts.TopKExpert(env, temp=.6, K=.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  3.  0. 12.  6. 18. 15.  0.  0.  3.  6.  6.  0.  0.  0.  0. 12.  3.  0.  3.  6.  0. 15.\n",
      "  0. 15.  0. 18. 21.  6.  3.  9. 24.  9.  3. 12.  0. 15. 21.  6. 12.  3.  0.  0.  6. 21.  6.  6.\n",
      " 12.  0.  6.  9. 12.  0.  9.  9. 15.  0.  0.  0.  3.  0.  3.  0.  6.  9.  3.  0.  3.  3.  3.  6.\n",
      "  0.  3.  3.  9. 24.  6.  0.  3.  3.  3.  0.  6.  9. 18. 12.  6.  3. 15.  6.  0.  6.  6.  0. 15.\n",
      "  0.  6. 15.  0.  6.  6.  9.  6.  9.  0.  0. 18. 15.  3.  9.  6.  0.  0.  6. 18.  3.  6.  3.  9.\n",
      "  3.  6.  0.  0.  0.  0.  3.  3. 15. 12.  6.  0.  9.  0.  3.  9.  9.  0.  9. 18. 12.  9.  0.  0.\n",
      "  0.  0.  3.  0.  3.  3.  6.  9.  0.  0. 18.  9. 21. 15.  9.  6. 15.  0. 12. 15.  9. 12.  0.  3.\n",
      "  6.  3.  9.  3.  0.  0.  6.  6. 18.  6.  6.  0.  6.  0.  9.  9. 15.  0.  0.  6.  9. 12. 18.  3.\n",
      "  6. 21.  0.  0. 12. 12. 12.  6.  6.  0.  3.  0.  0.  0. 12.  3.  0. 12.  3.  9. 15.  6.  0.  0.\n",
      "  9. 12.  3.  3. 12.  9. 12.  6.  0.  3. 15.  9.  0.  9.  3.  9.  3. 18.  3.  0.  3. 12.  6.  6.\n",
      " 18. 12.  3. 12.  3.  0.  9.  3. 12.  9. 15.  6.  3.  3.  3.  3.  3.  0.  3.  0.  0.  9.  0.  3.\n",
      "  0.  0.  0.  6.  0. 18.  3.  6.  6.  9.  0.  6.  0.  6.  0. 18.  9.  6.  0.  3.  6.  9.  0.  0.\n",
      "  0. 12.  0.  9.  3.  3.  6.  3.  0.  3.  9.  6. 12.  0.  3.  0.  0.  6.  0.  6. 18.  6. 12.  3.\n",
      "  0.  9.  0.  6. 15.  6.  6.  3.  6.  0.  0. 12.  3.  3.  3. 15.  6. 21. 24. 12.  3.  6.  0.  3.\n",
      "  0. 12.  0.  3.  3.  3.  3. 12.  9.  6.  3.  9. 12. 18.  3.  9. 21.  3.  9.  6.  0.  6. 15.  0.\n",
      "  9.  9.  6.  0. 15.  9.  6.  6. 12.  3.  0.  0.  3.  0.  0.  9. 15.  9.  9.  0.  3.  6. 12.  9.\n",
      "  3.  3.  3. 12.  3.  3. 12.  0.  6.  0.  0. 15.  6.  6. 12.  0.  0.  0.  0.  0.  6. 18.  6.  9.\n",
      " 12.  3.  0.  0.  9.  9.  6.  9.  3. 12.  0. 24.  3.  0. 12.  0. 18. 12.  0.  0. 15.  3. 18.  0.\n",
      "  3. 12.  3.  3.  9. 18.  3.  0.  0.  3.  0.  0.  3.  3. 12.  0.  3.  6.  0.  9.  9. 12.  9.  3.\n",
      "  0. 18.  9.  6.  0.  6.  0. 21.  0.  0.  9.  0. 15.  3.  0.  3.  9.  0.  0.  0.  9.  9. 15.  0.\n",
      " 12. 18. 15.  3. 12.  3.  9.  0.  6.  6.  0.  0. 12. 12.  9. 21. 21.  9.  0. 15.  6.  0. 24.  0.\n",
      "  9.  3. 12.  6.  6. 12.  6. 21.  3.  6.  9.  0. 15. 15.  3. 18. 18.  6.  3.  3.  3.  9.  9.  3.\n",
      " 21.  9.  6. 21.  9.  0.  0.  0.  3.  6. 12.  3.  9.  0.  6.  0. 12.  3.  0.  9.  9.  0.  0.  6.\n",
      "  3. 12.  0.  0.  9.  0.  0.  0.  0.  0.  6.  3.  0. 12. 15.  6.  0.  0.  6.  6.  0.  6.  6.  3.\n",
      "  3. 15. 15. 12.  3.  6.  6.  9.  6.  0. 18.  3.  6.  6.  3.  0.  6.  6. 18.  6.  0.  0. 12.  9.\n",
      "  0. 12.  3.  6.  3.  9. 12.  0. 15.  9.  6.  3.  3. 15. 15.  0.  3. 12.  3.  0. 15. 15.  3.  3.\n",
      "  0.  6. 15. 12.  3.  9.  9.  3.  6.  3. 24. 15.  6.  9.  0.  9.  0. 12.  0.  3.  6. 12.  3. 12.\n",
      "  6.  9. 12.  0. 12.  0. 15.  6.  6.  3.  0. 18.  0.  0.  0.  3.  0.  3. 15. 24.  0.  0.  0.  0.\n",
      " 12.  0.  3.  9.  0.  0. 12.  3. 12.  0.  0.  9.  0.  0.  6. 15.  9.  9.  6. 12. 21. 15.  6.  0.\n",
      " 24.  0. 15.  0.  0.  0.  3.  3.  0.  9. 15.  0. 15.  3.  9.  6.  0.  9.  3.  6.  0.  6. 24.  9.\n",
      "  9.  6. 15.  0. 12.  9.  9.  9.  6.  3.  0. 12.  3.  3.  9.  0.  0.  0.  9.  9.  6.  0.  0.  3.\n",
      "  0.  0. 12.  0.  0.  6.  0.  0.  3.  0. 12.  6. 15.  0.  9.  6.  0.  3.  0. 12.  0.  0.  0. 12.\n",
      "  0.  3.  9. 12. 21.  0.  6.  9.  6.  0.  9.  3. 12.  0.  0.  0.  0.  0. 21.  9. 21.  6. 12. 24.\n",
      " 12. 21.  6.  0.  0.  9.  0.  6.  3.  0.  3. 12. 12. 21. 12.  0.  3.  3.  0. 12.  3.  9.  9.  6.\n",
      "  3.  0.  0. 18.  0.  0.  9.  0.  0.  0.  6.  6. 15.  0. 15.  0.  9.  0.  0.  3.  9.  9.  3.  0.\n",
      "  3.  9. 18.  0.  6.  3.  6.  9.  6.  3. 15.  3.  6.  9.  0.  0.  0.  0.  0.  3.  3.  6.  3.  6.\n",
      "  6.  0. 21.  9. 15. 12.  6.  0.  0. 12.  0. 15.  0.  3.  6.  6.  0.  3.  6.  0.  3.  3.  6.  3.\n",
      "  6.  0. 12.  9.  3.  0.  0. 15.  6.  3. 15.  9.  6.  6.  0.  9.  0.  3.  0.  9.  0.  0.  0.  6.\n",
      " 12.  9.  3.  0.  0. 12. 18.  6.  6. 21.  6.  9.  3.  0. 18.  9.  0.  0. 15.  3.  0. 18.  6.  6.\n",
      "  6.  9.  9.  3.  9.  0.  9.  0.  3.  6.  0.  0.  0.  0.  0.  0. 18.  0.  0. 12.  3.  0. 15.  6.\n",
      "  6.  0.  9.  3.  0. 15.  3.  3. 12.  0. 12.  0.  9. 12. 15.  0.  0. 12.  9.  0.  0.  6.  0.  0.\n",
      "  9.  6.  6.  0.  6. 15.  3.  0. 21.  3.  0.  0.  3.  0.  0.  9.]\n",
      "50\n",
      "CUTOFF 18.0\n"
     ]
    }
   ],
   "source": [
    "# def genereate_top_K_dataset(n_traj): \n",
    "\n",
    "n_traj = 1000\n",
    "traj = mce_irl.mce_irl_sample(env, n_traj, R=np.ones((env.n_states,)))\n",
    "labels= top_K_expert.interact(traj)\n",
    "print('CUTOFF', top_K_expert.cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 9)\n",
      "step 0 loss 5.962\n",
      "(1000, 9)\n",
      "step 1 loss 5.947\n",
      "delta -0.014680386\n",
      "(1000, 9)\n",
      "step 2 loss 5.933\n",
      "delta -0.014676571\n",
      "(1000, 9)\n",
      "step 3 loss 5.918\n",
      "delta -0.014670372\n",
      "(1000, 9)\n",
      "step 4 loss 5.903\n",
      "delta -0.014661789\n",
      "(1000, 9)\n",
      "step 5 loss 5.889\n",
      "delta -0.014658928\n",
      "(1000, 9)\n",
      "step 6 loss 5.874\n",
      "delta -0.014652252\n",
      "(1000, 9)\n",
      "step 7 loss 5.859\n",
      "delta -0.014641285\n",
      "(1000, 9)\n",
      "step 8 loss 5.845\n",
      "delta -0.014639378\n",
      "(1000, 9)\n",
      "step 9 loss 5.830\n",
      "delta -0.014634132\n",
      "(1000, 9)\n",
      "step 10 loss 5.816\n",
      "delta -0.014625549\n",
      "(1000, 9)\n",
      "step 11 loss 5.801\n",
      "delta -0.014622688\n",
      "(1000, 9)\n",
      "step 12 loss 5.786\n",
      "delta -0.014611721\n",
      "(1000, 9)\n",
      "step 13 loss 5.772\n",
      "delta -0.014611721\n",
      "(1000, 9)\n",
      "step 14 loss 5.757\n",
      "delta -0.014601231\n",
      "(1000, 9)\n",
      "step 15 loss 5.742\n",
      "delta -0.014595985\n",
      "(1000, 9)\n",
      "step 16 loss 5.728\n",
      "delta -0.01458931\n",
      "(1000, 9)\n",
      "step 17 loss 5.713\n",
      "delta -0.014585495\n",
      "(1000, 9)\n",
      "step 18 loss 5.699\n",
      "delta -0.014575481\n",
      "(1000, 9)\n",
      "step 19 loss 5.684\n",
      "delta -0.014574051\n",
      "(1000, 9)\n",
      "step 20 loss 5.670\n",
      "delta -0.014564514\n",
      "(1000, 9)\n",
      "step 21 loss 5.655\n",
      "delta -0.014558792\n",
      "(1000, 9)\n",
      "step 22 loss 5.640\n",
      "delta -0.01455164\n",
      "(1000, 9)\n",
      "step 23 loss 5.626\n",
      "delta -0.014547825\n",
      "(1000, 9)\n",
      "step 24 loss 5.611\n",
      "delta -0.014538288\n",
      "(1000, 9)\n",
      "step 25 loss 5.597\n",
      "delta -0.014536858\n",
      "(1000, 9)\n",
      "step 26 loss 5.582\n",
      "delta -0.014528275\n",
      "(1000, 9)\n",
      "step 27 loss 5.568\n",
      "delta -0.014521599\n",
      "(1000, 9)\n",
      "step 28 loss 5.553\n",
      "delta -0.014516354\n",
      "(1000, 9)\n",
      "step 29 loss 5.539\n",
      "delta -0.014508724\n",
      "(1000, 9)\n",
      "step 30 loss 5.524\n",
      "delta -0.014504433\n",
      "(1000, 9)\n",
      "step 31 loss 5.510\n",
      "delta -0.014498234\n",
      "(1000, 9)\n",
      "step 32 loss 5.495\n",
      "delta -0.014491081\n",
      "(1000, 9)\n",
      "step 33 loss 5.481\n",
      "delta -0.014482498\n",
      "(1000, 9)\n",
      "step 34 loss 5.466\n",
      "delta -0.014480591\n",
      "(1000, 9)\n",
      "step 35 loss 5.452\n",
      "delta -0.014472485\n",
      "(1000, 9)\n",
      "step 36 loss 5.437\n",
      "delta -0.014466286\n",
      "(1000, 9)\n",
      "step 37 loss 5.423\n",
      "delta -0.01445961\n",
      "(1000, 9)\n",
      "step 38 loss 5.408\n",
      "delta -0.014454365\n",
      "(1000, 9)\n",
      "step 39 loss 5.394\n",
      "delta -0.014446259\n",
      "(1000, 9)\n",
      "step 40 loss 5.380\n",
      "delta -0.014441013\n",
      "(1000, 9)\n",
      "step 41 loss 5.365\n",
      "delta -0.014435291\n",
      "(1000, 9)\n",
      "step 42 loss 5.351\n",
      "delta -0.014428616\n",
      "(1000, 9)\n",
      "step 43 loss 5.336\n",
      "delta -0.0144228935\n",
      "(1000, 9)\n",
      "step 44 loss 5.322\n",
      "delta -0.014414787\n",
      "(1000, 9)\n",
      "step 45 loss 5.307\n",
      "delta -0.014410496\n",
      "(1000, 9)\n",
      "step 46 loss 5.293\n",
      "delta -0.01440382\n",
      "(1000, 9)\n",
      "step 47 loss 5.279\n",
      "delta -0.014397144\n",
      "(1000, 9)\n",
      "step 48 loss 5.264\n",
      "delta -0.014390945\n",
      "(1000, 9)\n",
      "step 49 loss 5.250\n",
      "delta -0.014382839\n",
      "(1000, 9)\n",
      "step 50 loss 5.236\n",
      "delta -0.014377117\n",
      "(1000, 9)\n",
      "step 51 loss 5.221\n",
      "delta -0.014370918\n",
      "(1000, 9)\n",
      "step 52 loss 5.207\n",
      "delta -0.014365673\n",
      "(1000, 9)\n",
      "step 53 loss 5.192\n",
      "delta -0.014359474\n",
      "(1000, 9)\n",
      "step 54 loss 5.178\n",
      "delta -0.014350891\n",
      "(1000, 9)\n",
      "step 55 loss 5.164\n",
      "delta -0.014345169\n",
      "(1000, 9)\n",
      "step 56 loss 5.149\n",
      "delta -0.014339447\n",
      "(1000, 9)\n",
      "step 57 loss 5.135\n",
      "delta -0.014332771\n",
      "(1000, 9)\n",
      "step 58 loss 5.121\n",
      "delta -0.014326096\n",
      "(1000, 9)\n",
      "step 59 loss 5.106\n",
      "delta -0.01431942\n",
      "(1000, 9)\n",
      "step 60 loss 5.092\n",
      "delta -0.014313698\n",
      "(1000, 9)\n",
      "step 61 loss 5.078\n",
      "delta -0.014305592\n",
      "(1000, 9)\n",
      "step 62 loss 5.063\n",
      "delta -0.014300346\n",
      "(1000, 9)\n",
      "step 63 loss 5.049\n",
      "delta -0.014293194\n",
      "(1000, 9)\n",
      "step 64 loss 5.035\n",
      "delta -0.014287472\n",
      "(1000, 9)\n",
      "step 65 loss 5.021\n",
      "delta -0.014280796\n",
      "(1000, 9)\n",
      "step 66 loss 5.006\n",
      "delta -0.01427269\n",
      "(1000, 9)\n",
      "step 67 loss 4.992\n",
      "delta -0.014267921\n",
      "(1000, 9)\n",
      "step 68 loss 4.978\n",
      "delta -0.014258385\n",
      "(1000, 9)\n",
      "step 69 loss 4.964\n",
      "delta -0.014253616\n",
      "(1000, 9)\n",
      "step 70 loss 4.949\n",
      "delta -0.014246941\n",
      "(1000, 9)\n",
      "step 71 loss 4.935\n",
      "delta -0.014238834\n",
      "(1000, 9)\n",
      "step 72 loss 4.921\n",
      "delta -0.0142326355\n",
      "(1000, 9)\n",
      "step 73 loss 4.907\n",
      "delta -0.01422596\n",
      "(1000, 9)\n",
      "step 74 loss 4.892\n",
      "delta -0.014221191\n",
      "(1000, 9)\n",
      "step 75 loss 4.878\n",
      "delta -0.014211655\n",
      "(1000, 9)\n",
      "step 76 loss 4.864\n",
      "delta -0.014206886\n",
      "(1000, 9)\n",
      "step 77 loss 4.850\n",
      "delta -0.01419878\n",
      "(1000, 9)\n",
      "step 78 loss 4.836\n",
      "delta -0.0141916275\n",
      "(1000, 9)\n",
      "step 79 loss 4.821\n",
      "delta -0.014185429\n",
      "(1000, 9)\n",
      "step 80 loss 4.807\n",
      "delta -0.014178276\n",
      "(1000, 9)\n",
      "step 81 loss 4.793\n",
      "delta -0.014172554\n",
      "(1000, 9)\n",
      "step 82 loss 4.779\n",
      "delta -0.014163494\n",
      "(1000, 9)\n",
      "step 83 loss 4.765\n",
      "delta -0.014158249\n",
      "(1000, 9)\n",
      "step 84 loss 4.751\n",
      "delta -0.014149666\n",
      "(1000, 9)\n",
      "step 85 loss 4.736\n",
      "delta -0.01414299\n",
      "(1000, 9)\n",
      "step 86 loss 4.722\n",
      "delta -0.014134884\n",
      "(1000, 9)\n",
      "step 87 loss 4.708\n",
      "delta -0.014128685\n",
      "(1000, 9)\n",
      "step 88 loss 4.694\n",
      "delta -0.014122486\n",
      "(1000, 9)\n",
      "step 89 loss 4.680\n",
      "delta -0.014114857\n",
      "(1000, 9)\n",
      "step 90 loss 4.666\n",
      "delta -0.0141067505\n",
      "(1000, 9)\n",
      "step 91 loss 4.652\n",
      "delta -0.014099598\n",
      "(1000, 9)\n",
      "step 92 loss 4.638\n",
      "delta -0.014092922\n",
      "(1000, 9)\n",
      "step 93 loss 4.624\n",
      "delta -0.014084816\n",
      "(1000, 9)\n",
      "step 94 loss 4.609\n",
      "delta -0.01407814\n",
      "(1000, 9)\n",
      "step 95 loss 4.595\n",
      "delta -0.014070511\n",
      "(1000, 9)\n",
      "step 96 loss 4.581\n",
      "delta -0.014063835\n",
      "(1000, 9)\n",
      "step 97 loss 4.567\n",
      "delta -0.014056206\n",
      "(1000, 9)\n",
      "step 98 loss 4.553\n",
      "delta -0.014047623\n",
      "(1000, 9)\n",
      "step 99 loss 4.539\n",
      "delta -0.01404047\n",
      "(1000, 9)\n",
      "step 100 loss 4.525\n",
      "delta -0.014032841\n",
      "(1000, 9)\n",
      "step 101 loss 4.511\n",
      "delta -0.014025688\n",
      "(1000, 9)\n",
      "step 102 loss 4.497\n",
      "delta -0.014017105\n",
      "(1000, 9)\n",
      "step 103 loss 4.483\n",
      "delta -0.014010429\n",
      "(1000, 9)\n",
      "step 104 loss 4.469\n",
      "delta -0.014003277\n",
      "(1000, 9)\n",
      "step 105 loss 4.455\n",
      "delta -0.013994217\n",
      "(1000, 9)\n",
      "step 106 loss 4.441\n",
      "delta -0.013987064\n",
      "(1000, 9)\n",
      "step 107 loss 4.427\n",
      "delta -0.013978958\n",
      "(1000, 9)\n",
      "step 108 loss 4.413\n",
      "delta -0.013970375\n",
      "(1000, 9)\n",
      "step 109 loss 4.399\n",
      "delta -0.013963699\n",
      "(1000, 9)\n",
      "step 110 loss 4.385\n",
      "delta -0.01395607\n",
      "(1000, 9)\n",
      "step 111 loss 4.371\n",
      "delta -0.013946533\n",
      "(1000, 9)\n",
      "step 112 loss 4.357\n",
      "delta -0.013939381\n",
      "(1000, 9)\n",
      "step 113 loss 4.343\n",
      "delta -0.013932228\n",
      "(1000, 9)\n",
      "step 114 loss 4.330\n",
      "delta -0.013922691\n",
      "(1000, 9)\n",
      "step 115 loss 4.316\n",
      "delta -0.013914585\n",
      "(1000, 9)\n",
      "step 116 loss 4.302\n",
      "delta -0.013906956\n",
      "(1000, 9)\n",
      "step 117 loss 4.288\n",
      "delta -0.013898373\n",
      "(1000, 9)\n",
      "step 118 loss 4.274\n",
      "delta -0.013890266\n",
      "(1000, 9)\n",
      "step 119 loss 4.260\n",
      "delta -0.0138812065\n",
      "(1000, 9)\n",
      "step 120 loss 4.246\n",
      "delta -0.0138731\n",
      "(1000, 9)\n",
      "step 121 loss 4.232\n",
      "delta -0.013864517\n",
      "(1000, 9)\n",
      "step 122 loss 4.218\n",
      "delta -0.013856411\n",
      "(1000, 9)\n",
      "step 123 loss 4.205\n",
      "delta -0.013846874\n",
      "(1000, 9)\n",
      "step 124 loss 4.191\n",
      "delta -0.013839722\n",
      "(1000, 9)\n",
      "step 125 loss 4.177\n",
      "delta -0.013830185\n",
      "(1000, 9)\n",
      "step 126 loss 4.163\n",
      "delta -0.013821602\n",
      "(1000, 9)\n",
      "step 127 loss 4.149\n",
      "delta -0.013812065\n",
      "(1000, 9)\n",
      "step 128 loss 4.135\n",
      "delta -0.013804436\n",
      "(1000, 9)\n",
      "step 129 loss 4.122\n",
      "delta -0.013794422\n",
      "(1000, 9)\n",
      "step 130 loss 4.108\n",
      "delta -0.013785362\n",
      "(1000, 9)\n",
      "step 131 loss 4.094\n",
      "delta -0.013777733\n",
      "(1000, 9)\n",
      "step 132 loss 4.080\n",
      "delta -0.013767719\n",
      "(1000, 9)\n",
      "step 133 loss 4.067\n",
      "delta -0.013759136\n",
      "(1000, 9)\n",
      "step 134 loss 4.053\n",
      "delta -0.013748169\n",
      "(1000, 9)\n",
      "step 135 loss 4.039\n",
      "delta -0.013741016\n",
      "(1000, 9)\n",
      "step 136 loss 4.025\n",
      "delta -0.013730049\n",
      "(1000, 9)\n",
      "step 137 loss 4.012\n",
      "delta -0.013721466\n",
      "(1000, 9)\n",
      "step 138 loss 3.998\n",
      "delta -0.013711691\n",
      "(1000, 9)\n",
      "step 139 loss 3.984\n",
      "delta -0.013701677\n",
      "(1000, 9)\n",
      "step 140 loss 3.971\n",
      "delta -0.013692856\n",
      "(1000, 9)\n",
      "step 141 loss 3.957\n",
      "delta -0.0136835575\n",
      "(1000, 9)\n",
      "step 142 loss 3.943\n",
      "delta -0.013672829\n",
      "(1000, 9)\n",
      "step 143 loss 3.930\n",
      "delta -0.013664007\n",
      "(1000, 9)\n",
      "step 144 loss 3.916\n",
      "delta -0.0136528015\n",
      "(1000, 9)\n",
      "step 145 loss 3.902\n",
      "delta -0.013643026\n",
      "(1000, 9)\n",
      "step 146 loss 3.889\n",
      "delta -0.013633728\n",
      "(1000, 9)\n",
      "step 147 loss 3.875\n",
      "delta -0.013622761\n",
      "(1000, 9)\n",
      "step 148 loss 3.861\n",
      "delta -0.013613462\n",
      "(1000, 9)\n",
      "step 149 loss 3.848\n",
      "delta -0.013602972\n",
      "(1000, 9)\n",
      "step 150 loss 3.834\n",
      "delta -0.013593197\n",
      "(1000, 9)\n",
      "step 151 loss 3.821\n",
      "delta -0.013581991\n",
      "(1000, 9)\n",
      "step 152 loss 3.807\n",
      "delta -0.013571024\n",
      "(1000, 9)\n",
      "step 153 loss 3.793\n",
      "delta -0.01356101\n",
      "(1000, 9)\n",
      "step 154 loss 3.780\n",
      "delta -0.013550758\n",
      "(1000, 9)\n",
      "step 155 loss 3.766\n",
      "delta -0.013539314\n",
      "(1000, 9)\n",
      "step 156 loss 3.753\n",
      "delta -0.013528585\n",
      "(1000, 9)\n",
      "step 157 loss 3.739\n",
      "delta -0.013517857\n",
      "(1000, 9)\n",
      "step 158 loss 3.726\n",
      "delta -0.0135064125\n",
      "(1000, 9)\n",
      "step 159 loss 3.712\n",
      "delta -0.0134961605\n",
      "(1000, 9)\n",
      "step 160 loss 3.699\n",
      "delta -0.01348424\n",
      "(1000, 9)\n",
      "step 161 loss 3.685\n",
      "delta -0.013473749\n",
      "(1000, 9)\n",
      "step 162 loss 3.672\n",
      "delta -0.013461351\n",
      "(1000, 9)\n",
      "step 163 loss 3.658\n",
      "delta -0.013449907\n",
      "(1000, 9)\n",
      "step 164 loss 3.645\n",
      "delta -0.013439894\n",
      "(1000, 9)\n",
      "step 165 loss 3.632\n",
      "delta -0.013426065\n",
      "(1000, 9)\n",
      "step 166 loss 3.618\n",
      "delta -0.013415813\n",
      "(1000, 9)\n",
      "step 167 loss 3.605\n",
      "delta -0.013404131\n",
      "(1000, 9)\n",
      "step 168 loss 3.591\n",
      "delta -0.013391495\n",
      "(1000, 9)\n",
      "step 169 loss 3.578\n",
      "delta -0.013379812\n",
      "(1000, 9)\n",
      "step 170 loss 3.565\n",
      "delta -0.013366938\n",
      "(1000, 9)\n",
      "step 171 loss 3.551\n",
      "delta -0.01335454\n",
      "(1000, 9)\n",
      "step 172 loss 3.538\n",
      "delta -0.013343334\n",
      "(1000, 9)\n",
      "step 173 loss 3.525\n",
      "delta -0.013331413\n",
      "(1000, 9)\n",
      "step 174 loss 3.511\n",
      "delta -0.013318062\n",
      "(1000, 9)\n",
      "step 175 loss 3.498\n",
      "delta -0.01330471\n",
      "(1000, 9)\n",
      "step 176 loss 3.485\n",
      "delta -0.013292551\n",
      "(1000, 9)\n",
      "step 177 loss 3.471\n",
      "delta -0.013279676\n",
      "(1000, 9)\n",
      "step 178 loss 3.458\n",
      "delta -0.013266563\n",
      "(1000, 9)\n",
      "step 179 loss 3.445\n",
      "delta -0.01325345\n",
      "(1000, 9)\n",
      "step 180 loss 3.432\n",
      "delta -0.013239861\n",
      "(1000, 9)\n",
      "step 181 loss 3.418\n",
      "delta -0.013228178\n",
      "(1000, 9)\n",
      "step 182 loss 3.405\n",
      "delta -0.0132136345\n",
      "(1000, 9)\n",
      "step 183 loss 3.392\n",
      "delta -0.013200283\n",
      "(1000, 9)\n",
      "step 184 loss 3.379\n",
      "delta -0.013186455\n",
      "(1000, 9)\n",
      "step 185 loss 3.366\n",
      "delta -0.013172388\n",
      "(1000, 9)\n",
      "step 186 loss 3.352\n",
      "delta -0.01315856\n",
      "(1000, 9)\n",
      "step 187 loss 3.339\n",
      "delta -0.013144255\n",
      "(1000, 9)\n",
      "step 188 loss 3.326\n",
      "delta -0.013131142\n",
      "(1000, 9)\n",
      "step 189 loss 3.313\n",
      "delta -0.013116121\n",
      "(1000, 9)\n",
      "step 190 loss 3.300\n",
      "delta -0.0131008625\n",
      "(1000, 9)\n",
      "step 191 loss 3.287\n",
      "delta -0.013087988\n",
      "(1000, 9)\n",
      "step 192 loss 3.274\n",
      "delta -0.013072014\n",
      "(1000, 9)\n",
      "step 193 loss 3.261\n",
      "delta -0.013057947\n",
      "(1000, 9)\n",
      "step 194 loss 3.248\n",
      "delta -0.013042927\n",
      "(1000, 9)\n",
      "step 195 loss 3.235\n",
      "delta -0.013028145\n",
      "(1000, 9)\n",
      "step 196 loss 3.222\n",
      "delta -0.013012171\n",
      "(1000, 9)\n",
      "step 197 loss 3.209\n",
      "delta -0.012997627\n",
      "(1000, 9)\n",
      "step 198 loss 3.196\n",
      "delta -0.01298213\n",
      "(1000, 9)\n",
      "step 199 loss 3.183\n",
      "delta -0.012966394\n",
      "(1000, 9)\n",
      "step 200 loss 3.170\n",
      "delta -0.012949944\n",
      "(1000, 9)\n",
      "step 201 loss 3.157\n",
      "delta -0.0129339695\n",
      "(1000, 9)\n",
      "step 202 loss 3.144\n",
      "delta -0.012918711\n",
      "(1000, 9)\n",
      "step 203 loss 3.131\n",
      "delta -0.01290226\n",
      "(1000, 9)\n",
      "step 204 loss 3.118\n",
      "delta -0.012886047\n",
      "(1000, 9)\n",
      "step 205 loss 3.105\n",
      "delta -0.012870073\n",
      "(1000, 9)\n",
      "step 206 loss 3.092\n",
      "delta -0.012852669\n",
      "(1000, 9)\n",
      "step 207 loss 3.080\n",
      "delta -0.012835503\n",
      "(1000, 9)\n",
      "step 208 loss 3.067\n",
      "delta -0.012819529\n",
      "(1000, 9)\n",
      "step 209 loss 3.054\n",
      "delta -0.012801886\n",
      "(1000, 9)\n",
      "step 210 loss 3.041\n",
      "delta -0.0127847195\n",
      "(1000, 9)\n",
      "step 211 loss 3.028\n",
      "delta -0.012767553\n",
      "(1000, 9)\n",
      "step 212 loss 3.016\n",
      "delta -0.012750626\n",
      "(1000, 9)\n",
      "step 213 loss 3.003\n",
      "delta -0.012732029\n",
      "(1000, 9)\n",
      "step 214 loss 2.990\n",
      "delta -0.012714624\n",
      "(1000, 9)\n",
      "step 215 loss 2.978\n",
      "delta -0.012696028\n",
      "(1000, 9)\n",
      "step 216 loss 2.965\n",
      "delta -0.012677908\n",
      "(1000, 9)\n",
      "step 217 loss 2.952\n",
      "delta -0.012660027\n",
      "(1000, 9)\n",
      "step 218 loss 2.940\n",
      "delta -0.01264143\n",
      "(1000, 9)\n",
      "step 219 loss 2.927\n",
      "delta -0.01262188\n",
      "(1000, 9)\n",
      "step 220 loss 2.914\n",
      "delta -0.01260376\n",
      "(1000, 9)\n",
      "step 221 loss 2.902\n",
      "delta -0.01258564\n",
      "(1000, 9)\n",
      "step 222 loss 2.889\n",
      "delta -0.012565613\n",
      "(1000, 9)\n",
      "step 223 loss 2.877\n",
      "delta -0.012545347\n",
      "(1000, 9)\n",
      "step 224 loss 2.864\n",
      "delta -0.012526512\n",
      "(1000, 9)\n",
      "step 225 loss 2.852\n",
      "delta -0.0125079155\n",
      "(1000, 9)\n",
      "step 226 loss 2.839\n",
      "delta -0.012486219\n",
      "(1000, 9)\n",
      "step 227 loss 2.827\n",
      "delta -0.012467146\n",
      "(1000, 9)\n",
      "step 228 loss 2.814\n",
      "delta -0.012446642\n",
      "(1000, 9)\n",
      "step 229 loss 2.802\n",
      "delta -0.012426615\n",
      "(1000, 9)\n",
      "step 230 loss 2.789\n",
      "delta -0.0124053955\n",
      "(1000, 9)\n",
      "step 231 loss 2.777\n",
      "delta -0.0123848915\n",
      "(1000, 9)\n",
      "step 232 loss 2.765\n",
      "delta -0.012364149\n",
      "(1000, 9)\n",
      "step 233 loss 2.752\n",
      "delta -0.012342691\n",
      "(1000, 9)\n",
      "step 234 loss 2.740\n",
      "delta -0.012321234\n",
      "(1000, 9)\n",
      "step 235 loss 2.728\n",
      "delta -0.0123000145\n",
      "(1000, 9)\n",
      "step 236 loss 2.715\n",
      "delta -0.01227808\n",
      "(1000, 9)\n",
      "step 237 loss 2.703\n",
      "delta -0.012256622\n",
      "(1000, 9)\n",
      "step 238 loss 2.691\n",
      "delta -0.012233973\n",
      "(1000, 9)\n",
      "step 239 loss 2.679\n",
      "delta -0.012212276\n",
      "(1000, 9)\n",
      "step 240 loss 2.666\n",
      "delta -0.012188673\n",
      "(1000, 9)\n",
      "step 241 loss 2.654\n",
      "delta -0.0121665\n",
      "(1000, 9)\n",
      "step 242 loss 2.642\n",
      "delta -0.012144327\n",
      "(1000, 9)\n",
      "step 243 loss 2.630\n",
      "delta -0.012120962\n",
      "(1000, 9)\n",
      "step 244 loss 2.618\n",
      "delta -0.01209712\n",
      "(1000, 9)\n",
      "step 245 loss 2.606\n",
      "delta -0.012074232\n",
      "(1000, 9)\n",
      "step 246 loss 2.594\n",
      "delta -0.012050152\n",
      "(1000, 9)\n",
      "step 247 loss 2.582\n",
      "delta -0.012026548\n",
      "(1000, 9)\n",
      "step 248 loss 2.570\n",
      "delta -0.012002468\n",
      "(1000, 9)\n",
      "step 249 loss 2.558\n",
      "delta -0.011977911\n",
      "(1000, 9)\n",
      "step 250 loss 2.546\n",
      "delta -0.011953592\n",
      "(1000, 9)\n",
      "step 251 loss 2.534\n",
      "delta -0.011929035\n",
      "(1000, 9)\n",
      "step 252 loss 2.522\n",
      "delta -0.011903763\n",
      "(1000, 9)\n",
      "step 253 loss 2.510\n",
      "delta -0.011879444\n",
      "(1000, 9)\n",
      "step 254 loss 2.498\n",
      "delta -0.011854172\n",
      "(1000, 9)\n",
      "step 255 loss 2.486\n",
      "delta -0.011827707\n",
      "(1000, 9)\n",
      "step 256 loss 2.475\n",
      "delta -0.011802435\n",
      "(1000, 9)\n",
      "step 257 loss 2.463\n",
      "delta -0.011776209\n",
      "(1000, 9)\n",
      "step 258 loss 2.451\n",
      "delta -0.011750221\n",
      "(1000, 9)\n",
      "step 259 loss 2.439\n",
      "delta -0.011723757\n",
      "(1000, 9)\n",
      "step 260 loss 2.428\n",
      "delta -0.011697292\n",
      "(1000, 9)\n",
      "step 261 loss 2.416\n",
      "delta -0.011670828\n",
      "(1000, 9)\n",
      "step 262 loss 2.404\n",
      "delta -0.011643171\n",
      "(1000, 9)\n",
      "step 263 loss 2.393\n",
      "delta -0.011616707\n",
      "(1000, 9)\n",
      "step 264 loss 2.381\n",
      "delta -0.011588812\n",
      "(1000, 9)\n",
      "step 265 loss 2.370\n",
      "delta -0.011561632\n",
      "(1000, 9)\n",
      "step 266 loss 2.358\n",
      "delta -0.01153326\n",
      "(1000, 9)\n",
      "step 267 loss 2.347\n",
      "delta -0.011505365\n",
      "(1000, 9)\n",
      "step 268 loss 2.335\n",
      "delta -0.011476755\n",
      "(1000, 9)\n",
      "step 269 loss 2.324\n",
      "delta -0.011448622\n",
      "(1000, 9)\n",
      "step 270 loss 2.312\n",
      "delta -0.011419296\n",
      "(1000, 9)\n",
      "step 271 loss 2.301\n",
      "delta -0.011391163\n",
      "(1000, 9)\n",
      "step 272 loss 2.289\n",
      "delta -0.011361361\n",
      "(1000, 9)\n",
      "step 273 loss 2.278\n",
      "delta -0.011333227\n",
      "(1000, 9)\n",
      "step 274 loss 2.267\n",
      "delta -0.011302471\n",
      "(1000, 9)\n",
      "step 275 loss 2.256\n",
      "delta -0.01127243\n",
      "(1000, 9)\n",
      "step 276 loss 2.244\n",
      "delta -0.011243105\n",
      "(1000, 9)\n",
      "step 277 loss 2.233\n",
      "delta -0.011212826\n",
      "(1000, 9)\n",
      "step 278 loss 2.222\n",
      "delta -0.011182308\n",
      "(1000, 9)\n",
      "step 279 loss 2.211\n",
      "delta -0.011151552\n",
      "(1000, 9)\n",
      "step 280 loss 2.200\n",
      "delta -0.011120796\n",
      "(1000, 9)\n",
      "step 281 loss 2.189\n",
      "delta -0.011089325\n",
      "(1000, 9)\n",
      "step 282 loss 2.178\n",
      "delta -0.011058092\n",
      "(1000, 9)\n",
      "step 283 loss 2.167\n",
      "delta -0.011026621\n",
      "(1000, 9)\n",
      "step 284 loss 2.156\n",
      "delta -0.010994673\n",
      "(1000, 9)\n",
      "step 285 loss 2.145\n",
      "delta -0.0109632015\n",
      "(1000, 9)\n",
      "step 286 loss 2.134\n",
      "delta -0.010930538\n",
      "(1000, 9)\n",
      "step 287 loss 2.123\n",
      "delta -0.010898352\n",
      "(1000, 9)\n",
      "step 288 loss 2.112\n",
      "delta -0.010865688\n",
      "(1000, 9)\n",
      "step 289 loss 2.101\n",
      "delta -0.010833025\n",
      "(1000, 9)\n",
      "step 290 loss 2.090\n",
      "delta -0.010799408\n",
      "(1000, 9)\n",
      "step 291 loss 2.079\n",
      "delta -0.010766506\n",
      "(1000, 9)\n",
      "step 292 loss 2.069\n",
      "delta -0.010732889\n",
      "(1000, 9)\n",
      "step 293 loss 2.058\n",
      "delta -0.010698795\n",
      "(1000, 9)\n",
      "step 294 loss 2.047\n",
      "delta -0.010665655\n",
      "(1000, 9)\n",
      "step 295 loss 2.037\n",
      "delta -0.010631323\n",
      "(1000, 9)\n",
      "step 296 loss 2.026\n",
      "delta -0.010596514\n",
      "(1000, 9)\n",
      "step 297 loss 2.016\n",
      "delta -0.01056242\n",
      "(1000, 9)\n",
      "step 298 loss 2.005\n",
      "delta -0.010527611\n",
      "(1000, 9)\n",
      "step 299 loss 1.995\n",
      "delta -0.010492444\n",
      "(1000, 9)\n",
      "step 300 loss 1.984\n",
      "delta -0.010457754\n",
      "(1000, 9)\n",
      "step 301 loss 1.974\n",
      "delta -0.010422111\n",
      "(1000, 9)\n",
      "step 302 loss 1.963\n",
      "delta -0.010386467\n",
      "(1000, 9)\n",
      "step 303 loss 1.953\n",
      "delta -0.010350823\n",
      "(1000, 9)\n",
      "step 304 loss 1.943\n",
      "delta -0.010314822\n",
      "(1000, 9)\n",
      "step 305 loss 1.932\n",
      "delta -0.010278463\n",
      "(1000, 9)\n",
      "step 306 loss 1.922\n",
      "delta -0.010242581\n",
      "(1000, 9)\n",
      "step 307 loss 1.912\n",
      "delta -0.010205746\n",
      "(1000, 9)\n",
      "step 308 loss 1.902\n",
      "delta -0.01016891\n",
      "(1000, 9)\n",
      "step 309 loss 1.892\n",
      "delta -0.010131836\n",
      "(1000, 9)\n",
      "step 310 loss 1.881\n",
      "delta -0.0100951195\n",
      "(1000, 9)\n",
      "step 311 loss 1.871\n",
      "delta -0.010057807\n",
      "(1000, 9)\n",
      "step 312 loss 1.861\n",
      "delta -0.010020137\n",
      "(1000, 9)\n",
      "step 313 loss 1.851\n",
      "delta -0.009982228\n",
      "(1000, 9)\n",
      "step 314 loss 1.841\n",
      "delta -0.009944439\n",
      "(1000, 9)\n",
      "step 315 loss 1.832\n",
      "delta -0.00990653\n",
      "(1000, 9)\n",
      "step 316 loss 1.822\n",
      "delta -0.009868145\n",
      "(1000, 9)\n",
      "step 317 loss 1.812\n",
      "delta -0.00982976\n",
      "(1000, 9)\n",
      "step 318 loss 1.802\n",
      "delta -0.009791732\n",
      "(1000, 9)\n",
      "step 319 loss 1.792\n",
      "delta -0.009752631\n",
      "(1000, 9)\n",
      "step 320 loss 1.783\n",
      "delta -0.009713411\n",
      "(1000, 9)\n",
      "step 321 loss 1.773\n",
      "delta -0.0096747875\n",
      "(1000, 9)\n",
      "step 322 loss 1.763\n",
      "delta -0.00963521\n",
      "(1000, 9)\n",
      "step 323 loss 1.754\n",
      "delta -0.009595633\n",
      "(1000, 9)\n",
      "step 324 loss 1.744\n",
      "delta -0.009556413\n",
      "(1000, 9)\n",
      "step 325 loss 1.735\n",
      "delta -0.009516835\n",
      "(1000, 9)\n",
      "step 326 loss 1.725\n",
      "delta -0.009476662\n",
      "(1000, 9)\n",
      "step 327 loss 1.716\n",
      "delta -0.009436965\n",
      "(1000, 9)\n",
      "step 328 loss 1.706\n",
      "delta -0.009396553\n",
      "(1000, 9)\n",
      "step 329 loss 1.697\n",
      "delta -0.009356022\n",
      "(1000, 9)\n",
      "step 330 loss 1.688\n",
      "delta -0.009315729\n",
      "(1000, 9)\n",
      "step 331 loss 1.678\n",
      "delta -0.009275317\n",
      "(1000, 9)\n",
      "step 332 loss 1.669\n",
      "delta -0.00923419\n",
      "(1000, 9)\n",
      "step 333 loss 1.660\n",
      "delta -0.009193778\n",
      "(1000, 9)\n",
      "step 334 loss 1.651\n",
      "delta -0.009152293\n",
      "(1000, 9)\n",
      "step 335 loss 1.642\n",
      "delta -0.009111285\n",
      "(1000, 9)\n",
      "step 336 loss 1.633\n",
      "delta -0.0090698\n",
      "(1000, 9)\n",
      "step 337 loss 1.624\n",
      "delta -0.00902915\n",
      "(1000, 9)\n",
      "step 338 loss 1.615\n",
      "delta -0.00898695\n",
      "(1000, 9)\n",
      "step 339 loss 1.606\n",
      "delta -0.008945823\n",
      "(1000, 9)\n",
      "step 340 loss 1.597\n",
      "delta -0.008903742\n",
      "(1000, 9)\n",
      "step 341 loss 1.588\n",
      "delta -0.008861899\n",
      "(1000, 9)\n",
      "step 342 loss 1.579\n",
      "delta -0.008820057\n",
      "(1000, 9)\n",
      "step 343 loss 1.570\n",
      "delta -0.008777738\n",
      "(1000, 9)\n",
      "step 344 loss 1.562\n",
      "delta -0.008735776\n",
      "(1000, 9)\n",
      "step 345 loss 1.553\n",
      "delta -0.008693695\n",
      "(1000, 9)\n",
      "step 346 loss 1.544\n",
      "delta -0.008651018\n",
      "(1000, 9)\n",
      "step 347 loss 1.536\n",
      "delta -0.008608937\n",
      "(1000, 9)\n",
      "step 348 loss 1.527\n",
      "delta -0.00856638\n",
      "(1000, 9)\n",
      "step 349 loss 1.519\n",
      "delta -0.008523703\n",
      "(1000, 9)\n",
      "step 350 loss 1.510\n",
      "delta -0.008481145\n",
      "(1000, 9)\n",
      "step 351 loss 1.502\n",
      "delta -0.008437872\n",
      "(1000, 9)\n",
      "step 352 loss 1.493\n",
      "delta -0.008395553\n",
      "(1000, 9)\n",
      "step 353 loss 1.485\n",
      "delta -0.008352399\n",
      "(1000, 9)\n",
      "step 354 loss 1.477\n",
      "delta -0.008309603\n",
      "(1000, 9)\n",
      "step 355 loss 1.468\n",
      "delta -0.008266568\n",
      "(1000, 9)\n",
      "step 356 loss 1.460\n",
      "delta -0.008223772\n",
      "(1000, 9)\n",
      "step 357 loss 1.452\n",
      "delta -0.008180141\n",
      "(1000, 9)\n",
      "step 358 loss 1.444\n",
      "delta -0.008137345\n",
      "(1000, 9)\n",
      "step 359 loss 1.436\n",
      "delta -0.008093953\n",
      "(1000, 9)\n",
      "step 360 loss 1.428\n",
      "delta -0.008050799\n",
      "(1000, 9)\n",
      "step 361 loss 1.420\n",
      "delta -0.008007646\n",
      "(1000, 9)\n",
      "step 362 loss 1.412\n",
      "delta -0.007963777\n",
      "(1000, 9)\n",
      "step 363 loss 1.404\n",
      "delta -0.00792098\n",
      "(1000, 9)\n",
      "step 364 loss 1.396\n",
      "delta -0.007877588\n",
      "(1000, 9)\n",
      "step 365 loss 1.388\n",
      "delta -0.007834077\n",
      "(1000, 9)\n",
      "step 366 loss 1.380\n",
      "delta -0.0077904463\n",
      "(1000, 9)\n",
      "step 367 loss 1.372\n",
      "delta -0.0077472925\n",
      "(1000, 9)\n",
      "step 368 loss 1.365\n",
      "delta -0.0077039003\n",
      "(1000, 9)\n",
      "step 369 loss 1.357\n",
      "delta -0.0076600313\n",
      "(1000, 9)\n",
      "step 370 loss 1.349\n",
      "delta -0.0076173544\n",
      "(1000, 9)\n",
      "step 371 loss 1.342\n",
      "delta -0.007573247\n",
      "(1000, 9)\n",
      "step 372 loss 1.334\n",
      "delta -0.007530093\n",
      "(1000, 9)\n",
      "step 373 loss 1.327\n",
      "delta -0.007486582\n",
      "(1000, 9)\n",
      "step 374 loss 1.319\n",
      "delta -0.007442951\n",
      "(1000, 9)\n",
      "step 375 loss 1.312\n",
      "delta -0.0073997974\n",
      "(1000, 9)\n",
      "step 376 loss 1.305\n",
      "delta -0.0073564053\n",
      "(1000, 9)\n",
      "step 377 loss 1.297\n",
      "delta -0.00731349\n",
      "(1000, 9)\n",
      "step 378 loss 1.290\n",
      "delta -0.0072693825\n",
      "(1000, 9)\n",
      "step 379 loss 1.283\n",
      "delta -0.0072267056\n",
      "(1000, 9)\n",
      "step 380 loss 1.276\n",
      "delta -0.0071828365\n",
      "(1000, 9)\n",
      "step 381 loss 1.269\n",
      "delta -0.0071401596\n",
      "(1000, 9)\n",
      "step 382 loss 1.261\n",
      "delta -0.0070967674\n",
      "(1000, 9)\n",
      "step 383 loss 1.254\n",
      "delta -0.0070536137\n",
      "(1000, 9)\n",
      "step 384 loss 1.247\n",
      "delta -0.0070108175\n",
      "(1000, 9)\n",
      "step 385 loss 1.240\n",
      "delta -0.0069674253\n",
      "(1000, 9)\n",
      "step 386 loss 1.233\n",
      "delta -0.00692451\n",
      "(1000, 9)\n",
      "step 387 loss 1.227\n",
      "delta -0.006881714\n",
      "(1000, 9)\n",
      "step 388 loss 1.220\n",
      "delta -0.006839037\n",
      "(1000, 9)\n",
      "step 389 loss 1.213\n",
      "delta -0.00679636\n",
      "(1000, 9)\n",
      "step 390 loss 1.206\n",
      "delta -0.0067532063\n",
      "(1000, 9)\n",
      "step 391 loss 1.200\n",
      "delta -0.0067107677\n",
      "(1000, 9)\n",
      "step 392 loss 1.193\n",
      "delta -0.0066685677\n",
      "(1000, 9)\n",
      "step 393 loss 1.186\n",
      "delta -0.0066256523\n",
      "(1000, 9)\n",
      "step 394 loss 1.180\n",
      "delta -0.0065835714\n",
      "(1000, 9)\n",
      "step 395 loss 1.173\n",
      "delta -0.0065410137\n",
      "(1000, 9)\n",
      "step 396 loss 1.167\n",
      "delta -0.0064991713\n",
      "(1000, 9)\n",
      "step 397 loss 1.160\n",
      "delta -0.006456971\n",
      "(1000, 9)\n",
      "step 398 loss 1.154\n",
      "delta -0.0064150095\n",
      "(1000, 9)\n",
      "step 399 loss 1.147\n",
      "delta -0.00637269\n",
      "(1000, 9)\n",
      "step 400 loss 1.141\n",
      "delta -0.006331563\n",
      "(1000, 9)\n",
      "step 401 loss 1.135\n",
      "delta -0.0062892437\n",
      "(1000, 9)\n",
      "step 402 loss 1.128\n",
      "delta -0.0062482357\n",
      "(1000, 9)\n",
      "step 403 loss 1.122\n",
      "delta -0.0062063932\n",
      "(1000, 9)\n",
      "step 404 loss 1.116\n",
      "delta -0.006165147\n",
      "(1000, 9)\n",
      "step 405 loss 1.110\n",
      "delta -0.006124139\n",
      "(1000, 9)\n",
      "step 406 loss 1.104\n",
      "delta -0.006082535\n",
      "(1000, 9)\n",
      "step 407 loss 1.098\n",
      "delta -0.006042242\n",
      "(1000, 9)\n",
      "step 408 loss 1.092\n",
      "delta -0.0060009956\n",
      "(1000, 9)\n",
      "step 409 loss 1.086\n",
      "delta -0.0059604645\n",
      "(1000, 9)\n",
      "step 410 loss 1.080\n",
      "delta -0.005919814\n",
      "(1000, 9)\n",
      "step 411 loss 1.074\n",
      "delta -0.005879402\n",
      "(1000, 9)\n",
      "step 412 loss 1.068\n",
      "delta -0.0058391094\n",
      "(1000, 9)\n",
      "step 413 loss 1.062\n",
      "delta -0.0057988167\n",
      "(1000, 9)\n",
      "step 414 loss 1.057\n",
      "delta -0.0057588816\n",
      "(1000, 9)\n",
      "step 415 loss 1.051\n",
      "delta -0.005719185\n",
      "(1000, 9)\n",
      "step 416 loss 1.045\n",
      "delta -0.005679369\n",
      "(1000, 9)\n",
      "step 417 loss 1.040\n",
      "delta -0.0056397915\n",
      "(1000, 9)\n",
      "step 418 loss 1.034\n",
      "delta -0.005600333\n",
      "(1000, 9)\n",
      "step 419 loss 1.029\n",
      "delta -0.0055611134\n",
      "(1000, 9)\n",
      "step 420 loss 1.023\n",
      "delta -0.005522132\n",
      "(1000, 9)\n",
      "step 421 loss 1.017\n",
      "delta -0.0054830313\n",
      "(1000, 9)\n",
      "step 422 loss 1.012\n",
      "delta -0.0054444075\n",
      "(1000, 9)\n",
      "step 423 loss 1.007\n",
      "delta -0.0054056644\n",
      "(1000, 9)\n",
      "step 424 loss 1.001\n",
      "delta -0.005367279\n",
      "(1000, 9)\n",
      "step 425 loss 0.996\n",
      "delta -0.0053289533\n",
      "(1000, 9)\n",
      "step 426 loss 0.991\n",
      "delta -0.0052911043\n",
      "(1000, 9)\n",
      "step 427 loss 0.985\n",
      "delta -0.0052529573\n",
      "(1000, 9)\n",
      "step 428 loss 0.980\n",
      "delta -0.0052151084\n",
      "(1000, 9)\n",
      "step 429 loss 0.975\n",
      "delta -0.005177915\n",
      "(1000, 9)\n",
      "step 430 loss 0.970\n",
      "delta -0.0051401258\n",
      "(1000, 9)\n",
      "step 431 loss 0.965\n",
      "delta -0.0051031113\n",
      "(1000, 9)\n",
      "step 432 loss 0.960\n",
      "delta -0.005065918\n",
      "(1000, 9)\n",
      "step 433 loss 0.955\n",
      "delta -0.0050290823\n",
      "(1000, 9)\n",
      "step 434 loss 0.950\n",
      "delta -0.004992366\n",
      "(1000, 9)\n",
      "step 435 loss 0.945\n",
      "delta -0.0049559474\n",
      "(1000, 9)\n",
      "step 436 loss 0.940\n",
      "delta -0.004919708\n",
      "(1000, 9)\n",
      "step 437 loss 0.935\n",
      "delta -0.004883468\n",
      "(1000, 9)\n",
      "step 438 loss 0.930\n",
      "delta -0.0048476458\n",
      "(1000, 9)\n",
      "step 439 loss 0.925\n",
      "delta -0.004811585\n",
      "(1000, 9)\n",
      "step 440 loss 0.920\n",
      "delta -0.0047762394\n",
      "(1000, 9)\n",
      "step 441 loss 0.916\n",
      "delta -0.0047410727\n",
      "(1000, 9)\n",
      "step 442 loss 0.911\n",
      "delta -0.0047057867\n",
      "(1000, 9)\n",
      "step 443 loss 0.906\n",
      "delta -0.00467062\n",
      "(1000, 9)\n",
      "step 444 loss 0.902\n",
      "delta -0.0046359897\n",
      "(1000, 9)\n",
      "step 445 loss 0.897\n",
      "delta -0.004601419\n",
      "(1000, 9)\n",
      "step 446 loss 0.893\n",
      "delta -0.004567027\n",
      "(1000, 9)\n",
      "step 447 loss 0.888\n",
      "delta -0.0045327544\n",
      "(1000, 9)\n",
      "step 448 loss 0.884\n",
      "delta -0.0044988394\n",
      "(1000, 9)\n",
      "step 449 loss 0.879\n",
      "delta -0.004464984\n",
      "(1000, 9)\n",
      "step 450 loss 0.875\n",
      "delta -0.004431367\n",
      "(1000, 9)\n",
      "step 451 loss 0.870\n",
      "delta -0.0043979287\n",
      "(1000, 9)\n",
      "step 452 loss 0.866\n",
      "delta -0.0043649077\n",
      "(1000, 9)\n",
      "step 453 loss 0.862\n",
      "delta -0.0043317676\n",
      "(1000, 9)\n",
      "step 454 loss 0.857\n",
      "delta -0.004299104\n",
      "(1000, 9)\n",
      "step 455 loss 0.853\n",
      "delta -0.004266262\n",
      "(1000, 9)\n",
      "step 456 loss 0.849\n",
      "delta -0.0042342544\n",
      "(1000, 9)\n",
      "step 457 loss 0.845\n",
      "delta -0.004201591\n",
      "(1000, 9)\n",
      "step 458 loss 0.840\n",
      "delta -0.0041698813\n",
      "(1000, 9)\n",
      "step 459 loss 0.836\n",
      "delta -0.0041380525\n",
      "(1000, 9)\n",
      "step 460 loss 0.832\n",
      "delta -0.0041064024\n",
      "(1000, 9)\n",
      "step 461 loss 0.828\n",
      "delta -0.0040750504\n",
      "(1000, 9)\n",
      "step 462 loss 0.824\n",
      "delta -0.0040439963\n",
      "(1000, 9)\n",
      "step 463 loss 0.820\n",
      "delta -0.0040128827\n",
      "(1000, 9)\n",
      "step 464 loss 0.816\n",
      "delta -0.003982246\n",
      "(1000, 9)\n",
      "step 465 loss 0.812\n",
      "delta -0.0039517283\n",
      "(1000, 9)\n",
      "step 466 loss 0.808\n",
      "delta -0.003921032\n",
      "(1000, 9)\n",
      "step 467 loss 0.804\n",
      "delta -0.0038912296\n",
      "(1000, 9)\n",
      "step 468 loss 0.800\n",
      "delta -0.0038609505\n",
      "(1000, 9)\n",
      "step 469 loss 0.797\n",
      "delta -0.0038314462\n",
      "(1000, 9)\n",
      "step 470 loss 0.793\n",
      "delta -0.0038018227\n",
      "(1000, 9)\n",
      "step 471 loss 0.789\n",
      "delta -0.0037724376\n",
      "(1000, 9)\n",
      "step 472 loss 0.785\n",
      "delta -0.0037433505\n",
      "(1000, 9)\n",
      "step 473 loss 0.782\n",
      "delta -0.0037143826\n",
      "(1000, 9)\n",
      "step 474 loss 0.778\n",
      "delta -0.0036857128\n",
      "(1000, 9)\n",
      "step 475 loss 0.774\n",
      "delta -0.0036569834\n",
      "(1000, 9)\n",
      "step 476 loss 0.771\n",
      "delta -0.00362885\n",
      "(1000, 9)\n",
      "step 477 loss 0.767\n",
      "delta -0.0036005378\n",
      "(1000, 9)\n",
      "step 478 loss 0.763\n",
      "delta -0.0035726428\n",
      "(1000, 9)\n",
      "step 479 loss 0.760\n",
      "delta -0.0035450459\n",
      "(1000, 9)\n",
      "step 480 loss 0.756\n",
      "delta -0.0035173297\n",
      "(1000, 9)\n",
      "step 481 loss 0.753\n",
      "delta -0.00349015\n",
      "(1000, 9)\n",
      "step 482 loss 0.749\n",
      "delta -0.0034632087\n",
      "(1000, 9)\n",
      "step 483 loss 0.746\n",
      "delta -0.0034359694\n",
      "(1000, 9)\n",
      "step 484 loss 0.743\n",
      "delta -0.0034092069\n",
      "(1000, 9)\n",
      "step 485 loss 0.739\n",
      "delta -0.0033826828\n",
      "(1000, 9)\n",
      "step 486 loss 0.736\n",
      "delta -0.0033563972\n",
      "(1000, 9)\n",
      "step 487 loss 0.732\n",
      "delta -0.00333035\n",
      "(1000, 9)\n",
      "step 488 loss 0.729\n",
      "delta -0.003304243\n",
      "(1000, 9)\n",
      "step 489 loss 0.726\n",
      "delta -0.0032784939\n",
      "(1000, 9)\n",
      "step 490 loss 0.723\n",
      "delta -0.003252864\n",
      "(1000, 9)\n",
      "step 491 loss 0.719\n",
      "delta -0.0032277107\n",
      "(1000, 9)\n",
      "step 492 loss 0.716\n",
      "delta -0.0032024384\n",
      "(1000, 9)\n",
      "step 493 loss 0.713\n",
      "delta -0.003177464\n",
      "(1000, 9)\n",
      "step 494 loss 0.710\n",
      "delta -0.003152728\n",
      "(1000, 9)\n",
      "step 495 loss 0.707\n",
      "delta -0.003127873\n",
      "(1000, 9)\n",
      "step 496 loss 0.704\n",
      "delta -0.0031036139\n",
      "(1000, 9)\n",
      "step 497 loss 0.701\n",
      "delta -0.0030793548\n",
      "(1000, 9)\n",
      "step 498 loss 0.697\n",
      "delta -0.0030554533\n",
      "(1000, 9)\n",
      "step 499 loss 0.694\n",
      "delta -0.0030314326\n",
      "(1000, 9)\n",
      "step 500 loss 0.691\n",
      "delta -0.0030076504\n",
      "(1000, 9)\n",
      "step 501 loss 0.688\n",
      "delta -0.0029845834\n",
      "(1000, 9)\n",
      "step 502 loss 0.686\n",
      "delta -0.0029609203\n",
      "(1000, 9)\n",
      "step 503 loss 0.683\n",
      "delta -0.0029379725\n",
      "(1000, 9)\n",
      "step 504 loss 0.680\n",
      "delta -0.0029149055\n",
      "(1000, 9)\n",
      "step 505 loss 0.677\n",
      "delta -0.0028924346\n",
      "(1000, 9)\n",
      "step 506 loss 0.674\n",
      "delta -0.0028694868\n",
      "(1000, 9)\n",
      "step 507 loss 0.671\n",
      "delta -0.002847433\n",
      "(1000, 9)\n",
      "step 508 loss 0.668\n",
      "delta -0.0028250217\n",
      "(1000, 9)\n",
      "step 509 loss 0.665\n",
      "delta -0.0028030872\n",
      "(1000, 9)\n",
      "step 510 loss 0.663\n",
      "delta -0.0027810335\n",
      "(1000, 9)\n",
      "step 511 loss 0.660\n",
      "delta -0.0027595162\n",
      "(1000, 9)\n",
      "step 512 loss 0.657\n",
      "delta -0.0027380586\n",
      "(1000, 9)\n",
      "step 513 loss 0.654\n",
      "delta -0.00271672\n",
      "(1000, 9)\n",
      "step 514 loss 0.652\n",
      "delta -0.0026953816\n",
      "(1000, 9)\n",
      "step 515 loss 0.649\n",
      "delta -0.0026744604\n",
      "(1000, 9)\n",
      "step 516 loss 0.646\n",
      "delta -0.0026538372\n",
      "(1000, 9)\n",
      "step 517 loss 0.644\n",
      "delta -0.0026329756\n",
      "(1000, 9)\n",
      "step 518 loss 0.641\n",
      "delta -0.0026126504\n",
      "(1000, 9)\n",
      "step 519 loss 0.639\n",
      "delta -0.0025921464\n",
      "(1000, 9)\n",
      "step 520 loss 0.636\n",
      "delta -0.0025722384\n",
      "(1000, 9)\n",
      "step 521 loss 0.633\n",
      "delta -0.002552092\n",
      "(1000, 9)\n",
      "step 522 loss 0.631\n",
      "delta -0.002532363\n",
      "(1000, 9)\n",
      "step 523 loss 0.628\n",
      "delta -0.0025126338\n",
      "(1000, 9)\n",
      "step 524 loss 0.626\n",
      "delta -0.0024932027\n",
      "(1000, 9)\n",
      "step 525 loss 0.623\n",
      "delta -0.0024739504\n",
      "(1000, 9)\n",
      "step 526 loss 0.621\n",
      "delta -0.0024545789\n",
      "(1000, 9)\n",
      "step 527 loss 0.619\n",
      "delta -0.0024358034\n",
      "(1000, 9)\n",
      "step 528 loss 0.616\n",
      "delta -0.0024167895\n",
      "(1000, 9)\n",
      "step 529 loss 0.614\n",
      "delta -0.0023981333\n",
      "(1000, 9)\n",
      "step 530 loss 0.611\n",
      "delta -0.0023797154\n",
      "(1000, 9)\n",
      "step 531 loss 0.609\n",
      "delta -0.0023612976\n",
      "(1000, 9)\n",
      "step 532 loss 0.607\n",
      "delta -0.002342999\n",
      "(1000, 9)\n",
      "step 533 loss 0.604\n",
      "delta -0.0023248196\n",
      "(1000, 9)\n",
      "step 534 loss 0.602\n",
      "delta -0.0023070574\n",
      "(1000, 9)\n",
      "step 535 loss 0.600\n",
      "delta -0.0022892356\n",
      "(1000, 9)\n",
      "step 536 loss 0.597\n",
      "delta -0.0022717714\n",
      "(1000, 9)\n",
      "step 537 loss 0.595\n",
      "delta -0.0022541285\n",
      "(1000, 9)\n",
      "step 538 loss 0.593\n",
      "delta -0.002236843\n",
      "(1000, 9)\n",
      "step 539 loss 0.591\n",
      "delta -0.0022194386\n",
      "(1000, 9)\n",
      "step 540 loss 0.589\n",
      "delta -0.0022028089\n",
      "(1000, 9)\n",
      "step 541 loss 0.586\n",
      "delta -0.0021858215\n",
      "(1000, 9)\n",
      "step 542 loss 0.584\n",
      "delta -0.0021688938\n",
      "(1000, 9)\n",
      "step 543 loss 0.582\n",
      "delta -0.002152443\n",
      "(1000, 9)\n",
      "step 544 loss 0.580\n",
      "delta -0.0021358728\n",
      "(1000, 9)\n",
      "step 545 loss 0.578\n",
      "delta -0.0021196008\n",
      "(1000, 9)\n",
      "step 546 loss 0.576\n",
      "delta -0.002103448\n",
      "(1000, 9)\n",
      "step 547 loss 0.574\n",
      "delta -0.0020874143\n",
      "(1000, 9)\n",
      "step 548 loss 0.572\n",
      "delta -0.0020712018\n",
      "(1000, 9)\n",
      "step 549 loss 0.569\n",
      "delta -0.0020558238\n",
      "(1000, 9)\n",
      "step 550 loss 0.567\n",
      "delta -0.0020400882\n",
      "(1000, 9)\n",
      "step 551 loss 0.565\n",
      "delta -0.0020244718\n",
      "(1000, 9)\n",
      "step 552 loss 0.563\n",
      "delta -0.002009213\n",
      "(1000, 9)\n",
      "step 553 loss 0.561\n",
      "delta -0.0019937754\n",
      "(1000, 9)\n",
      "step 554 loss 0.559\n",
      "delta -0.0019788146\n",
      "(1000, 9)\n",
      "step 555 loss 0.557\n",
      "delta -0.0019637942\n",
      "(1000, 9)\n",
      "step 556 loss 0.555\n",
      "delta -0.0019490123\n",
      "(1000, 9)\n",
      "step 557 loss 0.554\n",
      "delta -0.0019341707\n",
      "(1000, 9)\n",
      "step 558 loss 0.552\n",
      "delta -0.0019195676\n",
      "(1000, 9)\n",
      "step 559 loss 0.550\n",
      "delta -0.0019053221\n",
      "(1000, 9)\n",
      "step 560 loss 0.548\n",
      "delta -0.0018906593\n",
      "(1000, 9)\n",
      "step 561 loss 0.546\n",
      "delta -0.001876533\n",
      "(1000, 9)\n",
      "step 562 loss 0.544\n",
      "delta -0.0018624067\n",
      "(1000, 9)\n",
      "step 563 loss 0.542\n",
      "delta -0.0018485188\n",
      "(1000, 9)\n",
      "step 564 loss 0.540\n",
      "delta -0.0018345714\n",
      "(1000, 9)\n",
      "step 565 loss 0.539\n",
      "delta -0.0018209219\n",
      "(1000, 9)\n",
      "step 566 loss 0.537\n",
      "delta -0.0018070936\n",
      "(1000, 9)\n",
      "step 567 loss 0.535\n",
      "delta -0.0017938018\n",
      "(1000, 9)\n",
      "step 568 loss 0.533\n",
      "delta -0.0017802119\n",
      "(1000, 9)\n",
      "step 569 loss 0.531\n",
      "delta -0.0017670989\n",
      "(1000, 9)\n",
      "step 570 loss 0.530\n",
      "delta -0.0017538071\n",
      "(1000, 9)\n",
      "step 571 loss 0.528\n",
      "delta -0.0017407537\n",
      "(1000, 9)\n",
      "step 572 loss 0.526\n",
      "delta -0.0017279387\n",
      "(1000, 9)\n",
      "step 573 loss 0.525\n",
      "delta -0.0017148852\n",
      "(1000, 9)\n",
      "step 574 loss 0.523\n",
      "delta -0.0017023683\n",
      "(1000, 9)\n",
      "step 575 loss 0.521\n",
      "delta -0.0016896129\n",
      "(1000, 9)\n",
      "step 576 loss 0.519\n",
      "delta -0.0016772151\n",
      "(1000, 9)\n",
      "step 577 loss 0.518\n",
      "delta -0.0016647577\n",
      "(1000, 9)\n",
      "step 578 loss 0.516\n",
      "delta -0.0016524792\n",
      "(1000, 9)\n",
      "step 579 loss 0.514\n",
      "delta -0.0016403794\n",
      "(1000, 9)\n",
      "step 580 loss 0.513\n",
      "delta -0.0016281605\n",
      "(1000, 9)\n",
      "step 581 loss 0.511\n",
      "delta -0.00161618\n",
      "(1000, 9)\n",
      "step 582 loss 0.510\n",
      "delta -0.0016043782\n",
      "(1000, 9)\n",
      "step 583 loss 0.508\n",
      "delta -0.0015926957\n",
      "(1000, 9)\n",
      "step 584 loss 0.506\n",
      "delta -0.001580894\n",
      "(1000, 9)\n",
      "step 585 loss 0.505\n",
      "delta -0.0015693903\n",
      "(1000, 9)\n",
      "step 586 loss 0.503\n",
      "delta -0.001557827\n",
      "(1000, 9)\n",
      "step 587 loss 0.502\n",
      "delta -0.0015465617\n",
      "(1000, 9)\n",
      "step 588 loss 0.500\n",
      "delta -0.0015352368\n",
      "(1000, 9)\n",
      "step 589 loss 0.499\n",
      "delta -0.0015240908\n",
      "(1000, 9)\n",
      "step 590 loss 0.497\n",
      "delta -0.0015129447\n",
      "(1000, 9)\n",
      "step 591 loss 0.496\n",
      "delta -0.001502037\n",
      "(1000, 9)\n",
      "step 592 loss 0.494\n",
      "delta -0.0014910102\n",
      "(1000, 9)\n",
      "step 593 loss 0.493\n",
      "delta -0.0014803112\n",
      "(1000, 9)\n",
      "step 594 loss 0.491\n",
      "delta -0.0014697015\n",
      "(1000, 9)\n",
      "step 595 loss 0.490\n",
      "delta -0.0014588833\n",
      "(1000, 9)\n",
      "step 596 loss 0.488\n",
      "delta -0.0014484823\n",
      "(1000, 9)\n",
      "step 597 loss 0.487\n",
      "delta -0.0014380217\n",
      "(1000, 9)\n",
      "step 598 loss 0.485\n",
      "delta -0.0014276206\n",
      "(1000, 9)\n",
      "step 599 loss 0.484\n",
      "delta -0.0014173985\n",
      "(1000, 9)\n",
      "step 600 loss 0.483\n",
      "delta -0.0014072955\n",
      "(1000, 9)\n",
      "step 601 loss 0.481\n",
      "delta -0.0013970733\n",
      "(1000, 9)\n",
      "step 602 loss 0.480\n",
      "delta -0.0013871491\n",
      "(1000, 9)\n",
      "step 603 loss 0.479\n",
      "delta -0.0013772845\n",
      "(1000, 9)\n",
      "step 604 loss 0.477\n",
      "delta -0.0013673902\n",
      "(1000, 9)\n",
      "step 605 loss 0.476\n",
      "delta -0.0013576448\n",
      "(1000, 9)\n",
      "step 606 loss 0.474\n",
      "delta -0.001348108\n",
      "(1000, 9)\n",
      "step 607 loss 0.473\n",
      "delta -0.0013383627\n",
      "(1000, 9)\n",
      "step 608 loss 0.472\n",
      "delta -0.001328975\n",
      "(1000, 9)\n",
      "step 609 loss 0.470\n",
      "delta -0.0013195276\n",
      "(1000, 9)\n",
      "step 610 loss 0.469\n",
      "delta -0.0013100803\n",
      "(1000, 9)\n",
      "step 611 loss 0.468\n",
      "delta -0.0013009608\n",
      "(1000, 9)\n",
      "step 612 loss 0.467\n",
      "delta -0.0012917519\n",
      "(1000, 9)\n",
      "step 613 loss 0.465\n",
      "delta -0.0012825727\n",
      "(1000, 9)\n",
      "step 614 loss 0.464\n",
      "delta -0.001273632\n",
      "(1000, 9)\n",
      "step 615 loss 0.463\n",
      "delta -0.001264751\n",
      "(1000, 9)\n",
      "step 616 loss 0.461\n",
      "delta -0.0012557209\n",
      "(1000, 9)\n",
      "step 617 loss 0.460\n",
      "delta -0.0012470484\n",
      "(1000, 9)\n",
      "step 618 loss 0.459\n",
      "delta -0.0012383461\n",
      "(1000, 9)\n",
      "step 619 loss 0.458\n",
      "delta -0.0012295842\n",
      "(1000, 9)\n",
      "step 620 loss 0.457\n",
      "delta -0.0012209713\n",
      "(1000, 9)\n",
      "step 621 loss 0.455\n",
      "delta -0.0012126565\n",
      "(1000, 9)\n",
      "step 622 loss 0.454\n",
      "delta -0.0012040436\n",
      "(1000, 9)\n",
      "step 623 loss 0.453\n",
      "delta -0.0011957884\n",
      "(1000, 9)\n",
      "step 624 loss 0.452\n",
      "delta -0.0011875033\n",
      "(1000, 9)\n",
      "step 625 loss 0.451\n",
      "delta -0.0011791587\n",
      "(1000, 9)\n",
      "step 626 loss 0.449\n",
      "delta -0.0011711121\n",
      "(1000, 9)\n",
      "step 627 loss 0.448\n",
      "delta -0.0011630356\n",
      "(1000, 9)\n",
      "step 628 loss 0.447\n",
      "delta -0.0011549294\n",
      "(1000, 9)\n",
      "step 629 loss 0.446\n",
      "delta -0.0011470318\n",
      "(1000, 9)\n",
      "step 630 loss 0.445\n",
      "delta -0.0011391342\n",
      "(1000, 9)\n",
      "step 631 loss 0.444\n",
      "delta -0.001131326\n",
      "(1000, 9)\n",
      "step 632 loss 0.443\n",
      "delta -0.0011235774\n",
      "(1000, 9)\n",
      "step 633 loss 0.441\n",
      "delta -0.0011159182\n",
      "(1000, 9)\n",
      "step 634 loss 0.440\n",
      "delta -0.0011081398\n",
      "(1000, 9)\n",
      "step 635 loss 0.439\n",
      "delta -0.0011008084\n",
      "(1000, 9)\n",
      "step 636 loss 0.438\n",
      "delta -0.0010931492\n",
      "(1000, 9)\n",
      "step 637 loss 0.437\n",
      "delta -0.001085788\n",
      "(1000, 9)\n",
      "step 638 loss 0.436\n",
      "delta -0.001078397\n",
      "(1000, 9)\n",
      "step 639 loss 0.435\n",
      "delta -0.0010710955\n",
      "(1000, 9)\n",
      "step 640 loss 0.434\n",
      "delta -0.0010639131\n",
      "(1000, 9)\n",
      "step 641 loss 0.433\n",
      "delta -0.0010565221\n",
      "(1000, 9)\n",
      "step 642 loss 0.432\n",
      "delta -0.0010495186\n",
      "(1000, 9)\n",
      "step 643 loss 0.431\n",
      "delta -0.0010424852\n",
      "(1000, 9)\n",
      "step 644 loss 0.430\n",
      "delta -0.0010353625\n",
      "(1000, 9)\n",
      "step 645 loss 0.429\n",
      "delta -0.0010284483\n",
      "(1000, 9)\n",
      "step 646 loss 0.428\n",
      "delta -0.001021564\n",
      "(1000, 9)\n",
      "step 647 loss 0.427\n",
      "delta -0.0010146797\n",
      "(1000, 9)\n",
      "step 648 loss 0.426\n",
      "delta -0.0010079741\n",
      "(1000, 9)\n",
      "step 649 loss 0.425\n",
      "delta -0.001001209\n",
      "(1000, 9)\n",
      "step 650 loss 0.424\n",
      "delta -0.0009945333\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "from jax import random as jrnd\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, value_and_grad\n",
    "key = jrnd.PRNGKey(0)\n",
    "key, wkey = jrnd.split(key, 2)\n",
    "params['reward_est'] = jrnd.normal(wkey, (env.n_states,))\n",
    "params['bias'] = 10*jnp.ones((1,))\n",
    "params['temperature'] = .5*jnp.ones((1, ))\n",
    "lr = 1e-4\n",
    "steps = 10000\n",
    "loss_prev = float('Inf')\n",
    "delta = 100\n",
    "\n",
    "step = 0\n",
    "while(np.abs(delta) > .0001 and step<steps):\n",
    "    loss, gradients = (value_and_grad(top_K_feedback_model.loss)(params, traj['states'], labels))\n",
    "    print('step %d loss %.3f' %(step, loss))\n",
    "    if step > 0: \n",
    "        delta = loss-loss_prev\n",
    "        print('delta', delta)\n",
    "   \n",
    "    for k in params.keys(): \n",
    "        params[k] -= lr*gradients[k]\n",
    "        \n",
    "    loss_prev = loss\n",
    "    step +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['obs', 'states', 'acts'])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2082, -1.058 , -0.2937, -0.4412,  0.2367, -0.0175, -0.9893,  1.156 , -0.5381, -0.4527,\n",
       "        0.2703, -1.4129,  1.8543,  0.2276,  0.4975, -2.0897], dtype=float32)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(params['reward_est'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Estimated Reward Function')"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(params['reward_est'], label='estimated')\n",
    "plt.plot(rew_params, label='mce irl rewards')\n",
    "plt.plot(env.reward_matrix, label='real reward')\n",
    "plt.legend()\n",
    "plt.title('Estimated Reward Function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, topk_om = mce_irl.mce_occupancy_measures(env, R=params['reward_est'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state visitation frequencies for each grid cell:\n",
      "[[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 6.8535e+00 4.6748e-02 0.0000e+00]\n",
      " [0.0000e+00 1.0985e+00 1.2728e-03 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(optimal_om.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred w. Top K\n",
      "[[0.     0.     0.     0.    ]\n",
      " [0.     1.1558 0.3323 0.    ]\n",
      " [0.     2.5824 3.9295 0.    ]\n",
      " [0.     0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print('Inferred w. Top K')\n",
    "print(topk_om.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state visitation frequencies for each grid cell:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 3. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print('Optimal state visitation frequencies for each grid cell:')\n",
    "print(env.reward_matrix.reshape((random_gridworld.height, random_gridworld.width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward_est': DeviceArray([ 0.2082, -1.058 , -0.2937, -0.4412,  0.2367, -0.0175, -0.9893,  1.156 , -0.5381,\n",
       "              -0.4527,  0.2703, -1.4129,  1.8543,  0.2276,  0.4975, -2.0897], dtype=float32),\n",
       " 'bias': DeviceArray([9.9902], dtype=float32),\n",
       " 'temperature': DeviceArray([-0.0633], dtype=float32)}"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
